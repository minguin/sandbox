{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1dKLw-LLNXVxx7edzLU0pr9-8SUXBy1eR","timestamp":1683523235741},{"file_id":"181BSOH6KF_1o2lFG8DQ6eJd2MZyiSBNt","timestamp":1683212544593},{"file_id":"1a7JJvGUYyzEfAsmHleqlpCay6QlU7Kf9","timestamp":1680680723724}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["https://www.youtube.com/watch?v=TLf90ipMzfE"],"metadata":{"id":"ZcntN6bU6uPq"}},{"cell_type":"code","source":["# connect your Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"JaF1drgJtjG8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlorSbccWEDa"},"outputs":[],"source":["!pip install langchain\n","!pip install openai\n","!pip install faiss-cpu\n","\n","!pip install pypdf\n","!pip install unstructured\n","!pip install pytesseract"]},{"cell_type":"code","source":["# https://platform.openai.com/account/billing/overview\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = \"\"\n","\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n","\n","from IPython.display import display, Markdown\n","from langchain.docstore.document import Document"],"metadata":{"id":"nq0vKGFeW1KD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## URL\n","- https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/url.html\n","- https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/web_base.html"],"metadata":{"id":"-Smhx8cyGA6Q"}},{"cell_type":"code","source":["urls = [\"https://www.mizuhobank.co.jp/fintec/guide/index.html\",\"https://www.mizuhobank.co.jp/fintec/recruit/index.html\",\"https://www.mizuhobank.co.jp/fintec/recruit/environment.html\"]"],"metadata":{"id":"ZkFBybrEOKT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import UnstructuredURLLoader\n","loader = UnstructuredURLLoader(urls=urls)\n","raw_text = loader.load()\n","raw_text = [s.page_content for s in raw_text]\n","print(len(raw_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qoO13kL-NWJN","executionInfo":{"status":"ok","timestamp":1683348733330,"user_tz":-540,"elapsed":2003,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"6ae42c6b-8480-4c8b-af77-3ca19fb73340"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n"]}]},{"cell_type":"code","source":["from langchain.document_loaders import WebBaseLoader\n","loader = WebBaseLoader(urls)\n","raw_text = loader.load()\n","raw_text = [s.page_content for s in raw_text]\n","print(len(raw_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlJFik8YFmig","executionInfo":{"status":"ok","timestamp":1683348733582,"user_tz":-540,"elapsed":253,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"40bdb74a-a98b-4c7d-f6ac-3f13bc006940"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n"]}]},{"cell_type":"markdown","source":["## PDF\n","- https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html"],"metadata":{"id":"jngTDKRkF_AY"}},{"cell_type":"code","source":["from langchain.document_loaders import PyPDFLoader\n","reader = PyPDFLoader(\"/content/drive/My Drive/data/Supply chain logistics with quantum and classical annealing algorithms.pdf\")\n","reader = PyPDFLoader(\"/content/drive/My Drive/data/fg_fy.pdf\")\n","# ページごと（ページ最後の文章など前後関係がつけられなくなってしまう）\n","raw_text = reader.load_and_split()\n","# テキスト連結\n","raw_text = ''.join([s.page_content for s in raw_text])"],"metadata":{"id":"NalD3XkQWrJR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## YouTube transcripts\n","- https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/youtube_transcript.html"],"metadata":{"id":"xPKt49KBdWL6"}},{"cell_type":"code","source":["!pip install youtube-transcript-api\n","!pip install pytube"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UcOli8Qdcjo","executionInfo":{"status":"ok","timestamp":1683349762425,"user_tz":-540,"elapsed":8198,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"2e92185d-54c8-4d78-f060-85550d488d02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.10/dist-packages (0.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.27.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2022.12.7)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytube\n","  Downloading pytube-12.1.3-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pytube\n","Successfully installed pytube-12.1.3\n"]}]},{"cell_type":"code","source":["from langchain.document_loaders import YoutubeLoader\n","loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=Ix9WIZpArm0&t=463s\", add_video_info=False)\n","loader.load()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Psf-bgjXdVWg","executionInfo":{"status":"ok","timestamp":1683349861061,"user_tz":-540,"elapsed":811,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"ba4da223-6b13-4ff8-dbdc-a8172d922f60"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content=\"is mayor from chartered data and today we're going to be talking about how do you chat with multiple massive PDF documents across multiple files say in this case we're looking at Tesla we're looking at annual reports for 2022 2021 2020 and each year the PDF files are huge like here is 2022 is 449 and then 20 20 21 is around 300 and you add it up you're looking about a thousand pages of PDFs and it just goes on and on and on and just Financial reports and all that kind of stuff extremely tedious to go through and read and what we want is a situation like this where you go Warren Buffett asking you what you like to analyze but we can ask a question about specific PDF so what are the or what were the risk factors for Tesla 2022. let's say I start searching and it's going to give us a response so let's talk about cyclical industry inflationary pressures increasing interest rates the pandemic and what we want to do now is cross check because it's going to give us a reference and it points to page 33. it goes page 33. here oops and let's see we are focused on growing manufacturing demanding cells energy regeneration and let's see is this about risk okay so it covers the highlights so more or less it's kind of looking at this section that has the management discussion um and is able to kind of assess the risk factors from there but what if we don't just want to talk to One PDF I mean talking to one PDF is already insane right that's that was a 500 page PDF document but what if we wanted to talk to three of them simultaneously over the past three years and ask a question that analyzes the past three years so how have management formed over the past three years and let's see now this is where it gets interesting and it picks up on the three years of the annual reports and let's see if we can figure out because there was no performance to provide a more accurate processor more context is required fair enough and it manages to pick out sources as well for each of these years now what if you want to ask a more technical question and so we say something like uh what is how has testers gross margin changed over past three years and let's see so now it figures out I'm looking at these three reports increase okay interesting nice and you know it provides references so let's let's check the 2022 reference page 39 let's hop over they usually cover more than one page but let's see nice so we got a comparison of the revenues and is able to kind of pick up these things here as well I think I mentioned some of these numbers pretty accurately too so that's pretty cool so that's what we're talking about here is how do we talk across not just one not just two three or more PDF files to get insights so how does this work let's jump into the diagram and before I do that actually if this is your first time kind of seeing this kind of structure I'm about to show you it's going to be overwhelming so I advise you watch the previous video that kind of covered uh a PDF chatbot a simple PDF chatbot and you can go to the repository here and the link will be below as well and this is the starting point for this um this is a this from from here I kind of customized to design um this demo that I'm showing the demo is very buggy so please uh I'm PR I'm probably not going to release the code anytime soon because it's it's uh I'm I'm testing it as I'm recording this video so here is the multiple PDF chapter architecture now if you've seen the previous one in the repo or the previous video you're probably like what is going on here okay um just bear with me I'm gonna go through this slowly okay we're gonna go through make sense of what's going on so first of all we have the PDF Docs of each PDF that you have so in this case I have say 2021 Tesla and reports 2022 teslan reports I convert each of them to text because PDF is binary right so you need it in text and once we have the text we split each of them into chunks because open AI has context window that only allows so much and so we have these chunks we pass it to open AI to create these embeddings which are just number representations and this phase from here we'll call ingestion it's the process of converting your docs into text and ultimately to number representations that computers can understand and you can store somewhere you can search for quickly you can see that the 2022 got stored in these numbers here in this thing called a vector store and 2021 the same thing and what is a vector store you can think of it as a database of some sort that houses the number representation of your documents in different categories or different spaces and let's call these spaces name spaces so our namespace you can kind of just think of as just a little box in your house right and that box has specific things it could be a box that contains all your clothes or it could be a box that contains all your shoes the point is it contains something similar so in this case this box this namespace in this database contains the number representations of your PDF Docs but also the text of your PDF docs and any other quote-unquote metadata or information that's important related to your documents and the same thing for 2022 2022 in this case we're looking at pinecorn as a vector store and we'll be using uh gpt4 in this case this is Ada we're going to g54 and line chain we're going to jump into that in a second so this is phase one so you do this ingestion now here we go okay so just follow slowly what was Tessa's gross profit margin 2022 now the typical way would be you ask the question and we combine the question with the child history gbt will create a standalone question from these two and then we convert those questions to numbers and we go into the vector store we specify a specific namespace or specific box that we want to retrieve from and then we get the relevant Docs we combine the Standalone question with the relevant Docs and GT4 looks that context question The Prompt response so that's in a nutshell how we've looked at things so far right but when you're dealing with multiple PDF files right you could technically you know have standard question factors 2021 which we we did right where you can ask an individual for individual namespace you can say specify the namespace and say I want information relevant to 2021 I want information relevant to 2022. but what happens if you want to analyze things across multiple years for example or multiple namespaces so you want to be able to say I want to ask a question that is related to 2020 2021 and 2022. and I want to analyze information that's cross-border and so in that case we need a new strategy and so what was Tesla's gross profit margin in 2022 that's a straightforward question right so that would be a base case here right so if I just highlight this a second but what if you ask what was Tesla's gross profit margin in 2021 and 2022 right so now you need a way to search 2021 and 2022 by extracting by first extracting the namespace from the question somehow so we need to use GT4 to help us to somehow extract the namespace from the question so in this case if the namespace is called 2021 we ultimately don't want to hardcore this we want this to be dynamic does that make sense I'll go over the card and kind of go through that slowly too but you want the model to kind of figure out what year is user referring to what namespace is it associated with here whatever name you've decided to give them and then from there we're able to have this Dynamic relationship where we don't just hard code the namespace we've got the namespace down here so when there's a question we go revert the question to embeddings but now we also have this context this dynamic context specifying the name spaces to look at we check the namespaces 2021 2022 retrieve the relevant docs for each namespace and then we're back to the usual procedure so that's why if I go back to the demo it's able to come back with something like this when I specify that I'm looking over the past three years so if I say for example so this is a website called secant Alpha used by a lot of investors and so here it's saying Revenue year on year so let me ask what is Tesla's estimated here on here um over the past now let me just ask that I think that should be sufficient yeah Revenue growth Revenue growth year on here and so we asked the question let's see what happens and so now it's going to ask to specify so I'm going to specify what is Tesla's estimated Revenue growth year or year 2022. and now we've specified and now it's going to search 2022. so here we go let's see where it figures out it's doing the calculations based on the Consolidated statement of operations dividing the numbers and 51.33 let's see what Alpha has say I I have no idea no idea wow that is that's crazy okay I'd not expect that to happen but okay interesting interesting and this is uh trailing as well so this is this is accurate um and so that's what happens now let's uh I just got a bit excited there and that's uh that's insane that's that's look at the card now so I'll jump back to join I hope that makes sense what just happened now it figured out the years then attach it to the namespace and so when the question was asked it knew to ask to to draw the relevant docs from the two uh namespaces okay so let's jump to the card now okay now there's a lot going on here so let me start from the beginning so you can see in the reports folder I have each tester PDF that I showed earlier and I have a function well script that I'm running called ingest data what ingest data does is it will go into the reports folder right here go use a dietary loader from Lang chain to read the file path and as it goes to the file path is looking for PDFs and it's going to load each PDF into text right and so I can kind of show you what that looks like so we're gonna do uh let me just cross check I've got other things going on uh [Music] um let me show you what the output look like anyway so when you've run this script so I'd have run the script I'd have done a uh MPX TSX um fig and then you know had the scripts for Scripts here and ingest data.ts so I'll con I'll cover more of this in the upcoming upcoming Workshop uh but effectively what this does is it loads this page it checks the environment variable where that contains the API keys and then it also runs this it compiles this to JavaScript from typescript on the fly as well without having to um emit the files right so if I was going to run this it would be but I don't want to do that because I've already done the ingestion and it took a couple of minutes but once this process was done let me just clear that before I get into trouble this is what I look like right so you can see I create a file test.json right so this was the first phase right where this is this is literally you can see oh it's a 2020 and then you also have 20 20 21 and 2022 and you can see this is all of Tesla's files in here and what's good what the way I set things up was I have references to each of them and that's why you saw the page number so every page has a page number and also reference to the original source so that was when you saw in the UI that it had that as well and so if you see in ingest in the ingest uh date no testing just also split them so I split the I basically created these Dynamic name spaces right so the dietary loads as I showed in the diagram you basically go through each folder and then you create this grouping of all the different folders in a map again a mapping of each year to a document and I split them into chunks no no sorry I initialize it so I haven't split it up then preparing Pinecone so what's going on here effectively again I'm trying not to get into the details more focus and high level is once we have translated the PDFs into text and we have grouped them say into different categories so 2021 2020 2022 now we need to to split them into chunks right as we spoke about and so here I'm splitting into chunks of one thousand and two hundred overlap and for each chunk for each group I am assigning a namespace as we spoke about called Tesla and the year so what you're going to see what I did later on was once I finished this phase one in my config I change I create a name space years right it's a 2020 Tesla 2020 21 22 and these are all associated with namespaces in my Pinecone um account which I'm going to show you in a second so now we have say test of 2020 we split Tesla 2020 pages into chunks of a thousand characters each and what does that look like well let me show you so we split you can see all of this is just 20 20. literally all 20 20. and each one has the page number right um associated with the chunk so this is page five page five page five Page Six and again the date structure makes it easy to do searches and manipulation down the line so I did the same for 2021 2022 as well so then we have to kind of begin to put into this namespace we're using pinecorn at this point and uh we're trying to insert upset into the namespace but pinecorn has limits right so you cannot just do it all in one go you have to do it in chunks the recommendation is 100 vectors so for contacts again remember your chunks are converted into embeddings these embeddings are numbers so you can call them vectors so these vectors contain your numbers representation of your pages as well as your text and metadata as well so we need to split into chunks so we split into chunks of 50 in this case and each chunk is then inserted into the database right the Pinecone database and this is all using line chain functions to make it easier to run this process as well so once this is all done this is what it looks like so let me hop back kind of just show you so this is my pinecon dashboard so once you sign up you have all these variables here now in the previous video there was a lot of complaints about up certain inserting um the thing with Pinecone is you have to be very uh specific for example your environment is kind of where is your pod closest to and by default if you're on a free plan they will give you one this has to match your environment variable in your code cosine is calculation done for similarity and then Dimensions is a number of Dimensions that open UI creates a dimension is you can think of as one particular spot in Array of vectors so if you have 0.1 1.1 1.2 representing your text that's three dimensions so when opener AI does embeddings it does 1536 so you have to specify 1536 otherwise your the insertion is not going to work make sure your API keys are correct as well and that your index name matches the index name of the configuration of the code again I'm speaking more for everyone who tried to who are trouble with this Repository um which I just want to thank you all for for the support because that's uh it's just trending on GitHub for for a couple of uh for a couple of days so um appreciate that now what was I okay so these are the indexes the index info these are the namespaces and if I jump you can see Tesla 2020 there's a number of vectors for each namespace so remember in the previous video was just one now we have three and now we want to communicate across all three of them so I hope at least this explains what's going on here and in the previous video I showed you can just come here and kind of play around if you don't know what a the vectors look like it's pretty much a case of if I click fetch this is what Vector looks like so you got your namespace which is test in my case and you can see this is the ID and then you have values as I said which is just decibel floating decimal number representations of your text because if I open here you can see the text right it's it's the text so the text is here and these are the 1536 Dimensions I spoke about right so let's go back to the code let's see so once this ingestion process is done again for anyone having trouble with this from the previous video you just want to make sure that your config here the namespaces are matching what's in your dashboard and you've set your environment variable right what whatever whatever that was other than that don't tamper with the versions as well because if you upgrade Lang chain um there's there's some breaking changes so just make sure you cloning exactly as it is and you follow the instructions in the readme which I put there and also you have the video can check out as well but anyway come back to this particular video so once the ingestion is done now we want to go to the next phase and so what's the next phase well let me go back to the diagram just in case you lost track so we're done with this process so now we want to move to phase two we want to chat and we want to be able to specify drag that down we want to be able to specify what exactly or what namespace that we want to retrieve information from we want it to be dynamic and so if I hop back I think I have a script to have it so this is just a like when I was trying to experiment with how to kind of go about this and the way you want to think about it again these all functions from Lang chain and but kind of a bit customized but this is just a high level let's let's think about this highlight so first you have your opening eye um instance this is a line chain opening I chat that basically has the same functionality as the API for open AI except that it has extra Futures like caching and memory and stuff like that so yeah we want to think about on a high level so on a high level basically we have the reports prompt so basically we just have some some prompts that I wrote uh very quickly for this um and don't worry about the syntax this is all Lang change trying to make life easier effectively what we're trying to do is have kind of human problems with the system prompts but the history and we want to effectively be able to dialogue by calling the chain so Lang chain has these things called chains which are just basically a sequence of prompts with llm or call to GPT basically similar to the diagram we have so don't be in too intimidated by the stuff I'll cover this in the workshop upcoming Workshop but and you can also look at the line chain Docs um just focus on the high level and not too much on the chord per se and this is a function to extract the years from the query and basically the query is responding with a response that verifies it says something like this right or it could come back and say like you saw in the UI searching for whatever and So based on that you can extract the year use a regex and once you have the number you map over the number and then you're able to get the namespace right because remember we need to match the number to the namespace in my case this is the way I sell the date structure it could be any whatever you want to do so if the AI says this question is related to 2020 2021 then it the function is going to return the two namespaces that already exist so I hope that makes sense and then you have this custom QA chain and what it does effectively is that it takes the model the index the namespace so the model is whatever your chat open the eye or whatever you open your API is the indexes here so just pinecon index which is the line change as fundamentals and the namespaces is whatever it pulled out so basically what this chain does is it's saying going to the uh Pi cone and effectively it's it's basically set in the stage for the the phase that you saw in the diagram where we have the Standalone question and then we're able to go with a standalone question to retrieve specific name spaces the relevant docs and then combine that and get a response so this is all under the hood that's kind of happening and this is a custom QA train that I made what Lang chain has two others called chat Vector DBQ a chain in Vector DBQ a chain again don't get intimidated but if you focus on the diagram you realize it's not about the chord you can you can do it other ways this is just one way and so I said chat history to nothing and then we make the call and stringify and we're good to go so um let me let me show you what that looks like so if I run oops okay let's see made of TS I'm wondering if it's gonna complain about that it's not sufficient cool let's see so what I want to see is okay there was a question asked I think at the very top and we should see like gpt4 trying to figure out what what year you're referencing extracting the namespace and then we pass that namespace to um the chain and there we go so it's beautiful that's awesome right that's exactly what I was looking for because um here we go so search in 2021 2022 we extracted we now we extract the the years okay then we map the years to the namespaces there in the vector base we search the namespace for results and then this is what we retrieved about risk factors including the source document so I hope this kind of did demystifies what the UI was kind of doing by itself um and so basically yeah there's a front end the front end is already on the repo the gpt4 PDF repo so you can go use that really this is just an adaptation I'm just doing a ton of prompt engineering and just experimenting with talking across different PDFs and then the chat where I basically use the same logic as main.ts that you saw but I'm just using doing it through the API effectively um so it's it's a similar logic I just wanted to test it here first and then once I saw this was decent then I moved it over to the API all right so that's that's basically in a nutshell um I haven't tested it too much but so far I'm able to read three different files with over you know close to a thousand pages of in-depth financial analysis and gpt4 is able to do that and provide analysis across all three years that's decent so let's jump back to the demo and see aha I have a bit more fun and see what's possible so I'm going to use these guys because I think they have trailing oh they have year and year okay three years huh okay fingers crossed let's let's see let's see let's see so let's pick one let me pick um Revenue okay compound annual rate for Revenue over the past three years okay let's see what happens okay I doubt this is gonna work was the compound annual growth rate of Tesla's Revenue over the past three years if this doesn't work I I expect this not to work all right but okay here we go one two three so it's going to look at all three reports it's gonna think through that okay okay ah okay not not bad I mean fifty percent sixty percent it's referencing the the actual files itself wow so this is page 49 uh that's uh that's interesting let's see let's ask uh one more question for the road um you got profitability what what are the what are the kind of questions um so we talked about risk factors management um let's talk about growth rate um based on the past three years um annual reports what is the growth potential Tesla over uh that's just that's just uh stop there uh Warren Warren Buffett section let's see wow yeah this is incredible I I don't know how so talking about okay so expanding production status and uh for context I've only set this to K1 so K1 is just um the number of reference documents to return per PDF ask and uses context and that and that's all I'm doing for now I I could I could actually increase the uh The Returned Source documents as contacts and I'm pretty sure the accuracy is going to jump um go so this is uh very rough sketch um if you're asking for the card just go over here and kind of tweak and play around with it but this is the architecture um that you can kind of use for your own um application um like I said it's just a case of thinking through this step by step and um using GPT to help you along the way so um I'll also make some future changes to this um repository as well so I'll add futures for multiple PDF files as well here at some point in the future but in the meantime if you have any questions just uh shoot me a message on Twitter at mail ocean or you can stream your message on YouTube um I understand this is a bit complicated but just kind of re-watch the video and um I'm sure it makes sense so uh thanks for watching and if you want more in depth step by step details in this stuff I'm going to be doing a series of workshops soon so you can see that uh check sign up for the waitlist uh in the description section as well so um so that's it cheers\", metadata={'source': 'Ix9WIZpArm0&t=463s'})]"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["## Gutenberg\n","- https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/gutenberg.html"],"metadata":{"id":"_tdoQaS23oc-"}},{"cell_type":"code","source":["from langchain.document_loaders import GutenbergLoader\n","loader = GutenbergLoader('https://www.gutenberg.org/cache/epub/69972/pg69972.txt')\n","data = loader.load()\n","print(data[0].page_content[:300])\n","print(data[0].metadata)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VtJKMxVx3vQv","executionInfo":{"status":"ok","timestamp":1683356624025,"user_tz":-540,"elapsed":519,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"49ac1859-362d-42d4-8409-4710816f1d41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The Project Gutenberg eBook of The changed brides, by Emma Dorothy\r\n","\n","\n","Eliza Nevitte Southworth\r\n","\n","\n","\r\n","\n","\n","This eBook is for the use of anyone anywhere in the United States and\r\n","\n","\n","most other parts of the world at no cost and with almost no restrictions\r\n","\n","\n","whatsoever. You may copy it, give it away or re-u\n","{'source': 'https://www.gutenberg.org/cache/epub/69972/pg69972.txt'}\n"]}]},{"cell_type":"markdown","source":["## Hacker News\n","- https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/hacker_news.html"],"metadata":{"id":"2q4bW_Sz3rcV"}},{"cell_type":"code","source":["from langchain.document_loaders import HNLoader\n","loader = HNLoader(\"https://news.ycombinator.com/item?id=34817881\")\n","data = loader.load()\n","print(data[0].page_content[:300])\n","print(data[0].metadata)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OB6fapaV3u9_","executionInfo":{"status":"ok","timestamp":1683356624417,"user_tz":-540,"elapsed":2,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"9b75d1f8-0439-4dc6-b04e-b67c7a075602"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["delta_p_delta_x 78 days ago  \n","             | next [–] \n","\n","Astrophysical and cosmological simulations are often insightful. They're also very cross-disciplinary; besides the obvious astrophysics, there's networking and sysadmin, parallel computing and algorithm theory (so that the simulation programs a\n","{'source': 'https://news.ycombinator.com/item?id=34817881', 'title': 'What Lights the Universe’s Standard Candles?'}\n"]}]},{"cell_type":"markdown","source":["## Arxiv\n","- https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/arxiv.html"],"metadata":{"id":"HI-UG8UIay8J"}},{"cell_type":"code","source":["!pip install arxiv\n","!pip install pymupdf"],"metadata":{"id":"YXc_Om7qa1RA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import ArxivLoader\n","docs = ArxivLoader(query=\"strawberry\", load_max_docs=2).load()\n","len(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xO4V5Hfba8jI","executionInfo":{"status":"ok","timestamp":1683349109134,"user_tz":-540,"elapsed":4072,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"d4c67795-ee19-424e-aa7c-db75e19652d6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["docs[0].metadata"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-d3x11ua8fI","executionInfo":{"status":"ok","timestamp":1683349111595,"user_tz":-540,"elapsed":289,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"1be71bd1-ee87-4a1b-df72-0bd38fb9207f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Published': '2023-01-10',\n"," 'Title': 'Autonomous Strawberry Picking Robotic System (Robofruit)',\n"," 'Authors': 'Soran Parsa, Bappaditya Debnath, Muhammad Arshad Khan, Amir Ghalamzan E.',\n"," 'Summary': 'Challenges in strawberry picking made selective harvesting robotic technology\\ndemanding. However, selective harvesting of strawberries is complicated forming\\na few scientific research questions. Most available solutions only deal with a\\nspecific picking scenario, e.g., picking only a single variety of fruit in\\nisolation. Nonetheless, most economically viable (e.g. high-yielding and/or\\ndisease-resistant) varieties of strawberry are grown in dense clusters. The\\ncurrent perception technology in such use cases is inefficient. In this work,\\nwe developed a novel system capable of harvesting strawberries with several\\nunique features. The features allow the system to deal with very complex\\npicking scenarios, e.g. dense clusters. Our concept of a modular system makes\\nour system reconfigurable to adapt to different picking scenarios. We designed,\\nmanufactured, and tested a picking head with 2.5 DOF (2 independent mechanisms\\nand 1 dependent cutting system) capable of removing possible occlusions and\\nharvesting targeted strawberries without contacting fruit flesh to avoid damage\\nand bruising. In addition, we developed a novel perception system to localise\\nstrawberries and detect their key points, picking points, and determine their\\nripeness. For this purpose, we introduced two new datasets. Finally, we tested\\nthe system in a commercial strawberry growing field and our research farm with\\nthree different strawberry varieties. The results show the effectiveness and\\nreliability of the proposed system. The designed picking head was able to\\nremove occlusions and harvest strawberries effectively. The perception system\\nwas able to detect and determine the ripeness of strawberries with 95%\\naccuracy. In total, the system was able to harvest 87% of all detected\\nstrawberries with a success rate of 83% for all pluckable fruits. We also\\ndiscuss a series of open research questions in the discussion section.'}"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["docs[0].metadata['Summary']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"codHu0k3bUV5","executionInfo":{"status":"ok","timestamp":1683349141600,"user_tz":-540,"elapsed":3,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"dd141833-2bd4-48ed-85e5-91d1acf32334"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Challenges in strawberry picking made selective harvesting robotic technology\\ndemanding. However, selective harvesting of strawberries is complicated forming\\na few scientific research questions. Most available solutions only deal with a\\nspecific picking scenario, e.g., picking only a single variety of fruit in\\nisolation. Nonetheless, most economically viable (e.g. high-yielding and/or\\ndisease-resistant) varieties of strawberry are grown in dense clusters. The\\ncurrent perception technology in such use cases is inefficient. In this work,\\nwe developed a novel system capable of harvesting strawberries with several\\nunique features. The features allow the system to deal with very complex\\npicking scenarios, e.g. dense clusters. Our concept of a modular system makes\\nour system reconfigurable to adapt to different picking scenarios. We designed,\\nmanufactured, and tested a picking head with 2.5 DOF (2 independent mechanisms\\nand 1 dependent cutting system) capable of removing possible occlusions and\\nharvesting targeted strawberries without contacting fruit flesh to avoid damage\\nand bruising. In addition, we developed a novel perception system to localise\\nstrawberries and detect their key points, picking points, and determine their\\nripeness. For this purpose, we introduced two new datasets. Finally, we tested\\nthe system in a commercial strawberry growing field and our research farm with\\nthree different strawberry varieties. The results show the effectiveness and\\nreliability of the proposed system. The designed picking head was able to\\nremove occlusions and harvest strawberries effectively. The perception system\\nwas able to detect and determine the ripeness of strawberries with 95%\\naccuracy. In total, the system was able to harvest 87% of all detected\\nstrawberries with a success rate of 83% for all pluckable fruits. We also\\ndiscuss a series of open research questions in the discussion section.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["docs[0].page_content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"nJ5B91y7a8EY","executionInfo":{"status":"ok","timestamp":1683349147147,"user_tz":-540,"elapsed":4,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"96148d13-542b-486b-ce6d-4efff0fc4e70"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Autonomous Strawberry Picking Robotic System\\nSoran Parsa∗\\nLincoln Institute for Agri-food Technology\\nUniversity of Lincoln\\nLincoln, UK\\nsoran.parsa@gmail.com\\nBappaditya Debnath\\nKings College London\\nLondon, UK\\nb.debnath2017@gmail.com\\nMuhammad Arshad Khan\\nLincoln Institute for Agri-food Technology\\nUniversity of Lincoln\\nLincoln, UK\\nMuKhan@lincoln.ac.uk\\nAmir Ghalamzan E.∗†; ‡\\nLincoln Institute for Agri-food Technology\\nUniversity of Lincoln\\nLincoln, UK\\naghalamzanesfahani@lincoln.ac.uk\\nAbstract\\nChallenges in strawberry picking made selective harvesting robotic technology very demand-\\ning. However, the selective harvesting of strawberries is a complicated robotic task forming\\na few scientiﬁc research questions. Most available solutions only deal with a speciﬁc picking\\nscenario, e.g., picking only a single variety of fruit in isolation. Nonetheless, most econom-\\nically viable (e.g. high-yielding and/or disease-resistant) varieties of strawberry are grown\\nin dense clusters. The current perception technology in such use cases is ineﬃcient. In this\\nwork, we developed a novel system capable of harvesting strawberries with several unique\\nfeatures. These features allow the system to deal with very complex picking scenarios, e.g.\\ndense clusters. Our concept of a modular system makes our system reconﬁgurable to adapt\\nto diﬀerent picking scenarios. We designed, manufactured, and tested a patented picking\\nhead with 2.5 degrees of freedom (two independent mechanisms and one dependent cutting\\nsystem) capable of removing possible occlusions and harvesting the targeted strawberry\\nwithout any contact with the fruit ﬂesh to avoid damage and bruising. In addition, we\\ndeveloped a novel perception system to localise strawberries and detect their key points,\\npicking points, and determine their ripeness.\\nFor this purpose, we introduced two new\\ndatasets. Finally, we tested the system in a commercial strawberry growing ﬁeld and our\\nresearch farm with three diﬀerent strawberry varieties. The results show the eﬀectiveness\\nand reliability of the proposed system. The designed picking head was able to remove oc-\\nclusions and harvest strawberries eﬀectively. The perception system was able to detect and\\ndetermine the ripeness of strawberries with 95% accuracy. In total, the system was able to\\nharvest 87% of all detected strawberries with a success rate of 83% for all pluckable fruits.\\nWe also discuss a series of open research questions in the discussion section.\\nkeywords: Selective Harvesting; Robotic manipulation; Computer Vision; Motion planning; Precision\\nfarming; Agricultural robotics\\n∗Soran and Amir equally contributed to this work\\n†Corresponding author\\n‡This work was supported by CERES Agri Tech. It is also partially supported by the Centre for Doctoral Training, United\\nKingdom (CDT) in Agri-Food Robotics (AgriFoRwArdS) Grant reference: EP/S023917/1; Lincoln Agri-Robotics (LAR) funded\\nby Research England.\\narXiv:2301.03947v1  [cs.RO]  10 Jan 2023\\n1\\nIntroduction\\nSelective harvesting of crops using robotic technology aims to address the societal and economical challenges\\nof agricultural labour shortages. The industry is yet far from an eﬃcient and practical solution. Many\\naspects of crop harvesting still remain unsolved (scientiﬁc and technological) problems. The dexterity and\\neﬃciency of robotic end-eﬀectors are open questions. Most of the available picking heads (i.e. end-eﬀectors)\\nfor selective harvesting are capable of performing only two actions: opening the picking head, and closing\\nthe picking head.\\nStrawberry is a highly valued crop. While the annual retail value of the strawberries industry is over\\n$17 Billion globally, producers have to spend over $1 Billion for picking (selective harvesting) only (Web,\\n2020). Factors such as labour shortage, increasing labour costs, and the COVID-19 pandemic are having\\na negative impact on selective harvesting costs.\\nTherefore, robot-based automated selective harvesting\\ntechnologies are highly in demand (Duckett et al., 2018). Over the past decade, both private and public\\nentities have extensively invested to develop commercially viable robotic harvesting technology. Despite the\\nrecent investments and funding (CORDIS, 2020) and (CORDIS, 2015) for harvesting high-value crops, many\\nproblems are still unsolved which form very interesting scientiﬁc questions. Nevertheless, there is not yet a\\ncommercially viable robotic technology available for the selective harvesting of strawberries.\\nOne of the challenges of a desired robotic solution is the picking head they use. While human pickers\\nuse the sense of touch and active perception, multiple ﬁngers and two arms for picking strawberries, using a\\npicking head with a single degree of freedom (DOF) does not look suﬃcient. The available picking heads have\\nlimited ability to harvest strawberries in a dense cluster where a ripe strawberry to be picked is occluded.\\nThe problem is manifold: (1) a ripe strawberry may not be detected, (2) its segment, ripeness assessment,\\nand location may not be precise, (3) existing picking heads may not be able to reach a ripe strawberry\\nsurrounded by other unripe strawberries. We present a robotic picking system capable of addressing these\\nissues\\nMoreover, the asymmetric and irregular nature of the stems coming out of the fruit makes it diﬃcult\\nto localise the picking point. Commercially available depth sensors are designed for large objects under\\ncontrolled lighting conditions. Insuﬃcient quality of depth-sensing technologies makes strawberry picking\\npoint localisation on stem intractable. This is especially true under bright sunlight in farm conditions where\\nthe depth accuracy decreases further. In addition, the depth sensors are designed to work optimally for\\ndistances larger than 50 [cm], and their precision drops to 0 for distances below 15 [cm].\\nHowever, for\\npicking point localisation we require precise depth-sensing below the 15 [cm] range.\\nThis makes the robot perception challenging as some target fruits may be occluded by non-target fruits\\nand leaves. Commercially available depth sensors, e.g, Realsense D435i, also make the perception challenging\\nas they are designed for large objects’ 3-D perception and controlled lighting conditions. For small fruits\\nunder outdoor lighting, the depth maps are not precise. Detecting, segmenting, and localising a ripe fruit\\nto be picked in a complex cluster geometry, under outdoor lighting conditions make strawberry perception\\na very challenging problem.\\nIn this paper, we present a robotic system for automated selective harvesting of strawberries which aims\\nto address some of the challenges preventing large-scale commercial deployment of these systems. (A video of\\nthe system can be seen in this link.) We designed, prototyped and ﬁeld-tested our robotic system beneﬁting\\nfrom a novel picking head for robotic selective harvesting. The picking head demonstrated the ability to\\nnavigate through the cluster to reach a targeted fruit and harvest it successfully. One feature of the picking\\nhead that makes it diﬀerent from the existing technologies, is that it is able to grasp, detach and handle the\\nfruit from its stems without contact with the fruit body. This is important in harvesting and handling soft\\nfruits e.g. strawberries, to reduce bruising and damage and increase fruit shelf time. These characteristics\\nare able to contribute to reducing food waste signiﬁcantly.\\nOur state-of-the-art perception system proved to be eﬀective in detecting and localising fruit in diﬀerent\\nenvironments and lighting conditions. Moreover, the localising of the picking point on the stem is challenging.\\nThis is due to the nature of the 3D perception devices that work poorly on small objects, or under sunlight\\nconditions. To overcome this challenge we propose a novel Gaussian Process Regression method for picking\\npoint error estimation.\\nWe propose a modular and conﬁgurable approach to developing and integrating the robotic system for\\nselective fruit harvesting. The system was reconﬁgured based on two diﬀerent harvesting conditions and\\ntested. Unlike other approaches the robotic harvesting system is designed based on a speciﬁc requirement\\nand only work in a speciﬁc condition, this proposed system is modular and conﬁgurable based on the diﬀerent\\nvarieties and growing condition. The remainder of this paper is organised as follows. Section 2 presents a\\nthorough literature review of the current works. In section 3 and 4 the system architecture and end-eﬀector\\ndesign are discussed respectively. Section 5 presents the perception system. Finally, the ﬁeld experiments\\nand results are presented in section 6 and are discussed in section 7.\\n2\\nRelated Works\\n2.1\\nHarvesting systems and manipulators\\nRobotic harvesting systems in general are mechanisms that are designed to interact with agricultural crops.\\nA typical robotic harvesting system is equipped with a manipulator usually in form of a serial robotic\\narm, a custom-designed end-eﬀector for grasping and/or picking the targeted crop, a perception system\\nfor detection, and a platform to mount all these sub-components which itself could be an autonomous or\\nsemi-autonomous powered mobile platform (Arad et al., 2020).\\nOﬀ-the-shelf robotic arms and manipulators have proven to be functional and reliable and have been\\nemployed largely to develop robotic selective harvesting systems. However, custom-designed manipulators\\nhave been emphasised greater than using oﬀ-the-shelf ones. Among the oﬀ-the-shelf manipulators, six de-\\ngrees of freedom (DOF) were widely used. It has been studied that additional degrees of freedom were\\nadded to or removed from the available controllable DOFs in these oﬀ-the-shelf manipulators according to\\nthe harvesting requirements. Xiong et.al.(Xiong et al., 2019) used a 6-DOF oﬀ-the-shelf manipulator for har-\\nvesting strawberries where 1-DOF was kept ﬁxed during operation to meet selective harvesting orientation\\nrequirement.\\nIn addition to the single-arm manipulator, multiple-arm robots were also utilised for harvesting scenarios\\nto tackle the complexity of selective harvesting. In particular, dual-arm manipulators were developed to\\nwork either collaboratively or as standalone units. Sepulveda et.al.(Sep´uLveda et al., 2020) used a dual\\nrobotic arm to cooperatively harvest eggplants with very promising and successful results in dealing with\\noccluded fruit conditions. Zhao et.al.(Zhao et al., 2016) also used a dual-arm robot for tomato harvesting\\nwhich also operated collaboratively. In this proposed conﬁguration, one arm detaches a tomato from its stem\\nwhile the other arm grips it. Davidson J et.al.(Davidson et al., 2017) utilised a dual-arm mechanism for\\nan apple harvesting robot in which a six-degree of freedom (DOF) apple picker was assisted by a two-DOF\\ncatching mechanism. The second arm catches the picked apple and transfers it to storage. This reduces the\\nharvesting cycle time. A few other dual-arm conﬁgurations were also presented in (Armada et al., 2005;\\nCeres et al., 1998; Xiong et al., 2020b) to speed up the harvesting cycle. The autonomous kiwi harvesting\\nrobot(Scarfe et al., 2009) uses four robots in parallel.\\nThe harvesting robot must reach the varying height, widths, and depths of the targeted crop with respect\\nto the base of the manipulator. Hence, the harvesting platform needs a moving base to increase the limited\\nreachable workspace of a robotic manipulator. In addition to a mobile base that can navigate across ﬁelds,\\nmanipulators were also mounted on the vertical slide(s) (Zhao et al., 2016; Lehnert et al., 2017)(Bac et al.,\\n2017; Ling et al., 2019; Baeten et al., 2008), horizontal slide(s) (Davidson et al., 2017; Silwal et al., 2017;\\nVan Henten et al., 2002), slanting slide (Armada et al., 2005), or on a scissor lift mechanism (Arad et al.,\\n2020)(Feng et al., 2018) to enhance the reachability of a robotic arm. Moreover, a forklift vehicle was utilised\\nto enable the cutter mechanism to reach the various height and depths to harvest oranges in orchards (Lee\\nand Rosa, 2006).\\nIn addition to rigid mechanisms, other mechanisms were also employed for harvesting (Chowdhary et al.,\\n2019). For instance, (Tiefeng et al., 2015) proposed an elephant trunk-inspired mechanism to harvest fruits.\\nCombined soft and rigid mechanisms have been also tested for agroforestry activities which include harvesting\\nas well. (Chowdhary et al., 2019).\\nMotion planning and motion control are also important components of a successful selective harvesting\\nrobotic system. Mghames et al. (Mghames et al., 2020) proposed an interactive motion plan to push occluding\\nstrawberries away in a cluster to reach a target fruit. Pushing actions are encoded by movement primitives\\nand hand-designed features of pushable obstacles. A set of pushing demonstrations is used to train the\\nmotion primitives. Learning from demonstrations (Ragaglia et al., 2018), and imitation learning (Osa et al.,\\n2018) are used in many other contexts. However, their hand-designed features (such as the position of the\\ntarget and occluding strawberries) may limit the generalisation of such a method. Sanni et al. (Sanni et al.,\\n2022) proposed deep-Movement Primitives (d-MP) that do not need any hand-designed features and directly\\nmap visual information into robot movements based on the observed demonstrations. Tafuro et al. (Tafuro\\net al., 2022b) extended d-MP into deep-Probabilistic movement primitives (d-ProMP) in which the model\\ngenerates a distribution of trajectories given a single image of the scene.\\nTo control pushing motions occluded camera views are not suﬃcient and tactile sensings are necessary.\\nMandil et al. (Mandil et al., 2022) proposed a data-driven tactile predictive model which is then used\\nin (Nazari et al., 2022) to proactively control manipulation movements to avoid slip. This framework can be\\nadopted to control pushing actions.\\n2.2\\nEnd-eﬀectors\\nAn end-eﬀector is a tool attached to the wrist of a robotic manipulator to harvest the fruit, either by\\ngrasping or gripping the fruit or its peduncle (attachment), detaching it from the parent plant, and eventually\\ndelivering it to the storage. These end-eﬀectors execute the individual actions either simultaneously (eg:\\ngripping and detaching) or sequentially (gripping/grasping followed by detaching) to perform a successful\\nharvesting operation. The end-eﬀector of a selective harvesting robot is the unit that directly and physically\\ninteracts with crops. Across diﬀerent end-eﬀector technologies for fruit harvesting, the physical interactions\\ninclude (i) gripping/grasping the fruit by its peduncle or fruit body (attachment), (ii) detaching from the\\nplant by pulling, twisting, or cutting the peduncle, (iii) facilitating the fruit transport from the detachment\\nlocation to the storage, and (iv) pushing/parting of fruit in the cluster during detaching action(Xiong et al.,\\n2020a) that is a recently studied functionality.\\nAmong attachment and detachment actions, some end-eﬀectors perform simultaneous gripping and de-\\ntachment of the strawberry peduncle using a parallel jaw mechanism (Hayashi et al., 2010; Hayashi et al.,\\n2014). In such a mechanism, one jaw will be shaped in the form of a cutting blade or is provided with a\\nprovision to attach detaching blades. One such end-eﬀector design makes use of a suction cup to provide an\\nadditional grip by sucking the fruit body to avoid any positional errors during this simultaneous gripping\\nand cutting actions (Hayashi et al., 2010). Another suction-based approach is used by an end-eﬀector which\\nuses a suction head to grip the fruit body and then rotates so that a blade is positioned on the curved\\nopening of the suction head to trim the peduncle(Arima et al., 2004). Instead of using any cutting blades\\nfor trimming the peduncle, the end-eﬀector in (Yamamoto et al., 2014) uses a bending action to detach the\\nstrawberry after gripping the fruit body with a suction head and two-jaw gripper.\\nIn addition, a thermal-based cutting is reported to be used by another end-eﬀector that uses an electrically\\nheated wire on the gripping jaw to cut the peduncle (Feng et al., 2012). In this end-eﬀector, once the fruit\\nbody is gripped by the suction cup to position the peduncle between the two jaws (cutting device), the jaws\\nthen close and trim the peduncle using the heated wire. The end eﬀector developed by Octinion uses a soft\\ngripper to grip the strawberry fruit body and imposes a rotational motion while pulling the strawberry to\\ndetach it from the peduncle (De Preter et al., 2018). The end-eﬀectors above make either a gripping contact\\nwith the fruit body or with the peduncle during the harvesting action. But the end-eﬀector reported in\\n(Xiong et al., 2018) doesn’t grip the fruit body or the peduncle during the harvesting action. Instead, a\\ncombination of three active and passive ﬁngers guides the strawberry into the end-eﬀector housing. Once\\nthe fruit reaches the cutting location, scissor-shaped blades cut the peduncle to detach the strawberry.\\nAfter the detaching operation, the end eﬀector continues to catch/hold the fruit until it is dropped\\nintentionally or safely placed in the designated location by the manipulator. Some end-eﬀectors were reported\\nto have certain ﬁnger arrangements to perform the catching action when the harvested fruits were dropped\\nafter the fruit detachment action.\\nOne such catching provision was provided in the end eﬀector design\\nreported by Arad B et.al. (Arad et al., 2020) for the sweet pepper harvesting robot. It was a soft plastic\\ncoated six metallic ﬁngers arrangement just below the cutting blade assembly, that receives the fruit after\\nthe detachment. Another catching mechanism was proposed by Davidson J et.al. (Davidson et al., 2017) for\\nthe apple harvesting robot. It used a two DOF secondary mechanism with a funnel-like catching end eﬀector\\nwhich will be moved to the dropping position to catch the apple while the primary picking manipulator\\ndetaches and drops the apple. This pick and catch approach was determined to be superior to the conventional\\npick and place approaches as it resulted in a ﬁfty per cent reduction in the harvesting cycle time (Davidson\\net al., 2017).\\nConsidering the diﬀerent options for gripping and cutting, it is always beneﬁcial to avoid applying any\\nforce on the fruit body by the end-eﬀector contact surfaces. Since some fruits are very soft and delicate,\\nthere are higher chances of bruising during such operations. Aliasgarian et al. (Aliasgarian et al., 2013)\\nshowed strawberry fruits are more damaged when exposed to compression forces on their body. Hence, from\\nthe end-eﬀector design point of view, it is recommended to target the peduncle for gripping/cutting actions\\nor to avoid a grip action as demonstrated in (Xiong et al., 2018).\\n2.3\\nPerception\\nFrom traditional computer vision (CV) based to modern state-of-the-art deep neural networks, various\\nmethods exist for fruit detection and localisation of picking points. Traditional or classical CV approaches\\nare typically based on geometric, thresholding, colour, and morphology. Thus, similar to other areas of CV,\\nresearchers have taken advantage of Deep Learning (DL) methods for performance improvement. Some of\\nthe early methods for detecting ripe strawberries relied on colour thresholding in HSI colour map (Rajendra\\net al., 2009).\\nThe authors also used thresholding of diameter for detecting the strawberry stem.\\nThe\\nautomatic thresholding-based algorithm was shown to be more robust by Zhuang et al.\\n(Zhuang et al.,\\n2019). colour-based segmentation was used by Areﬁ et al. (Areﬁ et al., 2011) to segregate the background\\nfrom the fruit blob.\\nAreﬁ et al. (Areﬁ et al., 2011) used colour-based segmentation to remove the background and keep the\\nfruit blob. Instead of directly using colour, colour information can also be used with other features for\\na more robust approach. 3D-parametric model-ﬁtting was used for the localisation of sweet peppers by\\nLehner et al. (Lehnert et al., 2017). Tao et al. (Tao and Zhou, 2017) used geometric features with GA-SVM\\nfor apple classiﬁcation. Areﬁ et al. (Areﬁ et al., 2011) combined the water-shed algorithm to extract the\\nmorphology of tomatoes from colour-thresholded binary images. Zhuang et al. (Zhuang et al., 2019) were\\nable to improve the results obtained by colour segmentation by using iterative-retinax algorithm along with\\nOtsu’s thresholding. Similarly, geometry-based algorithms are among the early contributions in this domain.\\nLi et al. (Li et al., 2020) applied morphological operations for litchi harvesting. The connected component\\nalgorithm was used by Duran et al. (Durand-Petiteville et al., 2017) to identify strawberry blobs. Moving on\\nto geometry-based approaches, Hayashi et al. (Hayashi et al., 2010) relied on extracted geometric features of\\nstrawberries to calculate stem angle w.r.t. to the longitudinal axis. (Tao and Zhou, 2017) used a Fast Point\\nFeature (FPF) histogram, which is a geometric descriptor. The FPF descriptor consists of the parameterised\\nquery of the spatial diﬀerences between a point and its adjacent area which helps in describing the geometric\\nproperties within the K-neighbourhood of the point.\\nWhile threshold, colour, morphology, and geometry-based methods may provide good performance, they\\nlack generalisation and are prone to noise. This is especially true for fruits like strawberries which are not\\nregular in shape and lack symmetry. To improve generalisation, researchers need to engineer handcrafted\\nfeatures. However, with increasing size and variation in datasets handcrafting features become infeasible\\n(O’Mahony et al., 2019). The alternative is to use DL methods which are reviewed next. The most obvious\\nDL-based approach is to use CNNs. Liu et al. (Liu et al., 2018a) combined CNN-based fruit detection with\\ndepth data to localise the fruit in 3D. CNN-based model for strawberry detection was also used by Lamb\\net al. (Lamb and Chuah, 2018) where the network was optimised through image tiling, input compression,\\nnetwork compression, and colour masking. Zhang et al. (Zhang et al., 2018) relied on their CNN-based\\nmodel for tomato classiﬁcation. Instead of using colour images, Gao et al. (Gao et al., 2020) relied on a\\nspectral features-based CNN model for detecting the ripeness and quality of strawberries. Thermal images\\nhave also been used as input to a CNN-based model for bruise detection on pears (Zeng et al., 2020). In\\nrecent years DL has been demonstrated to be superior for tasks such as segmentation (He et al., 2017) and\\nkey-points detection (Cao et al., 2019). Thus, authors in selective harvesting have begun to adopt some of\\nthe DL techniques for fruit perception.\\nLamb et al. (Lamb and Chuah, 2018) used CNN for strawberry detection by optimising the network\\nthrough input compression, image tiling, colour masking, and network compression. Liu et al. (Liu et al.,\\n2018a) used CNN in combination with depth data to calculate the relative 3-D location of fruit. Similarly,\\nZhang et al. (Zhang et al., 2018) used CNN for tomato classiﬁcation. Spectral features with CNN are used for\\nstrawberry quality or ripeness detection (Gao et al., 2020). CNN model is also used for pear bruise detection\\nbased on thermal images (Zeng et al., 2020). CNNs have revolutionised object detection and recognition,\\nhowever for pixel-wise tasks such as semantic segmentation, Regional CNNs (RCNNs) is more appropriate.\\nSa et al. (Sa et al., 2016) relied on a faster RCNN model for bounding box detection of fruit while fusing\\nfaster RCNN, RGB, and Infrared (IR) images.\\nMore recently, Mask-RCNN (He et al., 2017) has been presented as a better alternative to the original\\nRCNN. In selective harvesting also Mask-RCNN has been shown to provide a higher degree of accuracy while\\nperforming a pixel-wise segmentation (Ge et al., 2019). Liu et al. (Liu et al., 2018b) relied on both YOLOv3\\nand mask-RCNN (M-RCNN) for bounding box detection of citrus fruit. M-RCNN with the ResNet-150 as\\nbackbone provided better performance than YOLO-v3. Similarly, Perez et al. (P´erez-Borrero et al., 2020)\\nrelied on M-RCNN for strawberry segmentation for harvesting. Yu et al. (Yu et al., 2019) presented another\\ninstance of M-RCNN used for selective harvesting where features from M-RCNN were used to determine the\\nstrawberry shapes. Afterwards, a geometrical algorithm was used to localise the strawberry picking point.\\nResearchers have extracted features from R-CNNs and combined them with their own algorithm to improve\\nthe localisation of picking points (Ge et al., 2019; Liu et al., 2019). Ge et al. (Ge et al., 2019) ﬁrst used\\nM-RCNN to determine strawberry pixels then, the extracted strawberry pixels were combined with depth\\ndata, and thereafter density-based clustering and Hough transformation were used to develop a richer scene\\nsegmentation. Liu et al. (Liu et al., 2019) combined M-RCNN with the logical green operator to come up\\nwith a more robust cucumber detection. Ganesh et al. (Ganesh et al., 2019) used both HSV and RGB images\\nto enhance the performance of M-RCNN for Orange detection. Yu et al. (Yu et al., 2019) applied M-RCNN\\nto segment strawberry images and then used geometrical features to localise the picking point.\\nOn the other hand, Tafuro et al. (Tafuro et al., 2022a) argue that localisation of picking points is not\\nfeasible by geometrical, statistical, or other such approaches even after fruit segmentation through M-RCNN.\\nInstead, the authors rely on key-point detection normally used for tasks like human pose estimation (Cao\\net al., 2019), and face pose estimation (Zhang et al., 2014) for localisation of picking points.\\nWe rely on the approach by Tafuro et al. for the initial detection and segmentation of strawberries.\\nHowever, 2D detection is not suﬃcient for strawberry harvesting in 3D. As discussed earlier, the depth\\ninformation is not suﬃciently accurate owing to sensor inaccuracies and sunlight. Moreover, in the real-\\nworld more inaccuracies are introduced by camera calibration errors.\\nThus, it is not feasible to simply\\ncombine the 2D localisation with depth information. Similar, to Ge et al. (Ge et al., 2019), we develop our\\nown algorithm to reﬁne and work around the inaccurate depth information obtained from depth sensors.\\nHowever, our advantages over Ge et al. (Ge et al., 2019) and other methods based on M-RCNN discussed\\nabove are two-fold: 1) We rely on M-RCNN-based key-point detection for picking points which gives us much\\nmore robust picking point localisation as compared to handcrafted methods adopted to reﬁne Mask-RCNN\\noutput. 2) We compensate for the lack of precise depth information by carefully ﬁne-tuning the end-eﬀector\\npose by translating the information to two additional cameras at the front.\\n3\\nConcept, Design, and Features\\n3.1\\nSystem overview\\nTo address the current challenges in the autonomous selective harvesting sector an autonomous system for\\nfruit harvesting was designed and developed. The system includes a robotic arm, a novel robotic end-eﬀector\\ndesigned and manufactured for this research, a comprehensive perception system, a mobile platform; and\\nan integrated control system for controlling the robotic arm and end-eﬀector. Figures 1a and 1b show the\\nautonomous fruit picking system and its components during ﬁeld tests in a commercial strawberry growing\\nglasshouse and research strawberry poly tunnels respectively. The robotic arm used in this work is an oﬀ-\\nthe-shelf arm Franka Emika Panda with a 3 kg payload and 7 degrees of freedom. A block diagram of the\\nsystem is shown in Figure 2. In this work, a novel universal picking head (UPH) for fruit harvesting was\\ndesigned and introduced. The design of this picking head was to address the shortcoming of the available\\nsolutions, speciﬁcally for harvesting in dense clusters. We designed, manufactured, and successfully tested\\nthe picking head which has 2.5 degrees of freedom to allow harvesting fruits and manipulating possible\\nocclusions independently. In the next sections, the design of the picking head is discussed in detail.\\nPanda \\nRobot Arm\\ncontainer\\nPicking \\nHead\\n(a)\\n(b)\\nFigure 1: a) The system compromises a robotic arm (Franka Emika Panda), a designed picking head, an\\nRGB-D sensor, and a fruit container. The system and its components including controllers are mounted on\\nthe commercial trolley capable of moving on a rail between the strawberry rows. b) All components of the\\nsystem were reconﬁgured and mounted on a commercial mobile robot to be tested in a diﬀerent strawberry\\ngrowing ﬁeld.\\nOur comprehensive perception system includes an RGB-D sensor and three RGB cameras, a novel dataset,\\nand state-of-the-art algorithms to detect and localise the fruit and determine its suitability for picking. The\\nRGB-D sensor is an Intel Realsense D435i model which is integrated into the picking head design and works\\nbased on eye-to-hand principles. This sensor provides an RGB image of the plant and also a three-dimensional\\npoint cloud to detect fruit, localise the picking point and predict the ripeness of the fruit. The RGB cameras\\nlocated underneath the UPH allow close-range view where the RGB-D sensor loses its view. These sensors\\nare coupled with a novel Mask-RCNN-based algorithm to form the perception system which is discussed in\\ndetail.\\nThe robotic arm controller, UPH controller, RGB-D sensor, and RGB cameras are all connected to a\\nlaptop using either USB or CAN to USB adaptors. The laptop has an Intel Core i7® CPU, with 16 GB\\nram, and runs on Ubuntu 20.04.4 LTS (Focal Fossa). We used ROS Noetic (Robotics Operating System)\\nas a middleware operating system to integrate all algorithms, sensors, robotic arm, and UPH and establish\\ncommunication between them. The system includes a second laptop with a powerful GPU unit (NVIDIA®\\nGeForce® RTX 2070 SUPER) to Handel the high-demand perception tasks. The second laptop is connected\\nto the ﬁrst laptop using an Ethernet connection and communicating through ROS. The system including\\nthe robot, controllers, and other components can be powered by a domestic power plug or provided by a\\nDC power source e.g. a battery. In our ﬁeld test, we tested the system using both methods. The DC power\\nwas sourced from the mobile robot batteries. The system also includes a container to hold the fruit punnets.\\nThe container could be replaced manually after it is ﬁlled. A rough estimate of the cost of the entire system\\n(including the robotic arm, UPH, sensors, and computing system) is around £25k.\\n3.2\\nA conﬁgurable and modular system\\nFull integration of the system allows the robot to continuously detect and harvest ripe strawberries along\\nthe table rows. Similar to the other agricultural robotic harvesters, all sequences are performed in a static\\ncondition, i.e. when a strawberry is detected the mobile platform stops, the system harvests all reachable\\nstrawberries and the mobile platform moves on. The block diagram for the system includes all sub-systems\\nFigure 2: System block diagram demonstrating all sub-systems and other components including; the robotic\\narm, mobile platform, perception, and control system.\\nand components shown in ﬁgure 2 demonstrating the hardware and software architecture of the system.\\nOne of the features of our system in comparison with previous works is the modular design of the\\nsystem.\\nAs can be seen, all main components i.e.\\nrobotic arm, UPH, perception, and mobile platform\\nare independent and could be integrated with other models with minimal development requirements. For\\ninstance, it is possible to use a variety of oﬀ-the-shelf robotic arms depending on the costs and the harvesting\\ncondition such as indoor/outdoor, task space, etc. More importantly, the system is conﬁgurable. In other\\nwords, the system can be conﬁgured for diﬀerent growing conditions such as on-the-rail for glasshouses, on\\na mobile robot for full autonomous harvesting, or on an XY gantry mechanism for vertical farming.\\nFor this work, we integrated and tested our system on two mobile platforms in diﬀerent conditions. First,\\nthe developed system including the robotic arm, UPH, and other components was mounted on Thorvald,\\na four-wheeled mobile robot made by Saga Robotics as shown in Figure 1b. In this case, the mobile robot\\nwas teleoperated remotely using a joystick.\\nThe harvesting test was carried out at strawberry-growing\\npolytunnels at the Riseholme campus of the University of Lincoln. This strawberry-growing facility was\\nestablished for research purposes.\\nThe second layout was a commercial strawberry harvesting trolley mounted on rails which was able to\\nmove between strawberry rows. In the current stage of this work, the trolley is operated manually, i.e. a\\nhuman operator pushed the trolley on the rail. However, the automation of the mobile platform on the rail\\nrequires a simple solution that is widely available and out of the scope of this work. This ﬁeld test was\\nperformed at the Dyson glasshouse strawberry growing facility which is a leading commercial strawberry\\ngrower in the UK as shown in Figure 1a.\\nIn addition to the hardware modularity, the software and control platform of the system is modular and\\nconﬁgurable. The diﬀerent units and control nodes are shown in Figure 2. These units include sensor data\\nacquisition and processing, UPH control, robotic arm control, central control unit, a detection unit, and a\\nripeness assessor. For this project, we used two laptops to separate computationally heavy fruit detection\\nwhich requires a powerful GPU, and robot control which requires high frequency. For both laptops, an image\\nprocessing node is required to provide the correct format as diﬀerent third-party packages are running on\\nboth laptops. These units could be developed independently and integrated depending on the harvesting\\ncondition, used hardware, and other requirements. As most available harvesting technologies were developed\\nfor a speciﬁc growing condition or strawberry variety, they are commercially unavailable. In addition, while\\nthey might have good performance for the condition they were designed for, they perform poorly for diﬀerent\\nconditions. This is important as the high number of combinations of diﬀerent varieties and diﬀerent growing\\nconditions makes it impossible to propose a robotic solution for all of them.\\n3.3\\nSystem work-ﬂow and algorithm\\nThe ﬂowchart of the system algorithm and workﬂow is shown in Figure 3.\\nAs can be seen, the entire\\nalgorithm was implemented in two laptops communicating through Ethernet protocol. The whole system\\nconsists of several control loops on diﬀerent levels, i.e. high-level, mid-level, and low-level. Initially, the\\nmain loop triggers the robot control loop and perception loop. The robot goes to the home position (i.e.,\\na suitable conﬁguration in which the robot looks at the tabletop strawberries as shown in Fig. 1) and\\nperception acquires the data, i.e. images, depths, point cloud, from multiple sensors. After pre-processing\\nthe received data, the perception algorithm detects all strawberries in the ﬁeld of view and publishes their\\ncoordinates through the berry topic. In addition, the perception classiﬁes all detected berries as ”pluckable”\\nor ”unpluckable” which is included in the berry topic.\\nReceiving an ”at least one pluckable berry detected” message, triggers the robot control loop which\\nconverts the coordinates of berries from the camera frame to the robot base frame and commands the\\nrobot end-eﬀector to move to the pre-grasp pose. As transformed coordinates of the picking points of the\\nStrawberries in the robot base frame always contain some level of error, instead of the robot moving directly\\nto the picking point pose, it goes to the pre-grasp pose. The pre-grasp pose is a point with the actual Y and\\nZ coordinate and X − d in the robot base frame where d is a predeﬁned distance of the end-eﬀector from\\nthe berry. If more than one pluckable berries are detected the scheduling algorithm is triggered to determine\\nthe sequence of picking in order to improve the eﬃciency of the system.\\nIn the pre-grasp pose, the targeted berry is detected with at least two of the bottom RGB cameras.\\nUsing the detected coordinates of the targeted berry in the bottom RGB cameras, the errors of the picking\\npoint coordinates are estimated. Using the estimated errors, the picking point coordinates are corrected and\\nthe robot moves to the picking pose. in this stage, the strawberry stem should be located in between the\\ngripping ﬁngers where the cutter is able to cut the stem eﬀectively. In addition, the remaining stem on the\\nberry should not be too long which damages the other fruit in the punnet, or too short where the gripper\\nﬁngers contact the fruit and bruise it. To conﬁrm that the fruit is in the right place, a cutting conﬁrmation\\nalgorithm based on the bottom RGB sensor was developed which is described in detail in Section ??.\\nAfter the conﬁrmation that the fruit is located in the right place, the cutting command is sent and\\nthe stem is cut. In this stage, diﬀerent conditions and scenarios can lead to unsuccessfully cutting and\\npicking. To improve the eﬃciency of the system, and avoid redundant movement of the robot arm, a picking\\nconﬁrmation is performed before moving to place the fruit in the punnet. After the cutting action, the robot\\nmoves back with a pre-deﬁned distance and performs a picking validation as outlined in Section ??.\\nIf the picking is conﬁrmed, the robot goes to punnet pose and places the picked berry in the punnet\\nand the sequence is repeated. Placing poses of the fruit in the punnet are pre-deﬁned points by which it is\\nensured that the fruits are placed evenly in order in the punnet to avoid bruising them.\\n3.4\\nControl and Motion planning\\nAs the space between the strawberry plants and the robot arm is very limited and the environment contains\\na high level of uncertainty, there is a high possibility of collision of the robot arm or the end-eﬀector\\nwith diﬀerent objects. The high possibility of collision and limited task space demands a thorough and\\ncomprehensive motion planning approach. The manipulator’s motion trajectory, velocity, and acceleration\\nshould be rectiﬁed from the beginning through all the way to putting the fruit in the punnet, including\\napproaching the fruit, cutting action, holding, and placing.\\nIn this work, to preserve a collision-free manipulation and prevent dropping/damaging fruit or equipment\\nwe deﬁned a set of n key points through the trajectory of the end eﬀector motion denoted by Pi where\\ni = {0, 1, 2, ..., n}. We assumed that the trajectory of the mobile robot is aligned with the fruit tables, hence,\\nthe robot arm’s base frame is almost the same distance as the fruit tables. In this way, the robot arm’s home\\nposition always is almost the same distance as the fruit tables. Diﬀerent motion planning algorithms and\\ntrajectory/velocity/acceleration proﬁles were employed for each segment of movement to ensure collision-free\\nand eﬃcient manipulation.\\nAt the beginning of the picking process, the end-eﬀector frame is located at P0, i.e. Home Position.\\nThe end-eﬀector frame denoted by Fee : {Oee; xee, yee, zee} is attached to the middle of the gripper ﬁngers.\\nThe end-eﬀector frame coordinates are a deﬁned point that coincides with deﬁned trajectory key points\\nFigure 3: Flowchart of the perception and workﬂow of the system. The algorithm is implemented in two\\nlaptops, one running the ROS nodes/topics and high-level controller and the other a GPU-enabled system\\nfor running the strawberry detection and scheduling system.\\nduring the robot’s movement. When the Fee locate at P0, the system is initialised and the picking process\\nis commenced.\\nWe exploited the Open Motion Planning Library (OMPL) (S, ucan A. et al., 2012) which is a sampling-\\nbased motion planning and Pilz Industrial Motion Planner to plan the manipulator’s movement.\\nMany\\nplanners in OMPL (including the default one) favour the speed of ﬁnding a solution path over path quality.\\nA feasible path is smoothed and shortened in a post-processing stage to obtain a path that is closer to\\noptimal.\\nHowever, there is no guarantee that a global optimum is found or that the same solution is\\nfound each time since the algorithms in OMPL are probabilistic. Pilz Industrial Motion Planner provides\\na trajectory generator to plan standard robot motions like Pint to Point (PTP), Line (LIN), and Circle\\n(CIRC) with the interface of a MoveIt planner. For this work, we used the LIN motion command. LIN\\nmotion planner connects the start point and end point with a straight line and generates a linear Cartesian\\ntrajectory between the goal and starts poses. The planner uses the Cartesian limits to generate a trapezoidal\\nvelocity proﬁle in Cartesian space. This planner generates more accurate movement in Cartesian space with\\na focus on the end-eﬀector trajectory.\\nFigure 4 shows a schismatic diagram of the end eﬀector trajectory ζ(t), attached frames, and key points.\\nFrom P0 to P1, i.e. Grasp Pose, we used OMPL for motion planning as it doesn’t require high accuracy\\nmovement nor a speciﬁc trajectory of movement. We set the speed at the highest level as long as preserves\\nthe safety and stability of the movement. For reach-to-grasp movement, P1 to P2, Pilz Industrial Motion\\nPlanner was employed to ensure the end eﬀector goes through a deﬁned trajectory to minimise disruption\\nof the objects or possible collision. It is the same case for picking and validation action, from P2 to P3.\\nFor placing the strawberry in the punnet OMPL was used, however, the movement acceleration should be\\ncalculated carefully to avoid dropping the harvested strawberry.\\nX \\nF\\nr\\ny r\\nz r\\nO r\\nr\\nX \\nF\\ne\\ny \\nz \\ne\\nee\\nee\\nee\\nFigure 4: The initial (t = 0) and ﬁnal state (t = T) of end-eﬀector trajectory ζ(t) (shown with a green line):\\nFe is the robot’s end-eﬀector frame, the frame Fpg is attached to pre-grasp and post-grasp points, and a\\ngrasping conﬁguration Fp is shown with a frame attached to the picking point. Also, a frame is attached to\\nthe container as the ﬁnal goal of the end-eﬀector is shown as Fg. All frames are expressed using the inertial\\nglobal frame Fr.\\nAssuming that the mass of the strawberry to be gripped in the end eﬀector is about 50 grams. This\\nis higher than the average mass value recorded during our ﬁeld studies (Rajendran Sugathakumary et al.,\\n2022). For peak forces (FC) about 22.53 N, coeﬃcient of friction of 0.3, and safety factor of 2, under dynamic\\nconditions, using Equation 1 the end eﬀector would be able to handle a 50 g strawberry for a manipulator\\nacceleration of up to about 50 m/s2.\\nFc = m.(g + a).S\\nµ\\n(1)\\nwhere; ’Fc’ is the maximum gripping force (N), ’m’ is the mass to be handled (Kg), ’g’ is the acceleration\\ndue to gravity (m/s2), ’µ’ is the coeﬃcient of friction, and ’S’ is a factor of safety.\\nTo place the harvested fruit in the punnet six key points were deﬁned with respect to the frame attached\\nto the punnet Fg. These points were selected in a way that distributes the fruits in the punnet evenly and\\navoids bruising or damaging them. From placing point to the home position, again OMPL with the highest\\npossible speed was used to increase the eﬃciency of the system\\n3.5\\nHarvesting sequence planning\\nselecting a berry as the target for picking among many possible pickable fruits is important to increase the\\nsuccess rate and reduce possible occlusions. Picking a free fruit that occludes other pickable fruits, not only\\nnormally has a higher success rate, but also removes the possible occlusion. There are notable studies on\\nsequence planning for robotic harvesting that proposed near-optimal solutions (Kurtser and Edan, 2020).\\nIn this work we mainly implemented and tested two methods for simplicity and reducing the amount of\\ncomputation to determine the target berry as described in the followings:\\n1- Target Berry Selection Using Min/Max:\\nThe min-max algorithm attempts to ﬁnd the maximum of the minimum distances among all the bounding\\nboxes of the detected berries. Considering that there are N berries detected in the top camera image view,\\nthe minimum of all the distances between each berry with respect to all other detected berries are calculated\\nas:\\ndmin\\ni\\n= min(di,1, di,2 . . . di,j); ∀i ̸= j\\n(2)\\nThen the maximum of these minimum distances is taken as follows:\\ndtarget = max(dmin\\n1\\n, dmin\\n2\\n. . . dmin\\nN\\n, )\\n(3)\\nwhere dtarget represents the most isolated berry in a cluster which is scheduled to be harvested ﬁrst. This\\nensures that the diﬃcult-to-reach berries are harvested later which aids in avoiding damage to other berries\\nwhile trying to reach the diﬃcult ones.\\n2- Coordinate based sorting:\\nAlthough the previous method gives a systematic tool to select the target berry, it was noticed that it is\\nmore complicated to achieve a reasonable success rate. One reason is that by changing the viewpoint of\\nthe RGB-D camera, the targeted berry might change without a change in reality. The more eﬃcient and\\npractical approach could be sorting berries based on their coordinate in the image frame e.g. left-to-right or\\nright-to-left depending on the direction of moving of the mobile platform. In this way, the mobile platform\\ndoesn’t have to go forward and backwards to harvest all berries.\\n4\\nUniversal picking head\\n4.1\\nconcepts and requirement of a universal picking head\\nPicking fruits in diﬀerent growing conditions is a challenging problem for selective harvesting technology, as\\nit is diﬃcult to design and build an eﬀective robotic device (known as an end-eﬀector or picking head) that\\nis able to deal with complex picking operations. A human’s hand enables dexterous manipulation of fruit\\nwith 27 degrees of freedom, and over 80 per cent of the grasping information can be encoded into just 6\\nEigen grasps. In contrast, conventional robotic end-eﬀectors are customised for speciﬁc applications, such\\nas pick-and-place operations in industrial environments (Jarrass´e et al., 2014).\\nCurrently, there are two types of picking heads available for robotic harvesting of high-value crops: (i)\\na picking head having a parallel jaw gripper, which may not be suitable for all types of crops, and (ii) a\\npicking head that has a customised design for picking particular fruit in a very speciﬁc picking scenario,\\nwhich is only suitable for a speciﬁc type of crop or method of harvesting. Consequently, the eﬀectiveness of\\ncommonly available robotic picking heads is limited, as diﬀerent robotic picking heads may be needed for\\ndiﬀerent crop types. Some robotic picking heads are used to pick soft fruits such as strawberries. Some of\\nthe robotic picking heads that are currently available for picking strawberries are cup-shaped picking heads,\\nwhich have opening parts that locate the peduncle of a strawberry and position the strawberry in front of\\ncutting scissors in order to harvest the strawberry. The cutting action causes the strawberry to detach from\\nthe plant and fall into a punnet for collecting the strawberries. In this example, the picking head does not\\ndirectly touch the ﬂesh of the strawberry, which minimises bruising. However, as the strawberry falls from\\na height into the punnet, the harvesting can inadvertently cause damage/bruising to the fruit.\\nFurthermore, fruit placement within the punnet is not controlled, which may result in uneven distribution\\nof the fruit in the punnet (which may also cause damage to fruit that are below other fruit). Similarly, the\\ndesign of the cup-shaped picking head, and the design of other types of picking heads, may not be suitable\\nfor harvesting crops that grow in dense clusters. The present design has therefore identiﬁed the need for\\nan improved apparatus for the automatic detection, selection, and harvesting of crops that grow in dense\\nclusters.\\n4.2\\nPeduncle gripping and cutting force for end-eﬀector design\\nTo develop an end-eﬀector solution that attempts to detach strawberries by targeting the peduncle, it\\nis essential to understand certain physical properties of the peduncle. This includes the estimate of the\\nrequired cutting force and the gripping force that can be applied to the peduncle. Knowing the cutting\\nforce while using a particular cutting blade proﬁle, gives insight into a better selection of actuation systems\\nto provide the required cutting force. Moreover, the practice of using oﬀ-the-shelf blades takes away the\\nneed of investing eﬀort to design optimum blade proﬁles. Rather such blades can be directly used or can\\nbe custom-made according to the standard proﬁle. If the end-eﬀector is designed in such a way as to use\\ninterchangeable blades, replacing the blades would be easier during worn-out situations. Hence it is wise to\\nuse cutting blades with a standard proﬁle and in an interchangeable conﬁguration in the end-eﬀectors. For\\nthis work, a comprehensive study on the gripping and cutting forces of strawberry peduncle was carried out\\n(Rajendran Sugathakumary et al., 2022).\\nThis study intended to estimate the limit of the gripping force that can be applied to the strawberry\\npeduncle without crushing it. To understand this force limit, experiments were conducted by applying com-\\npression force (analogous to the gripping force) to the peduncle specimens using a Universal Testing Machine\\n(UTM). The peduncles of ripe strawberries of both varieties were selected for preparing the specimens. 15\\nspecimens of each variety were prepared so that the peduncle was 10 mm in length and were trimmed at\\na distance of 10 mm from the top surface of the ripe strawberry fruit. This specimen measurement can\\nsimulate the situation where an end eﬀector grips the peduncle within 10-20 mm from the top surface of\\nthe strawberry top surface during harvesting. The specimen diameter varied from 1.40 mm to 2.22 mm for\\nKatrina with a mean and standard deviation of 1.75 mm and 0.24 mm. And for Zara, the diameter varied\\nfrom 1.43 mm to 2.33 mm with a mean and standard deviation of 1.76 mm and 0.25mm.\\nIn addition, we studied the force required to cut the peduncle of a ripe strawberry using a standard blade.\\nAnd as an extension to this, the variation of this force at diﬀerent cutting orientations was also studied.\\nThe proﬁle of the selected cutting blade was studied using a scanning electron microscope. The blade had a\\ndouble bevel cutting edge with a blade angle of 16.60 and a thickness of 0.22 mm. We studied the cutting\\nforce variation for 00,100, 200, 300 inclinations. 15 peduncle samples from both Zara and Katrina varieties\\nwere prepared for this study, i.e., 15 samples from each variety for each orientation of cut. These specimens\\nwere prepared by trimming the peduncle at a length of 30 mm from the top surface of the ripe strawberry.\\nDuring the experimental trials for studying the limit of the gripping force, all tested specimens showed a\\ncommon force proﬁle under compression load. While applying compression load to the specimen, there is a\\ngradual increase in the resistive force to a certain point, and from then it shows a sudden drop. After then,\\nthe specimen gets squeezed completely on further application of compression load. It has been noticed that,\\nafter the drop in the resistive force, the specimen goes into permanent deformation, and ﬁnally leads toward\\ncomplete squeezing. So this peak force (FC) before the drop is considered the point of interest. The trend of\\nthis force (FC) can be studied to limit the gripping force on the peduncle such that there is a lesser chance\\nof squeezing the peduncle during the gripping action. The squeezing or crushing of the peduncle during the\\ngripping action can result in the detached strawberry falling oﬀ from the grip during the harvesting process.\\nConsidering the peak forces, we determined that the lowest of these peak forces (FC) recorded is about\\n26.83 N and 22.53 N for Katrina and Zara respectively. This means that at these lowest values of compression\\nforce (analogous to gripping force), the respective test specimen went into permanent deformation before\\nsqueezing. Hence if we allocate a factor of safety of 2 to the lowest of these two values of forces (26.83 N\\nand 22.53 N), the gripping force should be limited to around 10 N.\\nIn addition, while analyzing the force values recorded during the cutting trials, again a common force\\nproﬁle has been noticed for all the tested specimens. In the proﬁle, there is an increase in force value during\\nthe cutting action but with two sudden drops after two force peaks (FP 1 and FP 2). After the second peak\\nforce, there is a ﬂat proﬁle followed by a sharp rise in force after a point (E). This sharp increase happens\\nwhen the blade touches the peduncle supports after cutting the peduncle oﬀ. So from the force proﬁle for\\neach specimen, the maximum of the two peak forces (FP 1 or FP 2) is taken as the peak cutting force (FP )\\nrequired for that specimen.\\nFrom the force values recorded at diﬀerent cutting orientations, it has been noticed that the mean cutting\\nforce shows a relatively lowest value at 300 orientation compared to other studied orientations. At 300 cutting\\norientation, the maximum of the peak cutting force (FP ) recorded is about 7.20 N for Katrina, and 5.80 N\\nfor Zara. And hence, with a factor of safety 2, the cutting force requirement can be approximated to 15\\nN which could be considered suﬃcient to cut the strawberry peduncle at 300 orientation. Also, this force\\nwould be suﬃcient to handle other cutting orientations studied. We exploited these results to optimise the\\ndesign of the end-eﬀector and increase the harvesting success rate.\\n(a)\\n(b)\\nFigure 5: a) End-eﬀector overall view and its components. The end-eﬀector compromises an RGB-D sensor\\nmounted on top of it for a better wide view, three RGB sensors for close-range view, the ﬁrst pair of\\nﬁngers (Separators) for occlusion removal, second pair of ﬁngers for gripping and holding strawberry stem\\n(grippers), and a cutting mechanism for cutting the stem (Cutter). b) End-eﬀector internal design and\\ncomponents. It includes two independent actuators controlling grippers, separators, and cutter providing 2.5\\ndegrees of freedom. The protector separates actuators from the power transmission mechanisms for better\\nheat reduction and dust and water isolation.\\n4.3\\nEnd-eﬀector design\\nThe proposed robotic end-eﬀector for fruit harvesting, comprising: a vision system for identifying the location\\nof the ripe fruits on the plant; the ﬁrst pair of ﬁngers for moving any objects that at least partly occlude the\\nidentiﬁed ripe fruit on the plant (Separators); the second pair of ﬁngers for gripping a stem of the identiﬁed\\nripe fruit (Grippers); and a cutting mechanism for cutting the stem of the identiﬁed ripe fruit when the stem\\nis gripped between the second pair of ﬁngers (Cutter), wherein a portion of the stem that remains attached\\nto the fruit remains gripped by the grippers after the stem has been cut. The design and components of the\\nuniversal picking head are shown in Figures 5a and 5b.\\nIn general, the end-eﬀector described herein beneﬁts from 2.5 degrees of freedom, which is higher than\\nthose of available picking heads. This added degree of freedom allows the end-eﬀector to deal with complex\\npicking scenarios where the available picking heads fail. The end-eﬀector beneﬁts from an eﬀective combi-\\nnation of actuation systems and sensors to resolve the limitations of currently available picking heads. The\\nend-eﬀector includes three separate movements (moving objects, gripping a stem, and cutting a stem) that\\nare actuated using two actuators. This is useful as the ability of the end-eﬀector is increased without also\\nsigniﬁcantly increasing the complexity or component count of the device. The actuators and protector design\\nfor the picking head are shown in Figure 5b\\nThe presented techniques advantageously enable ripe fruit to be harvested without bruising or damaging\\nthe fruit. These techniques are particularly advantageous for harvesting fruit that grows in dense clusters,\\nsuch as strawberries. Strawberry is an example – and not limited – fruit that may be harvested using the\\nrobotic end-eﬀector of the present techniques. More generally, the presented techniques may be used to\\nharvest diﬀerent types of fruit and vegetable crops, including those which grow individually and those which\\ngrow in clusters.\\nThe robotic end-eﬀector comprises a ﬁrst actuation mechanism for controlling the actuation of the ﬁrst\\npair of ﬁngers. Thus, a dedicated actuation mechanism is used to control the movement and operation of the\\nﬁrst pair of ﬁngers which allows one degree of freedom for manipulating the cluster and removing possible\\nocclusion independent from griping and holding the fruit.\\nResponsive for receiving the location of an object that at least partly occludes a fruit, the ﬁrst actuation\\nmechanism controls the ﬁrst pair of ﬁngers to push away the object, by increasing the separation distance\\nbetween the ﬁrst pair of ﬁngers. Thus, the ﬁrst pair of ﬁngers are closed together when the end-eﬀector is\\nbeing used to image a plant and identify ripe fruits, and/or when it is moving towards an identiﬁed ripe\\nfruit. The ﬁrst pair of ﬁngers are moved further apart when an object that at least partly occludes fruit\\nneeds to be moved away so that the fruit can be better seen (to determine if it is suitable for harvesting)\\nand/or so that the second pair of ﬁngers can grip the stem of the fruit.\\nThe cutting mechanism is located in proximity to the second pair of ﬁngers, such that when the stem\\nof the identiﬁed ripe fruit is cut by the cutting mechanism, the second pair of ﬁngers continues to grip a\\nportion of the stem that remains attached to the fruit. In other words, when a cutting operation performed\\nby the cutting mechanism is complete, the fruit is not immediately dropped into a container for collecting\\nthe harvested fruit. Instead, the fruit continues to be gripped, via a portion of the stem that is still attached\\nto the fruit, by the second pair of ﬁngers. This is advantageous because the robotic end-eﬀector may be\\ncontrolled to gently place the harvested fruit in a container.\\nThe second pair of ﬁngers release their grip on the portion of the stem that remains attached to the fruit\\nwhen the end-eﬀector is close to the container. The cutting mechanism may be partially or fully covered or\\nencased for safety reasons, i.e. to avoid any risk of a human operator being able to come into contact with\\nthe cutting mechanism.\\nThe second pair of ﬁngers is moved by a second actuation mechanism which also moves the cutting\\nmechanism. As the cutting mechanism is only operated when it is conﬁrmed that the target fruit is located\\nin between the second pair of ﬁngers, advantageously a single actuation mechanism is used to control both\\nthe second pair of ﬁngers and the cutting mechanism, thereby reducing complexity and the number of\\ncomponents needed to control the end-eﬀector.\\nThe vision system comprises a depth sensor for generating a three-dimensional map of the plant. The\\nvision system uses the three-dimensional map to identify the location of ripe fruits on a plant and any objects\\nthat at least partly occlude the identiﬁed ripe fruits. The depth sensor is an RGB-D (red-green-blue-depth)\\ncamera.\\nFurthermore, The vision system includes three RGB image sensors for capturing images of the\\nfruit/cluster of fruits at the bottom of the end eﬀector in the vicinity of the second pair of ﬁngers to\\nenable a better view in close range. Having the RGB sensors in the vicinity of the second pair of ﬁngers is\\nadvantageous because the sensors capture images of the fruit or cluster of fruits at the fruit level, whereas\\nother sensors of the vision system may view the fruit from a diﬀerent perspective/angle. This also reduces\\nthe risk of every sensor of the vision system being occluded during the picking process, i.e. it provides some\\nredundancy in the vision system.\\nThe eﬀective conﬁguration of RGB and RGB-D sensors helps to eﬃciently detect and localise the ripe\\nfruits. In addition, the combined sensory information can be used to estimate the size, weight, and sort\\nquality of the fruits to be picked. Also, they are further used to control fruit picking and occlusion removal\\nactions.\\n5\\nPerception for selective harvesting robotic system\\nStrawberries are grown in dense clusters and they come in diﬀerent conﬁgurations and wide varieties with\\nvarying shapes. Moreover, the asymmetric and irregular nature of the stems coming out of the fruit makes\\nit diﬃcult to localize the picking point. Commercially available depth sensors are designed for large objects\\nunder controlled lighting conditions.\\nInsuﬃcient quality of depth-sensing technologies makes strawberry\\npicking point localization on stem intractable. Our depth sensing, namely RealSenhas 435i, which is widely\\nused in robotics, has deteriorated performance under bright sunlight in farm conditions. In addition, it is\\ndesigned to work best for distances larger than 50 [cm] where the preciseness drops to 0 for distances below\\n15 [cm]. As picking strawberries by grasping them contributes to their bruising, we considered picking by\\ngriping and cutting the fruit stem requiring picking point (PP) localization. However, localizing the picking\\npoint in the depth image is challenging because of the low resolution of RealSense 435i, in particular where\\nthe distance is below 15 [cm].\\nTraditional methods rely on the color-based (Areﬁ et al., 2011), geometry-based, or shape-based (Li et al.,\\n2020) methods. Strawberries are neither very symmetrical nor their orientation is ﬁxed making an accurate\\nassumption, or shape-based prediction about their stem location diﬃcult. Instead, we take advantage of the\\nDeep Learning (DL)-based strawberry segmentation and key-point detection method proposed by Tafuro et\\nal (Tafuro et al., 2022a). We use the key points to understand the orientation of the strawberry and pose\\nthe end-eﬀector. The key point also localizes the picking point to a reasonable accuracy. However, due to\\nthe very thin cross-section of a strawberry stem, this cannot always exactly localize the stem giving rise to\\ninaccuracies in the depth perception of the picking point.\\nIn addition, we address the lack of precise depth sensing and ﬁne-tuning the localization in close prox-\\nimity by a novel camera conﬁguration (short and medium-distance focused cameras) and a combination of\\nlocalization for short and medium distances. The system uses an Intel Realsense D435i depth-sensing cam-\\nera as the main sensing system. The device is placed at the back and top of the end eﬀector which allows\\nsuﬃcient distance from the cutting action for the depth sensor to be reasonably accurate. In addition to\\nthe Realsense camera, there are three RGB cameras placed at the front bottom of the end-eﬀector. At this\\nclose range, it is also not feasible to calibrate the RGB cameras as a stereo system. Instead, we carefully ﬁnd\\nthe project of 3D coordinates of the strawberry segmentation into 2D pixel coordinates of the front RGB\\ncameras. Then, based on the coordinates of the strawberry bounding box visible in each image we trained\\nan AI-Based model to make meticulously adjust the position of the end-eﬀector to the ﬁnal cutting position.\\nIn addition to localizing the fruit and picking point, the perception should be able to determine the ripeness,\\nsize, and shape of the fruit and determine whether it is suitable for picking or not. we present two novel\\ndatasets of strawberries annotated with picking points, key-points (such as the shoulder points, the contact\\npoint between the calyx and ﬂesh, and the point on the ﬂesh farthest from the calyx), and the weight and\\nsize of the berries. We performed experiments to predict if the fruit is suitable for picking or not.\\nIn contrast to the existing works in which classic CV methods are used to determine picking points and\\nsuitability for picking, our approach includes SOTA MRCNN models. We collected two datasets to train our\\nmodels: Dataset-1 is collected at a new 15-acre table-top strawberry glasshouse in Carrington, Lincolnshire,\\nwhich is the latest addition to SOTA Dyson Farming’s circular farming system (Dyson, 2022); Dataset-2\\nhas been derived from the Strawberry Digital Images (SDI) (P´erez-Borrero et al., 2020). Dataset-1 is a\\nnovel dataset that presents strawberry dimensions, weights, suitability for picking, instance segmentation,\\nand key-points for grasping and picking action. The main purpose of this dataset is to facilitate autonomous\\nrobotic strawberry picking.\\nFor each strawberry, the dataset presents ﬁve diﬀerent key-points: the picking point (PP), the top, and\\nbottom points of the fruit, the left grasping point (LGP), and the right grasping point (LGP). While the PP\\nindicates the position on the stem where the cutting action has to be performed, the left and right grasping\\npoints can provide a reference to the end eﬀector for the grasping action. In addition, the dataset also\\ncontains annotation for instance segmentation for each of the strawberries. To determine the suitability of\\nstrawberries for harvesting each strawberry is labeled as ‘pluckable’–ready to be picked– or ‘”unpluckable”’–\\nnot to be picked–. ”unpluckable” strawberries include unripe, semi-, and over-ripe or rotten berries. The\\n‘pluckable’ category includes strawberries that are nearly ripe and perfectly ripe. The dataset contains a\\nset of 532 strawberry sets. Each set has three colors, depth, and point cloud data of the same strawberry\\ncluster from diﬀerent distances. The farthest image captures the entire cluster whereas the nearest image\\nfocuses on one target strawberry in the cluster. In total, this dataset includes 1588 strawberry images. All\\nthe images have been captured with Intel Realsense RGB-D sensor D435i.\\nDataset-2 is an enhancement of the SDI dataset (P´erez-Borrero et al., 2020). SDI dataset contains a total\\nof 3100 images. These are dense strawberry clusters that contain an average of 5.8 strawberries per image.\\nWe carefully annotated 10999 berries each with 5 diﬀerent key-points. Moreover, we labeled ”pluckability”\\n(i.e. suitability to be picked) of all the strawberries. The strawberries not annotated for key-points are\\neither severely occluded or are in an early ﬂowering stage where a meaningful annotation is not possible.\\nmore details on the datasets and proposed methods were published previously and can be reached in (Tafuro\\net al., 2022a).\\n5.1\\nSegmentation, Key-points and Pluckable Detection\\nOur proposed approach includes Detectron-2 (Wu et al., 2019) for segmentation and key-points estimation.\\nThe Detectron-2 model is based on MRCNN (He et al., 2017) and has become the standard for instance\\nsegmentation. It also has an added capability of key-points detection for human pose estimation. We adapted\\nthis key-points detection method integrated within Detectron-2 to estimate the strawberry key-points. The\\ndatasets’ key-points, segmentation masks, and strawberry categories (‘pluckable’ and ‘”unpluckable”’) are\\nconverted to MSCOCO JSON format (Lin et al., 2015). This MSCOCO JSON is the default format for\\nfeeding data into Detectron-2. It is also essential to recalculate the bounding box. Without the key-points,\\nthe bounding box aligns to the extremities of the segmentation mask. However, the PP key-point lies outside\\nthe segmentation mask of the strawberries and thus outside the bounding box. Because of the nature of\\nMRCNN, a key-point outside the bounding box is not detectable. Thus, the bounding boxes are expanded to\\naccommodate all the key-points. We performed experiments with three backbone networks for Detectron-2,\\nR50-FPN, X101, and X101-FPN. ResNeXt (Xie et al., 2017) (X101-FPN and X101) is a more recent network\\nthat was introduced as an improvement to ResNet-50 (R50-FPN). Section 6.1 discusses the results in detail.\\n5.2\\nPerception Setup\\nWe use two laptops to control the entire system and run the perception system. The ﬁrst laptop runs the\\nROS nodes as shown in Figure 2 (left). The second laptop with Nvidia GPU runs the Detectron node as\\nshown in Figure 2 (right). The vision system consists of three cameras: An Intel Realsense d435i color and\\ndepth-sensing camera and three colors (RGB) cameras. In robotic perception, depth-sensing cameras are\\nessential for the 3D localization of the target and for generating a point cloud. However, most commercially\\navailable depth cameras including Realsense d435i are not suitable for close proximity sensing (≤ 15 cm).\\nThus, for the depth sensing to be feasible the Realsense camera is mounted on the back and top of the\\nUPH 5a. However, due to the positioning of the Realsense, the berries are occluded by the picking head\\nin close proximity to maneuvering. So, to compensate for the lack of depth sensing and occlusion in close\\nproximity maneuvering three RGB cameras are additionally mounted for ﬁne-tuning the trajectory, cutting\\nconﬁrmation, and picking success validation. Considering that the purpose of these RGB cameras is error\\nreduction instead of primary strawberry detection, these cameras were mounted ﬁrmly on the front bottom\\nof the picking head with a 25 mm horizontal distance between them.\\n5.3\\nPerception pipeline and approach\\nThe berry plants are on both sides of a lane of strawberry farms with a tabletop system. Hence, our robot,\\nsimilar to human pickers, picks the ripe fruits on one table on one side before picking the fruits on the other\\nside. This simpliﬁes motion planning as collision is checked for one side of the row of tabletop strawberries.\\nThis was achieved using joint constraint in the robot arm planning. Let xtop\\nc\\n, xleft\\nc\\n, xmiddle\\nc\\n, and xright\\nc\\nbe\\nthe coordinates of strawberry in the top (Intel RealSense), left, middle, and right color cameras respectively.\\nNotice, c ∈ x, y, z for xtop\\nc\\ni.e. 3D, c ∈ x, y for xright\\nc\\nand xleft\\nc\\ni.e 2D. Ptop is the camera projection matrix\\nof the top camera for translating an image or pixel coordinates to camera link coordinates. This consists\\nof camera intrinsic (Ktop) and extrinsic (Rtop, ttop) parameters, where R and t stands for rotation and\\ntranslation respectively. T top is the camera calibration parameter/matrix which translates the camera link\\ncoordinates to the robot base frame. K is the intrinsic parameters of the left, middle and right color camera.\\nT left, T middle, and T right are the camera calibration parameters/matrices for projection from the robot base\\nframe to the camera coordinate. The sequence of steps taken below for the picking action is shown in Figure\\n2 and described below:\\n• At the home position, all strawberries are detected in the top (RealSense) camera image frame\\nand scheduled. The depth estimation for the picking point is not reliable for something as thin as\\nthe strawberry stem. Thus the 2D segmented strawberry pixels are used as binary masks on the\\ndepth image to ﬁlter the depth pixels belonging to the strawberry. Further, depth pixels indicating\\ndepths less than 20cm and more than 50 cm are ﬁltered out. The minimum distance of the gripping\\npoint from Real-Sense is roughly 20cm. Clearly, any strawberry pixel should be more than 20cm.\\nFurther, the initial position of the robot cannot be more than 50 cm from the berry due to the farm\\nstructure. Then we take the average value of the remaining strawberry depth pixels which gives us\\na more reliable 3D coordinate xtop\\nc\\n• The scheduled berry coordinate is transformed from the top image or pixel coordinate to the robot\\nbase frame using the camera calibration matrix\\nXbase\\nc\\n= T top\\nbasePtopxtop\\nc\\n(4)\\nwhere P top = [Rtop, ttop]Ktop\\n(5)\\nBasically, the pixel coordinate is ﬁrst transformed to the camera link coordinate using camera\\nintrinsic and extrinsic parameters (Eq. 5) and then the camera link is transformed to the robot base\\nframe (Eq. 4).\\n• Using careful calibration, the targeted berry coordinate Xbase\\nc\\nin the top camera is back-projected\\nto the bottom left, middle and right camera using the camera intrinsic (Kleft, Kmiddle, Kright) and\\ncamera calibration parameters (T left, T middle, T right).\\nxright′\\nc\\n= KrightT base\\nleft Xbase\\nc\\n(6)\\nxmiddle′\\nc\\n= KmiddleT base\\nmiddleXbase\\nc\\n(7)\\nxleft′\\nc\\n= KleftT base\\nrightXbase\\nc\\n(8)\\n• This back projection (Eqs. 6, 7 and 8) helps in establishing a target berry association in the top\\nand bottom cameras at the robot home position based on image plane position error.\\nγleft = xleft\\nc\\n− xleft′\\nc\\n(9)\\nγmiddle = xmiddle\\nc\\n− xmiddle′\\nc\\n(10)\\nγright = xright\\nc\\n− xright′\\nc\\n(11)\\nγ is the error of the projected bounding box and the bounding boxes in the bottom cameras. We\\nconsider the berry with the lowest γ the targeted berry in each camera view.\\nThe error arises\\nmainly from inaccuracies in the depth perception of the RealSense camera as well as errors in\\ncamera calibration.\\n• From this point on, for any arm movement, this berry association is identiﬁed in bottom cameras\\nfor later ﬁne-tuning, grasp alignment, and cutting conﬁrmation.\\n• The UPH is then moved to a pre-grasp pose which is at a ﬁxed distance D from the berry. This\\nmeans the arm is moved by (Xbase\\nz\\n- D) along the depth axis while aligned to the strawberry picking\\npoint in X and Y coordinates (Xbase\\nx′y′ ).\\n• Once the arm is in the pre-grasp pose, the UPH gripping point is adjusted based on the estimated\\nerrors calculated. The goal here is to move the gripper to the picking point (Xbase\\nz\\n) based on initial\\ndepth sensing. However, the X and Y alignment is continuously ﬁne-tuned, where the goal is to\\nlocate the grasp pose of the strawberry stem in between the gripper ﬁngers.\\nDue to errors in depth sensing and camera calibration, the gripper cutting point does not align\\nperfectly with the strawberry picking point. The novelty of the proposed method is that we back-\\nproject the 3D coordinate to 2D coordinates in the front color cameras.\\nBy assuming the fruit\\nwith the lowest γleft and γright as the targeted fruit, we are able to associate the same berry with\\nthe bottom color cameras and ﬁne-tune the X, and Y alignment based on more reliable strawberry\\ncoordinates in the bottom cameras. This enables the system to perfectly align with the strawberries\\non X,Y-axis.\\nFigure 6: The harvesting sequences to remove a possible occlusion and pick fruit. The separator ﬁngers\\npenetrate in between the occluding fruits and by opening remove the occlusion and make way for gripping\\nﬁngers to grasp the targeted fruit’s stem and cut it.\\n6\\nField experiments results\\nIn order to validate the integrity of the system and verify its accuracy, ﬁeld tests were carried out at two\\ndiﬀerent sites. First, experiments were conducted at Berry Gardens strawberry poly-tunnels at the Riseholme\\ncampus of the University of Lincoln 2021. The second ﬁeld test was carried out at a commercial strawberry\\ngrowing glasshouse facility owned by Dyson Farming in Carrington, England in 2022. Figure 1a and 1b\\nshow the system harvesting at Dyson glasshouse and Berry Gardens respectively. As can be seen at Berry\\nGardens the harvesting system was mounted on Thorvald mobile robot, and at Dyson glasshouse, it was\\nmounted on a commercial harvesting trolley.\\nThe Berry Gardens poly-tunnels is a research strawberry farming facility with two main strawberry\\nvarieties Driscoll Zara and Driscoll Katrina. The variety Zara has a longer calyx as compared to the Katrina\\nvariety which makes it more complicated for robotic harvesting. The mean diameter for the Katrina variety\\nis 1.75 mm with a standard deviation of 0.24 mm, and for the Zara is 1.76 mm with a standard deviation\\nof 0.25 mm (Rajendran Sugathakumary et al., 2022).\\nThe strawberry variety at Dyson glasshouse is a\\ncommercial variety that is grown and available widely.\\nFigure 6 demonstrates harvesting sequences including removing possible occlusion using separator ﬁngers.\\nThe separator ﬁngers penetrate in between the occluding fruits and by opening remove the occlusion and\\nmake way for gripping ﬁngers to grasp the targeted fruit’s stem and cut it. In addition, the separator ﬁngers\\ncan be used to remove detection occlusion as well to allow the perception system to detect all possible fruits.\\n6.1\\nSegmentation, Key-points and Pluckable Detection Results\\nTable 1 summarises the results for segmentation and key-points detection of strawberries for both the datasets\\nwith Detectron-2 (Wu et al., 2019).\\nAlthough, our results demonstrate diﬀerent backbones used in our\\nexperiments can produce consistent results across the dataset, ResNeXt based model performs better than\\nResNet-50 based model. The ﬁrst two columns of Table 1 show segmentation Average Precision (AP) values\\nfor pluckable and ”unpluckable” berries separately. The sub-columns show AP for Intersection over Union\\n(IoU), and thresholds of 0.5, 0.7, and 0.9. The standard practice is to consider IoU threshold 0.5 (He et al.,\\n2017), however, we also show up to IoU threshold 0.9. Using Dataset-2, our proposed models yield decent AP\\nvalues for both pluckable and ”unpluckable” strawberries at IoU threshold 0.5. However, as shown in Table\\n1, the ‘”unpluckable”’ berries in Dataset-2 signiﬁcantly outnumber the ‘pluckable’ berries. This results in\\nbetter segmentation performance of ‘”unpluckable”’ berries. The performance drops signiﬁcantly for stricter\\nthresholds 0.7 and 0.9. This dataset represents berries in very dense clusters and thus Dataset-2 is a very\\nTable 1: Segmentation and key-points detection results.The sub-columns show AP for Intersection over\\nUnion(IoU), and thresholds of 0.5, 0.7, and 0.9.\\nDataset\\nBackbone\\nSegn Pluckable\\nSegn ”unpluckable” Key-points Pluckable Key-points ”unpluckable”\\n0.5\\n0.7\\n0.9\\n0.5\\n0.7\\n0.9\\n0.5\\n0.7\\n0.9\\n0.5\\n0.7\\n0.9\\nDataset-1\\nR50-FPN\\n93.32 90.97 83.55 59.46 53.61\\n42.91\\n91.27 89.10\\n81.90\\n51.36 46.20\\n37.30\\nX101\\n94.19 92.83 88.70 61.12 56.22\\n45.64\\n92.71 91.40\\n87.74\\n61.26 56.52\\n46.84\\nDataset-2 X101-FPN 71.12 64.70 43.24 76.83 74.52\\n68.79\\n64.32 58.93\\n39.92\\n73.26 71.39\\n66.46\\nX101\\n72.12 66.84 47.86 78.09 76.65\\n70.30\\n59.29 54.40\\n42.12\\n74.67 71.45\\n65.30\\nFigure 7: We introduce two novel datasets targeted toward the robotic selective harvesting of strawberries.\\nThe datasets provide instance segmentation, ”pluckability”, key-points, and weight information about the\\nstrawberries.\\nchallenging dataset and has the potential to further advance the research in selective harvesting. On the\\nother hand, Dataset-1 shows very reliable AP values for pluckable strawberries for both the backbones across\\nIoU thresholds. With IOU threshold of 0.5, the Detectron-2 produces 93.32 (R50-FPN) and (X101-FPN)\\n94.19 AP values, while with a very strict IoU threshold of 0.9 the Detectron-2 provides AP of 83.55, and 88.70\\nAP with R50-FPN and X101-FPN, respectively. This shows that for selective harvesting the dataset can\\nbe reliably used. For Dataset-1, the performance of our models on ‘”unpluckable”’ berries is comparatively\\nless reliable as there are fewer samples of ‘”unpluckable”’ berries in this dataset. However, from a selective\\nharvesting perspective instance segmentation of ‘pluckable’ berries is more essential.\\nThe results of the key-points detection expressed in terms of AP at diﬀerent IoU thresholds are similar to\\nsegmentation. At each IoU threshold, we take the average results from 0.5, 0.3, and 0.1 OKS. OKS (Wu et al.,\\n2019) is the standard performance metric used by Detectron-2 (Wu et al., 2019) and MSCOCO (Lin et al.,\\n2015) for key-point detection. While the OKS threshold normally used is 0.5, 0.1 is a stricter threshold. The\\nexperimental results show that similarly to segmentation, the results are consistent across the two backbones\\nalthough X101-FPN performs slightly better. Also, the key-points detection for ‘pluckable’ berries is much\\nbetter than ‘”unpluckable”’ berries for Dataset-1. The results for Dataset-2 obtained comparing X101-FPN\\nand X101 networks, provide a good baseline for future research. Figure 7 shows an example of creating a\\nfruit bounding box, key-points detection, and predicting pluckable and unpluckable fruits.\\nFigure 8: Predicted x, y, and z error of the fruit coordinate in the robot base frame using Gaussian Process\\nRegression model.\\n6.2\\nGaussian Process Regression for picking point error estimation results\\nDuring the ﬁeld experiment, the results of picking point error estimation using the Gaussian Process Regres-\\nsion model were recorded which are shown in Figure 8. It presents the predicted x, y, and z error of the fruit\\ncoordinate in the robot base frame corresponding to the euclidean distance of the fruit from the base of the\\nrobot. After predicting the errors using the model, the targeted fruit picking point coordinate is corrected\\nand the new coordinate is sent to the system to re-plan the trajectory of the end-eﬀector. The results show\\nthat the values of the errors are ﬂuctuating around a speciﬁc mean value for each axis. The Mean of errors\\nof x-axis is MEx = 0.062m with a standard deviation of σx = 0.012, for y-axis is MEy = 0.009m with a\\nstandard deviation of σy = 0.014, and for z-axis is MEz = −0.019m with a standard deviation of σz = 0.016.\\n6.3\\nHarvesting analyses\\nWe perform a series of experiments where at each attempt we consider all the berries in a series of clusters.\\nThis typically means a target range from around 5 to 15 ripe fruits in a cluster among other yet to ripe fruit.\\nDiﬀerent trails in multiple harvesting sessions were carried out. The data collected from harvesting trails\\nare presented in Table 3. For this experiment, ﬁrst all fruit Na in the harvesting section including ripe and\\nunripe fruits were counted. To determine the performance of the fruit ripeness detection of the system, the\\npluckable fruits Np, also were counted based on human judgment and compared with the system’s ripeness\\ndetection Nd. The ratio of pluckable fruit to all fruits including unripe fruits (Na/Np) was 0.42. The result\\nof calculating ripeness detection ratio (Nd/Np) shows 95% accuracy of our ripeness detection model.\\nThe trials show that the performance of the robot seemed to be inﬂuenced by the position of the fruit\\nin the cluster, which varies signiﬁcantly from one variety to another. The failure of the robot increases if\\nthe target fruit is occluded by too many fruit and/or leaves. However, our novel design of the end-eﬀector,\\nequipped with separator ﬁngers, is able to unblock the most common occlusion.\\nThe successfully harvested fruits were counted during the harvesting trails.\\nA harvest attempt was\\nconsidered a successful harvest where a ripe fruit was detected, griped, cut, and put in the punnet without\\ndamage or bruise. Where a fruit dropped midway, bruised or damaged with cutting or gripping, or harvested\\nwith a too long stem remaining on the fruit, considered an unsuccessful harvest. Therefore, the successful\\nharvesting rate Sr was calculated as Sr = (Ns/Np) × 100 where Ns is the total harvested fruits, and Np is\\nthe total pluckable fruit. The results presented in Table 3 show Sr = 83% for all trials. Another important\\nparameter is the success rate of detected fruits which calculate as SDr = (Nd/Np) × 100 where Nd is the\\ntotal number of detected fruit by the system detection model, and Np is the total pluckable fruit. This\\nparameter shows the performance of the designed end-eﬀector, position error estimation model, and motion\\nplanning control system, regardless of whether fruit is detected or not. The results show SDr = 87% for all\\ntrials.\\nTable 2: Comparison of picking success rate after the ﬁrst attempt with other strawberry harvesting systems.\\n*Success of peduncle detection\\nAuthor\\nSuccess rate (%)\\n(Dimeas et al., 2015)\\n65\\n(Hayashi et al., 2010)\\n65*\\n(Feng et al., 2012)\\n71\\n(Ge et al., 2019)\\n74\\nThis work (Sr)\\n83\\nThis work (SDr)\\n87\\nTable 2 shows the performance of our system in comparison to existing approaches that have evaluated\\ntheir systems. Among other methods only (Ge et al., 2019) perform close in comparison to this work. It\\nis not surprising that (Ge et al., 2019) also relies on mask-RCNN for strawberry detection. Moreover, they\\nalso propose their own algorithm for reﬁning the depth information obtained from the depth sensor.\\nTo analyze the unsuccessful harvests, ﬁve more parameters were deﬁned and recorded during ﬁeld experi-\\nments; total attempt At, cut command failure Fc, gripping/cutting failure Fgc, picking validation failure Fv,\\nand position failure Fp. The total attempt is the sum of all robot attempts to harvest all detected fruits in\\na trail. Cut command failure is when the robot end-eﬀector is successfully positioned at the picking point,\\nbut the system fails to detect the fruit and send a cutting command. The gripping/cutting failure parameter\\nshows unsuccessful or partially cutting oﬀ the stem, and/or unsuccessful gripping leads to harvesting failure.\\nPicking validation failure happens when the stem of the targeted fruit is gripped and cut successfully, but the\\nsystem fails to validate picking and doesn’t put the fruit in the punnet. The position failure parameter shows\\nthe failures due to inaccurate localizing of the picking point where the end-eﬀector fails to grasp and/or cut\\nthe stem of the targeted fruit.\\nThe results show in total 201 attempts were conducted by the robot to harvest all detected pluckable\\nfruit which is 1.23 attempts per fruit. It can be seen that there are 66 failed attempts where 12% is the\\nresult of detection failure, 29% due to cut command failure, 14% because of gripping/cutting failure, 9% is\\nthe result of picking validation failure, and ﬁnally, 26% was the result of position inaccuracy.\\nThe histogram of the time of picking one fruit successfully is presented in Figure 9. The time of harvest\\nof one fruit here was measured from the capture of a fruit image to put that fruit in the punnet. The average\\nexecution time from capturing the image to placing the fruit was 28.2 s. In addition, the total time of a trial\\nwas measured as can be seen in Table 3. This time was measured from starting the robot at the beginning\\nof a trail to the last harvest of that trail. The average time per successful harvest of this measurement is\\nslightly higher than the average picking time. This is due to failed/delayed defections, failed attempts, etc.\\n7\\nDiscussion\\nThe results of the ﬁeld experiments demonstrate the robustness and eﬀectiveness of the new robotic harvest-\\ning system. The experiments in two diﬀerent growing conditions and three diﬀerent strawberry varieties show\\nthe high-level adaptability of the system. It proves that the modular characteristics of the system alongside\\nits high ability to reconﬁgure the system based on the required condition, show distinguished performance.\\nthis ability is highly important to generalise its developed technologies to other fruits or growing conditions\\nwith minimal changes. The current alternatives have been designed and developed based on speciﬁc needs\\nFigure 9: The distribution of time of picking one fruit.\\nThe time was measured from the beginning of\\ncapturing the image of the targeted fruit using the vision sensor to the end of placing the harvested fruit in\\nthe punnet.\\nand conditions, which makes it challenging to adapt them to other environments.\\nThe vision system and ripeness detection proved to be eﬀective with just 4.9% of the ripe fruit not\\ndetected. From the graph shown in Figure 8 it can be seen that the localisation of the vision system contains\\na level of signiﬁcant errors, however, the errors ﬂuctuate around a speciﬁc value. Some of these errors could\\nbe caused by the system’s internal calibration errors. For instance, camera calibration errors, or errors of\\ntransformation from camera coordinate to robot coordinate can contribute to internal calibration errors.\\nAnother source of error could be the diﬀerent lighting conditions of the experiment environment. Regardless\\nof the source or nature of the errors, the proposed Gaussian Process Regression for picking point error\\nestimation method proved to be eﬀective of mitigate the position errors. From Table 3 it can be seen that\\njust 8.4% of the attempts failed due to position error. It is notable that we used a limited dataset to train\\nthe model. A more comprehensive dataset with more data points can enhance the performance of the model\\nin diﬀerent conditions.\\nThe cutting conﬁrmation and picking validation methods were also tested during ﬁeld experiments. This\\nsub-system improves the eﬃciency of the whole system by reducing the redundant movement of the robot.\\nHowever, the results show that around 12.9% of all failed harvesting attempts were because of the cutting\\nconﬁrmation and picking validation failure. One reason for these failures that were observed during the ﬁeld\\nexperiment lighting condition of the environment. As the strawberry, in general, has a shiny texture that\\nreﬂects direct sunlight, in some scenarios the direction of the sunlight reﬂection prevents the RGB sensors\\nto pick up and transmit the correct colour. In this situation, the pixels show bright colours instead of red\\ncolour which leads to classifying the picking as unsuccessful or the cutting command as not sent.\\nDiﬀerent factors aﬀect the harvesting time of the robotic system. Figure 9 shows that harvesting time\\nvaries from 21 seconds to 35 seconds with an average of 28.2 seconds. Field experiment observations indicate\\nthat one reason for time variation is robot correction movements to adjust to the targeted fruit location\\nTable 3: Results of ﬁeld experiments.\\nTrail No.\\nTotal\\nfruit\\nPluckable\\nfruit\\nPluckable\\nnot detected\\nCut comm.\\nFailure\\nGrip/Cut\\nfailure\\nPicking\\nValid. failure\\nPosition\\nfailure\\nSuccessful\\nharvest\\nTotal\\nattempt\\nTotal trail\\ntime (s)\\n1\\n17\\n7\\n1\\n2\\n0\\n0\\n1\\n6\\n10\\n265\\n2\\n15\\n7\\n0\\n1\\n1\\n1\\n3\\n5\\n11\\n355\\n3\\n18\\n8\\n1\\n0\\n0\\n0\\n0\\n7\\n7\\n245\\n4\\n24\\n10\\n0\\n1\\n0\\n0\\n3\\n8\\n15\\n441\\n5\\n25\\n12\\n1\\n2\\n1\\n1\\n1\\n11\\n15\\n453\\n6\\n14\\n9\\n1\\n0\\n0\\n0\\n0\\n8\\n8\\n231\\n7\\n28\\n12\\n2\\n1\\n0\\n2\\n0\\n9\\n15\\n335\\n8\\n8\\n3\\n0\\n0\\n0\\n0\\n0\\n3\\n5\\n78\\n9\\n14\\n6\\n0\\n2\\n0\\n0\\n0\\n5\\n10\\n221\\n10\\n35\\n18\\n0\\n0\\n0\\n1\\n0\\n16\\n17\\n448\\n11\\n31\\n9\\n0\\n4\\n1\\n0\\n1\\n8\\n14\\n311\\n12\\n17\\n10\\n0\\n2\\n0\\n0\\n1\\n8\\n11\\n340\\n13\\n24\\n11\\n0\\n3\\n1\\n0\\n2\\n8\\n12\\n401\\n14\\n21\\n6\\n1\\n3\\n1\\n0\\n2\\n4\\n10\\n270\\n15\\n20\\n9\\n0\\n1\\n1\\n1\\n2\\n7\\n10\\n261\\n16\\n14\\n7\\n1\\n2\\n0\\n0\\n0\\n5\\n8\\n193\\n17\\n23\\n8\\n0\\n1\\n2\\n0\\n0\\n8\\n11\\n231\\n18\\n29\\n11\\n0\\n1\\n1\\n0\\n1\\n9\\n12\\n245\\nTotal\\n377\\n163\\n8\\n26\\n9\\n6\\n17\\n135\\n201\\n5324 (s)\\nand/or bring the targeted fruit into the ﬁeld view of the bottom RGB sensor.\\nIn some scenarios, the\\nrobot carries out multiple adjustment movements such as moving back, left, right, down, or up to bring the\\ntargeted fruit into the bottom sensors’ ﬁeld of view. These adjustment movements are important to capture\\nthe targeted fruit which is later used for validation, although it might increase the harvesting time slightly.\\nThe ﬁeld experiments demonstrated the eﬀectiveness of the novel design of the picking head. Only 4.4%\\nof all attempts failed because of the cutting or gripping failure. It means that the picking head design was\\neﬀectively capable of a stable grip and successful cut of the strawberry stem. The picking head is successfully\\ncapable of grasping and manipulating the harvested strawberry without contact with the fruit. In contrast,\\nmost available technologies handle the fruit by grasping it using grippers or suction cups or directly dropping\\nit into the container after cutting the stem. Our design grasps and handles the fruit from its stem reducing\\nthe possibility of bruising or damaging the fruit signiﬁcantly. In addition, an ineﬀective cutting mechanism\\nleads to partial cutting which damages the plant and increases plant disease possibility. We tested the design\\non diﬀerent strawberry varieties which have diﬀerent stem diameters and strengths. The experiments proved\\nthat the cutting mechanism is able to cut the diﬀerent varieties of stems eﬀectively. (A video of the system\\ncan be seen in this link.)\\n8\\nConclusion\\nWe designed, prototyped and ﬁeld tested a novel picking head that can navigate through possible clusters and\\npick a targeted fruit. The picking head consists of two independent mechanisms for grasping and removing\\nocclusion providing 2.5 DOF including the cutting mechanism.\\nThis novel design allows the system to\\nmanipulate occlusions independent of picking actions.\\nIn addition, the picking head design provides a\\ncontact-free grasping and picking of the fruit. This is highly important to reduce fruit damage or bruising\\nand to reduce the corresponding waste as strawberries are a very delicate fruit. We developed and proposed\\na state-of-the-art perception system using RGB-D and three RGB sensors. To train our vision system, we\\nproduced two new datasets from a real strawberry growing farm with diﬀerent features such as key points,\\npicking points, ripeness, etc. We designed and developed the autonomous harvesting system modular and\\nconﬁgurable to increase its adaptability for diﬀerent strawberry varieties and growing conditions. Finally,\\nto test the system we performed ﬁeld experiments on two diﬀerent commercial farms with three varieties.\\nThe ﬁeld experiment results show the eﬃciency and reliability of the system with an 87% success rate.\\nFurthermore, the perception system demonstrated 95% success in detecting ripe fruits.\\nReferences\\nAliasgarian, S., Ghassemzadeh, H. R., Moghaddam, M., Ghaﬀari, H., et al. (2013). Mechanical damage of\\nstrawberry during harvest and postharvest operations. World Applied Sciences Journal, 22(7):969–974.\\nArad, B., Balendonck, J., Barth, R., Ben-Shahar, O., Edan, Y., Hellstr¨om, T., Hemming, J., Kurtser, P.,\\nRingdahl, O., Tielen, T., et al. (2020). Development of a sweet pepper harvesting robot. Journal of\\nField Robotics, 37(6):1027–1039.\\nAreﬁ, A., Motlagh, A. M., Mollazade, K., Teimourlou, R. F., et al. (2011). Recognition and localization of\\nripen tomato based on machine vision. Australian Journal of Crop Science, 5(10):1144.\\nArima, S., Kondo, N., and Monta, M. (2004). Strawberry harvesting robot on table-top culture. In 2004\\nASAE Annual Meeting, page 1. American Society of Agricultural and Biological Engineers.\\nArmada, M. A., Muscato, G., Prestiﬁlippo, M., Abbate, N., and Rizzuto, I. (2005). A prototype of an orange\\npicking robot: past history, the new robot and experimental results. Industrial Robot: An International\\nJournal.\\nBac, C. W., Hemming, J., Van Tuijl, B., Barth, R., Wais, E., and van Henten, E. J. (2017). Performance\\nevaluation of a harvesting robot for sweet pepper. Journal of Field Robotics, 34(6):1123–1139.\\nBaeten, J., Donn´e, K., Boedrij, S., Beckers, W., and Claesen, E. (2008). Autonomous fruit picking machine:\\nA robotic apple harvester. In Field and service robotics, pages 531–539. Springer.\\nCao, Z., Hidalgo, G., Simon, T., Wei, S.-E., and Sheikh, Y. (2019). Openpose: realtime multi-person 2d pose\\nestimation using part aﬃnity ﬁelds. IEEE transactions on pattern analysis and machine intelligence,\\n43(1):172–186.\\nCeres, R., Pons, J. L., Jimenez, A., Martin, J., and Calderon, L. (1998). Design and implementation of an\\naided fruit-harvesting robot (agribot). Industrial Robot: An International Journal.\\nChowdhary, G., Gazzola, M., Krishnan, G., Soman, C., and Lovell, S. (2019). Soft robotics as an enabling\\ntechnology for agroforestry practice and research. Sustainability, 11(23):6751.\\nCORDIS (2015). Sweeper: Sweet pepper harvesting robot. In https://cordis.europa.eu/project/id/644313.\\n[accessed on 07.08.2022].\\nCORDIS (2020). Bachus:mobile robotic platforms for active inspection and harvesting in agricultural areas.\\nIn https://cordis.europa.eu/project/id/871704. [accessed on 07.08.2022].\\nDavidson, J. R., Hohimer, C. J., Mo, C., and Karkee, M. (2017). Dual robot coordination for apple harvesting.\\nIn 2017 ASABE annual international meeting, page 1. American Society of Agricultural and Biological\\nEngineers.\\nDe Preter, A., Anthonis, J., and De Baerdemaeker, J. (2018).\\nDevelopment of a robot for harvesting\\nstrawberries. IFAC-PapersOnLine, 51(17):14–19.\\nDimeas, F., Sako, D. V., Moulianitis, V. C., and Aspragathos, N. A. (2015). Design and fuzzy control of a\\nrobotic gripper for eﬃcient strawberry harvesting. Robotica, 33(5):1085–1098.\\nDuckett, T., Pearson, S., Blackmore, S., Grieve, B., Chen, W.-H., Cielniak, G., Cleaversmith, J., Dai, J.,\\nDavis, S., Fox, C., et al. (2018). Agricultural robotics: the future of robotic agriculture. arXiv preprint\\narXiv:1806.06762.\\nDurand-Petiteville, A., Vougioukas, S., and Slaughter, D. C. (2017). Real-time segmentation of strawberry\\nﬂesh and calyx from images of singulated strawberries during postharvest processing. Computers and\\nelectronics in agriculture, 142:298–313.\\nDyson\\n(2022).\\nDyson\\nfarming\\nand\\nstate\\nof\\nthe\\nart\\nstrawberry\\nglasshouse.\\nIn\\nhttps://dysonfarming.com/strawberries/. [accessed on 07.08.2022].\\nFeng, Q., Wang, X., Zheng, W., Qiu, Q., and Jiang, K. (2012).\\nNew strawberry harvesting robot for\\nelevated-trough culture. International Journal of Agricultural and Biological Engineering, 5(2):1–8.\\nFeng, Q., Zou, W., Fan, P., Zhang, C., and Wang, X. (2018). Design and test of robotic harvesting system\\nfor cherry tomato. International Journal of Agricultural and Biological Engineering, 11(1):96–100.\\nGanesh, P., Volle, K., Burks, T., and Mehta, S. (2019). Deep orange: Mask r-cnn based orange detection\\nand segmentation. IFAC-PapersOnLine, 52(30):70–75.\\nGao, Z., Shao, Y., Xuan, G., Wang, Y., Liu, Y., and Han, X. (2020). Real-time hyperspectral imaging for\\nthe in-ﬁeld estimation of strawberry ripeness with deep learning. Artiﬁcial Intelligence in Agriculture.\\nGe, Y., Xiong, Y., Tenorio, G. L., and From, P. J. (2019). Fruit localization and environment perception for\\nstrawberry harvesting robots. IEEE Access, 7:147642–147652.\\nHayashi, S., Shigematsu, K., Yamamoto, S., Kobayashi, K., Kohno, Y., Kamata, J., and Kurita, M. (2010).\\nEvaluation of a strawberry-harvesting robot in a ﬁeld test. Biosystems engineering, 105(2):160–171.\\nHayashi, S., Yamamoto, S., Saito, S., Ochiai, Y., Kamata, J., Kurita, M., and Yamamoto, K. (2014). Field\\noperation of a movable strawberry-harvesting robot using a travel platform. Japan Agricultural Research\\nQuarterly: JARQ, 48(3):307–316.\\nHe, K., Gkioxari, G., Doll´ar, P., and Girshick, R. (2017). Mask r-cnn. In Proceedings of the IEEE interna-\\ntional conference on computer vision, pages 2961–2969.\\nJarrass´e, N., Ribeiro, A. T., Sahbani, A., Bachta, W., and Roby-Brami, A. (2014). Analysis of hand synergies\\nin healthy subjects during bimanual manipulation of various objects. Robotics and Autonomous Systems,\\n131:113.\\nKurtser, P. and Edan, Y. . (2020).\\nPlanning the sequence of tasks for harvesting robots.\\nJournal of\\nNeuroEngineering and Rehabilitation, 11:103591.\\nLamb, N. and Chuah, M. C. (2018). A strawberry detection system using convolutional neural networks. In\\n2018 IEEE International Conference on Big Data (Big Data), pages 2515–2520. IEEE.\\nLee, B. and Rosa, U. (2006). Development of a canopy volume reduction technique for easy assessment and\\nharvesting of valencia citrus fruits. Transactions of the ASABE, 49(6):1695–1703.\\nLehnert, C., English, A., McCool, C., Tow, A. W., and Perez, T. (2017).\\nAutonomous sweet pepper\\nharvesting for protected cropping systems. IEEE Robotics and Automation Letters, 2(2):872–879.\\nLi, J., Tang, Y., Zou, X., Lin, G., and Wang, H. (2020). Detection of fruit-bearing branches and localization\\nof litchi clusters for vision-based harvesting robots. IEEE Access, 8:117746–117758.\\nLin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick,\\nC. L., and Doll´ar, P. (2015). Microsoft coco: Common objects in context.\\nLing, X., Zhao, Y., Gong, L., Liu, C., and Wang, T. (2019). Dual-arm cooperation and implementing for\\nrobotic harvesting tomato using binocular vision. Robotics and Autonomous Systems, 114:134–143.\\nLiu, X., Chen, S. W., Aditya, S., Sivakumar, N., Dcunha, S., Qu, C., Taylor, C. J., Das, J., and Kumar,\\nV. (2018a). Robust fruit counting: Combining deep learning, tracking, and structure from motion. In\\n2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1045–1052.\\nIEEE.\\nLiu, X., Zhao, D., Jia, W., Ji, W., Ruan, C., and Sun, Y. (2019). Cucumber fruits detection in greenhouses\\nbased on instance segmentation. IEEE Access, 7:139635–139642.\\nLiu, Y.-P., Yang, C.-H., Ling, H., Mabu, S., and Kuremoto, T. (2018b). A visual system of citrus pick-\\ning robot using convolutional neural networks. In 2018 5th international conference on systems and\\ninformatics (ICSAI), pages 344–349. IEEE.\\nMandil, W., Nazari, K., et al. (2022). Action conditioned tactile prediction: a case study on slip prediction.\\narXiv preprint arXiv:2205.09430.\\nMghames, S., Hanheide, M., and Ghalamzan, A. (2020). Interactive movement primitives: Planning to push\\noccluding pieces for fruit picking. In 2020 IEEE/RSJ International Conference on Intelligent Robots\\nand Systems (IROS), pages 2616–2623. IEEE.\\nNazari, K., Mandil, W., et al. (2022). Proactive slip control by learned slip model and trajectory adaptation.\\narXiv preprint arXiv:2209.06019.\\nOsa, T., Pajarinen, J., Neumann, G., Bagnell, J. A., Abbeel, P., Peters, J., et al. (2018). An algorithmic\\nperspective on imitation learning. Foundations and Trends® in Robotics, 7(1-2):1–179.\\nO’Mahony, N., Campbell, S., Carvalho, A., Harapanahalli, S., Hernandez, G. V., Krpalkova, L., Riordan,\\nD., and Walsh, J. (2019). Deep learning vs. traditional computer vision. In Science and Information\\nConference, pages 128–144. Springer.\\nP´erez-Borrero, I., Mar´ın-Santos, D., Geg´undez-Arias, M. E., and Cort´es-Ancos, E. (2020).\\nA fast and\\naccurate deep learning method for strawberry instance segmentation. Computers and Electronics in\\nAgriculture, 178:105736.\\nP´erez-Borrero, I., Mar´ın-Santos, D., Geg´undez-Arias, M. E., and Cort´es-Ancos, E. (2020).\\nA fast and\\naccurate deep learning method for strawberry instance segmentation. Computers and Electronics in\\nAgriculture, 178:105736.\\nRagaglia, M. et al. (2018). Robot learning from demonstrations: Emulation learning in environments with\\nmoving obstacles. Robotics and autonomous systems, 101:45–56.\\nRajendra, P., Kondo, N., Ninomiya, K., Kamata, J., Kurita, M., Shiigi, T., Hayashi, S., Yoshida, H., and\\nKohno, Y. (2009).\\nMachine vision algorithm for robots to harvest strawberries in tabletop culture\\ngreenhouses. Engineering in Agriculture, Environment and Food, 2(1):24–30.\\nRajendran Sugathakumary, V., Parsa, S., Parsons, S., and Ghalamzan Esfahani, A. (2022). Peduncle grip-\\nping and cutting force for strawberry harvesting robotic end-eﬀector design. In 2022 4th International\\nConference on Control and Robotics (ICCR 2022). IEEE.\\nSa, I., Ge, Z., Dayoub, F., Upcroft, B., Perez, T., and McCool, C. (2016). Deepfruits: A fruit detection\\nsystem using deep neural networks. Sensors, 16(8):1222.\\nSanni, O., Bonvicini, G., Khan, M. A., L´opez-Custodio, P. C., Nazari, K., et al. (2022). Deep movement\\nprimitives: toward breast cancer examination robot. In Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence, volume 36, pages 12126–12134.\\nScarfe, A. J., Flemmer, R. C., Bakker, H., and Flemmer, C. L. (2009). Development of an autonomous\\nkiwifruit picking robot. In 2009 4th International Conference on Autonomous Robots and Agents, pages\\n380–384. IEEE.\\nSep´uLveda, D., Fern´andez, R., Navas, E., Armada, M., and Gonz´alez-De-Santos, P. (2020).\\nRobotic\\naubergine harvesting using dual-arm manipulation. IEEE Access, 8:121889–121904.\\nSilwal, A., Davidson, J. R., Karkee, M., Mo, C., Zhang, Q., and Lewis, K. (2017). Design, integration, and\\nﬁeld evaluation of a robotic apple harvester. Journal of Field Robotics, 34(6):1140–1159.\\nTafuro, A., Adewumi, A., Parsa, S., Esfahani, A., and Debnath, B. (2022a). Strawberry picking point local-\\nization ripeness and weight estimation. In 2022 International Conference on Robotics and Automation\\n(ICRA). IEEE.\\nTafuro, A., Debnath, B., Zanchettin, A. M., et al. (2022b). dpmp-deep probabilistic motion planning: A use\\ncase in strawberry picking robot. arXiv preprint arXiv:2208.09074.\\nTao, Y. and Zhou, J. (2017). Automatic apple recognition based on the fusion of color and 3d feature for\\nrobotic fruit picking. Computers and electronics in agriculture, 142:388–396.\\nTiefeng, S., Mingyu, D., Guanjun, B., Libin, Z., and Qinghua, Y. (2015). Fruit harvesting continuum ma-\\nnipulator inspired by elephant trunk. International Journal of Agricultural and Biological Engineering,\\n8(1):57–63.\\nVan Henten, E. J., Hemming, J., Van Tuijl, B., Kornet, J., Meuleman, J., Bontsema, J., and Van Os, E.\\n(2002). An autonomous robot for harvesting cucumbers in greenhouses. Autonomous robots, 13(3):241–\\n258.\\nWeb (2020). Strawberry picking cost\\n. In\\nhttps://www.producebluebook.com/2020/11/25/strawberries-in-2020. [accessed on 07.08.2022].\\nWu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Girshick, R. (2019). Detectron2. https://github.com/\\nfacebookresearch/detectron2.\\nXie, S., Girshick, R., Doll´ar, P., Tu, Z., and He, K. (2017). Aggregated residual transformations for deep\\nneural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\\npages 1492–1500.\\nXiong, Y., From, P. J., and Isler, V. (2018). Design and evaluation of a novel cable-driven gripper with per-\\nception capabilities for strawberry picking robots. In 2018 IEEE International Conference on Robotics\\nand Automation (ICRA), pages 7384–7391. IEEE.\\nXiong, Y., Ge, Y., and From, P. J. (2020a). An obstacle separation method for robotic picking of fruits in\\nclusters. Computers and Electronics in Agriculture, 175:105397.\\nXiong, Y., Ge, Y., Grimstad, L., and From, P. J. (2020b). An autonomous strawberry-harvesting robot:\\nDesign, development, integration, and ﬁeld evaluation. Journal of Field Robotics, 37(2):202–224.\\nXiong, Y., Peng, C., Grimstad, L., From, P. J., and Isler, V. (2019). Development and ﬁeld evaluation of\\na strawberry harvesting robot with a cable-driven gripper. Computers and electronics in agriculture,\\n157:392–402.\\nYamamoto, S., Hayashi, S., Yoshida, H., and Kobayashi, K. (2014). Development of a stationary robotic\\nstrawberry harvester with a picking mechanism that approaches the target fruit from below. Japan\\nAgricultural Research Quarterly: JARQ, 48(3):261–269.\\nYu, Y., Zhang, K., Yang, L., and Zhang, D. (2019). Fruit detection for strawberry harvesting robot in non-\\nstructural environment based on mask-rcnn. Computers and Electronics in Agriculture, 163:104846.\\nZeng, X., Miao, Y., Ubaid, S., Gao, X., and Zhuang, S. (2020). Detection and classiﬁcation of bruises of\\npears based on thermal images. Postharvest Biology and Technology, 161:111090.\\nZhang, L., Jia, J., Gui, G., Hao, X., Gao, W., and Wang, M. (2018).\\nDeep learning based improved\\nclassiﬁcation system for designing tomato harvesting robot. IEEE Access, 6:67940–67950.\\nZhang, Z., Luo, P., Loy, C. C., and Tang, X. (2014). Facial landmark detection by deep multi-task learning.\\nIn European conference on computer vision, pages 94–108. Springer.\\nZhao, Y., Gong, L., Liu, C., and Huang, Y. (2016). Dual-arm robot design and testing for harvesting tomato\\nin greenhouse. IFAC-PapersOnLine, 49(16):161–165.\\nZhuang, J., Hou, C., Tang, Y., He, Y., Guo, Q., Zhong, Z., and Luo, S. (2019). Computer vision-based\\nlocalisation of picking points for automatic litchi harvesting applications towards natural scenarios.\\nBiosystems Engineering, 187:1–20.\\nS, ucan A., I., Moll, M., and Kavraki E., L. (2012). The open motion planning library. IEEE Robotics and\\nAutomation Magazine, 19(4):72–82.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["## 前処理"],"metadata":{"id":"B4iuT956GEkx"}},{"cell_type":"code","source":["import re\n","code_regex = re.compile('[!\"#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥％…]')\n","if type(raw_text)==str:\n","  raw_text = code_regex.sub('', raw_text)\n","if type(raw_text)==list:\n","  raw_text = [Document(page_content=code_regex.sub('', s)) for s in raw_text]"],"metadata":{"id":"UZ0BisHuA375"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 文章のチャンク化"],"metadata":{"id":"pIzF4EC8M9NH"}},{"cell_type":"code","source":["# 情報検索の際に、トークンのサイズ制限に引っかからないように、読んだテキストを小さな塊に分割。1000トークン、チャンク間は200だけ重なるように分割\n","text_splitter = CharacterTextSplitter(        \n","    separator = \"\\n\",\n","    chunk_size = 400,\n","    chunk_overlap  = 50,\n","    length_function = len,\n",")\n","if type(raw_text)==str:\n","  texts = text_splitter.split_text(raw_text)\n","if type(raw_text)==list:\n","  texts = text_splitter.split_documents(raw_text)\n","  texts = [s.page_content for s in texts]"],"metadata":{"id":"VdXzkpf9XAfP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(texts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozkNTiNuZ0TX","executionInfo":{"status":"ok","timestamp":1683348748645,"user_tz":-540,"elapsed":7,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"61124d24-af9c-4f17-8839-97d496e28c31"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["751"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["texts[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"1SqdR3wFZ3Ih","executionInfo":{"status":"ok","timestamp":1683348748645,"user_tz":-540,"elapsed":7,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"7e33c7e9-1c0f-4c52-f804-e4d062d2dbbc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'有価証券報告書\\n１本書はEDINETElectronic Disclosure for Investors NETwork\\nシステムを利用して金融庁に提出した有価証券報告書の記載事項を\\n出力印刷したものであります\\n２ 本書には上記の方法により提出した有価証券報告書に添付された\\n監査報告書及び上記の有価証券報告書と併せて提出した内部統制\\n報告書確認書を末尾に綴じ込んでおります \\n株式会社みずほフィナンシャルグループ \\nE03615 事 業 年 度\\n第 20 期自\\u30002021年４月１日\\n至\\u30002022年３月31日頁\\n表紙   1\\n第一部 企業情報  2\\n第１企業の概況  2\\n１．主要な経営指標等の推移  2\\n２．沿革  5\\n３．事業の内容  7\\n４．関係会社の状況  10\\n５．従業員の状況  16\\n第２事業の状況  17\\n１．経営方針経営環境及び対処すべき課題等  17\\n２．事業等のリスク  24'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["texts[-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"059PoKYUZ6dJ","executionInfo":{"status":"ok","timestamp":1683348748645,"user_tz":-540,"elapsed":6,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"8c6151aa-ec5e-435e-b12e-3d5d7d4f145b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'接的に関連する損益科目に至る業務プロセスを評価の対象としたさらに選定した重要な事業拠点にかかわらず\\nそれ以外の事業拠点をも含めた範囲について重要な虚偽記載の発生可能性が高く見積りや予測を伴う重要な勘定\\n科目に係る業務プロセスやリスクが大きい取引を行っている事業又は業務に係る業務プロセスを財務報告への影響を\\n勘案して重要性の大きい業務プロセスとして評価対象に追加している\\n３評価結果に関する事項\\n\\u3000上記の評価の結果当事業年度末日時点において当社の財務報告に係る内部統制は有効であると判断した\\n４付記事項\\n\\u3000該当事項なし\\n５特記事項\\n\\u3000該当事項なし'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## 文章の埋め込みと検索"],"metadata":{"id":"rNlnk_-jNEpZ"}},{"cell_type":"code","source":["# Download embeddings from OpenAI\n","embeddings = OpenAIEmbeddings()"],"metadata":{"id":"TcZUsQVyXBPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docsearch = FAISS.from_texts(texts, embeddings)"],"metadata":{"id":"9C8py6wQXE5_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LangChain"],"metadata":{"id":"BCYcJ9OZNIQx"}},{"cell_type":"code","source":["from langchain.chains.question_answering import load_qa_chain\n","from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n","\n","from langchain.llms import OpenAI\n","from langchain.chat_models import ChatOpenAI"],"metadata":{"id":"wpQ2VnBvXI2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n","# llm = OpenAI(temperature=0)\n","\n","chain = load_qa_chain(llm, chain_type=\"stuff\")\n","# chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")"],"metadata":{"id":"_L_Ywm-iXLhm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"労働環境について教えて\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","output_chain = chain.run(input_documents=docs, question=query)\n","display(Markdown(output_chain))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"id":"9pSfTrzFH-Wn","executionInfo":{"status":"ok","timestamp":1683348757851,"user_tz":-540,"elapsed":4005,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"ee235ed6-c81d-4c5b-cf85-89ee98fcf253"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.35154343, 0.35436386, 0.3575907, 0.35905534]\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"申し訳ありませんが、与えられた文脈からは具体的な労働環境に関する情報は得られません。文脈は、当社グループが環境社会課題に対処するために取り組んでいることや、世界的な気候変動対策の進展について説明しています。"},"metadata":{}}]},{"cell_type":"code","source":["query = \"この文章のキーワードをMarkdown記法で抽出して\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","output_chain = chain.run(input_documents=docs, question=query)\n","display(Markdown(output_chain))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":360},"id":"RVp58TQlM1FV","executionInfo":{"status":"ok","timestamp":1683348763015,"user_tz":-540,"elapsed":5166,"user":{"displayName":"guin min","userId":"14250521523391161248"}},"outputId":"98a320b8-ed64-4a77-b436-25e6f1759e91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.44249836, 0.45967847, 0.46544528, 0.46787745]\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"# キーワード抽出\n\n- 温室効果ガス排出量\n- カーボンニュートラル\n- サステナビリティ\n- 顧客ニーズ\n- 金融グループ\n- 経済情勢\n- 新型コロナウイルス\n- インフレ圧力\n- 商品資源価格\n- 経常収益\n- コンプライアンス\n- 外為法令\n- アンチマネーロンダリング\n- テロ資金供与対策\n- 反社会的勢力"},"metadata":{}}]},{"cell_type":"code","source":["query = \"事業内容・サービスに関するキーワードをMarkdown記法で抽出して\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","output_chain = chain.run(input_documents=docs, question=query)\n","display(Markdown(output_chain))"],"metadata":{"id":"vzQ7GNztGQX-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"事業内容について要約して\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"id":"IvTs3g05GLR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"事業戦略について要約して\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"id":"0U6IWrDfAkdJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"リスクについて要約して\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"id":"E_BeKPxsBe3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"経営課題について要約して\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"id":"DbTcr8b8_fZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"経営課題とその課題に対する事業戦略について要約して\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"id":"YOe8isoCBqQv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"Who is the author of this paper?\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"id":"3mtAth2jXNKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"Summarize this paper.\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"id":"jKOClW9D2Y6H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"Extract Abstract.\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"id":"_i_F54MP2vbG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"What is the title of this paper?\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"id":"qUhHExpI2z5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"Summarize the main points of this paper in about three bullet points.\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"id":"h2i5uu9923CV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"Noisy intermediate-scale quantum (NISQ) hardware is almost universally incompatible with fullscale optimization problems of practical importance which can have many variables and unwieldy objective functions. As a consequence, there is a growing body of literature that tests quantum algorithms on miniaturized versions of problems that arise in an operations research setting. Rather than taking this approach, we investigate a problem of substantial commercial value, multi-truck vehicle routing for supply chain logistics, at the scale used by a corporation in their operations. Such a problem is too complex to be fully embedded on any near-term quantum hardware or simulator; we avoid confronting this challenge by taking a hybrid workflow approach: we iteratively assign routes for trucks by generating a new binary optimization problem instance one truck at a time. Each instance has \u001f 2500 quadratic binary variables, putting it in a range that is feasible for NISQ quantum computing, especially quantum annealing hardware. We test our methods using simulated annealing and the D-Wave Hybrid solver as a place-holder in wait of quantum hardware developments. After feeding the vehicle routes suggested by these runs into a highly realistic classical supply chain simulation, we find excellent performance for the full supply chain. Our work gives a set of techniques that can be adopted in contexts beyond vehicle routing to apply NISQ devices in a hybrid fashion to large-scale problems of commercial interest.\"\n","query = f\"Translate the following sentence into Japanese.\\n {text}\"\n","output = docsearch.similarity_search_with_score(query)\n","docs = [s[0] for s in output]\n","scores = [s[1] for s in output]\n","print(scores)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"id":"qLynnMo0cj8m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ChatGPT arXiv"],"metadata":{"id":"F2bChWOjz2V_"}},{"cell_type":"code","source":["# In[]\n","# ref\n","\"\"\"\n","https://zenn.dev/team_zenn/articles/117424abb5605b\n","https://zenn.dev/ozushi/articles/ebe3f47bf50a86\n","https://developers.notion.com/reference/post-page\n","https://info.arxiv.org/help/api/user-manual.html\n","https://api.semanticscholar.org/api-docs/graph\n","\"\"\"\n","import requests\n","import time\n","from datetime import datetime, timezone\n","    \n","from scipy.stats import rankdata\n","import numpy as np\n","    \n","import openai\n","import arxiv\n","\n","# 現在時刻\n","now = datetime.now(timezone.utc)\n","\n","#arXiv Settings\n","ARXIV_NUM_PAPERS = 500\n","RESULT_NUM_PAPERS = 10\n","\n","# タイトルキーワード:ti\n","ARXIV_QUERY = 'ti:%22 reciprocal %22 AND ti:%22 recommend %22'\n","ARXIV_QUERY = 'ti:%22 strawberry %22'\n","ARXIV_QUERY = 'ti:%22 tomato %22'\n","\n","# すべて:all\n","ARXIV_QUERY = 'all:%22 reciprocal %22 AND all:%22 recommend %22' # reciprocal AND recommendで検索\n","ARXIV_QUERY = 'all:cs.CV' # コンピュータービジョンとパターン認識\n","ARXIV_QUERY = 'all:stat.ML' # 機械学習\n","ARXIV_QUERY = 'all:q-fin.MF' # 数学ファイナンス\n","ARXIV_QUERY = 'all:q-fin.PM' # ポートフォリオマネジメント\n","ARXIV_QUERY = 'all:q-fin.PR' # 証券のプライシング\n","ARXIV_QUERY = 'all:q-fin.RM' # リスク管理\n","ARXIV_QUERY = 'all:q-fin.TR' # Trading and Market Microstructure\n","\n","# カテゴリ(いまいち？):cat\n","# ARXIV_QUERY = 'cat:cs.AI' # 人工知能\n","\n","\n","#OpenAI Settings\n","openai.api_key = \"\"\n","OPENAI_PROMPT = \"\"\"与えられた論文の要点を3点のみでまとめ、以下のフォーマットで日本語で出力してください。```\n","Titleの日本語訳\n","・要点1\n","・要点2\n","・要点3\n","```\"\"\"\n","\n","#Notion Settings\n","NOTION_API_KEY = \"\"\n","DATABASE_ID = \"\"\n","NOTION_API_URL = 'https://api.notion.com/v1/pages'\n","\n","headers = {\n","    \"Accept\": \"application/json\",\n","    \"Notion-Version\": \"2022-06-28\",\n","    \"Content-Type\": \"application/json\",\n","    \"Authorization\": \"Bearer \" + NOTION_API_KEY\n","}\n","\n","payload = {\n","    \"parent\": {\n","        \"database_id\": DATABASE_ID\n","    },\n","    \"properties\": {\n","        \"Title\": {\n","            \"title\": [\n","            {\n","                \"text\": {\n","                \"content\": \"\"\n","                }\n","            }\n","            ],\n","        },\n","        \"タイトル\": {\n","            \"rich_text\": [\n","            {\n","                \"text\": {\n","                \"content\": \"\"\n","                }\n","            }\n","            ],\n","        },\n","        \"Detail\": {\n","            \"rich_text\": [\n","            {\n","                \"text\": {\n","                \"content\": \"\"\n","                }\n","            }\n","            ],\n","        },\n","        \"URL\":{\n","            \"url\": \"\"\n","        },\n","        \"Citation\": {\n","            \"number\": 0\n","        },\n","        \"InfluentialCitation\": {\n","            \"number\": 0\n","        },\n","        \"Date\": {\n","            \"date\": {\n","                \"start\": \"\"\n","                }\n","            },\n","        \"arXiv query\": {\n","            \"rich_text\": [\n","            {\n","                \"text\": {\n","                \"content\": ARXIV_QUERY\n","                }\n","            }\n","            ],\n","        },\n","    },\n","    \"children\":[\n","        {\n","            \"object\": 'block',\n","            \"type\": 'heading_1',\n","            \"heading_1\":{\n","                \"rich_text\": [\n","            {\n","                \"text\": {\n","                \"content\": \"Abstract\"\n","                }\n","            }\n","            ],\n","            }\n","        },\n","        {\n","            \"object\": 'block',\n","            \"type\": 'paragraph',\n","            \"paragraph\":{\n","                \"rich_text\": [\n","            {\n","                \"text\": {\n","                \"content\": \"\"\n","                }\n","            }\n","            ],\n","            }\n","        },\n","        {\n","            \"object\": 'block',\n","            \"type\": 'heading_1',\n","            \"heading_1\":{\n","                \"rich_text\": [\n","            {\n","                \"text\": {\n","                \"content\": \"要約\"\n","                }\n","            }\n","            ],\n","            }\n","        },\n","        {\n","            \"object\": 'block',\n","            \"type\": 'paragraph',\n","            \"paragraph\":{\n","                \"rich_text\": [\n","            {\n","                \"text\": {\n","                \"content\": \"\"\n","                }\n","            }\n","            ],\n","            }\n","        },\n","        {\n","            \"object\": 'block',\n","            \"type\": 'heading_1',\n","            \"heading_1\":{\n","                \"rich_text\": [\n","            {\n","                \"text\": {\n","                \"content\": \"TLDR\"\n","                }\n","            }\n","            ],\n","            }\n","        },\n","        {\n","            \"object\": 'block',\n","            \"type\": 'paragraph',\n","            \"paragraph\":{\n","                \"rich_text\": [\n","            {\n","                \"text\": {\n","                \"content\": \"\"\n","                }\n","            }\n","            ],\n","            }\n","        },\n","        {\n","            \"object\": 'block',\n","            \"type\": 'heading_1',\n","            \"heading_1\":{\n","                \"rich_text\": [\n","            {\n","                \"text\": {\n","                \"content\": \"To do\"\n","                }\n","            }\n","            ],\n","            }\n","        },\n","        {\n","            \"object\": 'block',\n","            \"type\": 'to_do',\n","            \"to_do\":{\n","                \"rich_text\": [\n","            {\n","                \"text\": {\n","                \"content\": \"\"\n","                }\n","            }\n","            ],\n","            \"checked\": False,\n","            \"color\": \"default\",\n","            }\n","        }\n","    ],\n","}\n","\n","# In[]\n","def create_notion_page(result,summary,response_semanticscholar):\n","    # from arxiv API\n","    payload['properties']['Title']['title'][0]['text']['content'] = result.title\n","    payload['properties']['URL']['url'] = result.entry_id\n","    payload['children'][1]['paragraph']['rich_text'][0]['text']['content'] = result.summary\n","    payload['properties']['Date']['date']['start'] = result.published.strftime(\"%Y-%m-%d %H:%M:%S\")\n","    \n","    # from OpenAI API\n","    title_jp, *body = summary.split('\\n')\n","    body = '\\n'.join(body)\n","    \n","    payload['properties']['タイトル']['rich_text'][0]['text']['content'] = title_jp\n","    payload['properties']['Detail']['rich_text'][0]['text']['content'] = body\n","    # payload['children'][3]['paragraph']['rich_text'][0]['text']['content'] = abstract_jp\n","    \n","    # from Semantic Scholar Academic Graph API\n","    if not 'error' in response_semanticscholar:\n","        payload['properties']['Citation']['number'] = response_semanticscholar['citationCount']\n","        payload['properties']['InfluentialCitation']['number'] = response_semanticscholar['influentialCitationCount']\n","        if not response_semanticscholar['tldr'] is None:\n","            payload['children'][5]['paragraph']['rich_text'][0]['text']['content'] = response_semanticscholar['tldr']['text']\n","        else:\n","            payload['children'][5]['paragraph']['rich_text'][0]['text']['content'] = \"\"\n","            \n","    else:\n","        payload['properties']['Citation']['number'] = 0\n","        payload['properties']['InfluentialCitation']['number'] = 0\n","        payload['children'][5]['paragraph']['rich_text'][0]['text']['content'] = \"\"\n","    \n","    \n","    response_notion = requests.post(NOTION_API_URL, json=payload, headers=headers)\n","    return response_notion\n","\n","def main():\n","    # arxiv APIで最新の論文情報を取得する\n","    search = arxiv.Search(\n","        query=ARXIV_QUERY,  # 検索クエリ\n","        max_results=ARXIV_NUM_PAPERS,  # 取得する論文数\n","        sort_by=arxiv.SortCriterion.Relevance,  # 論文を投稿された日付でソートする(SubmittedDate,Relevance,LastUpdatedDate)\n","        sort_order=arxiv.SortOrder.Descending,  # 新しい論文から順に取得する(Descending,Ascending)\n","    )\n","    \n","    #searchの結果をリストに格納\n","    results = []\n","    arxiv_list = []\n","    for result in search.results():\n","        results.append(result)\n","        arxiv_list.append(\"ARXIV:\"+result.entry_id.split(\"/\")[-1].split(\"v\")[0])\n","    \n","    # to Semantic Scholar Academic Graph API\n","    try:\n","        response_semanticscholar = requests.post(\n","        'https://api.semanticscholar.org/graph/v1/paper/batch',\n","        params={'fields': 'citationCount,influentialCitationCount,tldr'},\n","        json={\"ids\": arxiv_list}\n","        )\n","        response_semanticscholar = response_semanticscholar.json()\n","    except:\n","        print(\"Semantic Scholar Academic Graph API Error\")\n","\n","    # 並び替え    \n","    order_list = []\n","    for i,s in enumerate(response_semanticscholar):\n","        if s is None:\n","            order_list.append(0)\n","        else:\n","            # 日数あたりの引用回数\n","            order_list.append(-s['citationCount']/(now-results[i].published).days) # citationCount,influentialCitationCount\n","    order_list = rankdata(order_list, method='ordinal')-1\n","    order_list = sorted(range(len(order_list)), key=lambda k: order_list[k])\n","    \n","    results = list(np.array(results)[order_list])\n","    response_semanticscholar = list(np.array(response_semanticscholar)[order_list])\n","    \n","    # 論文情報をNotion投稿する\n","    for i,result in enumerate(results[:RESULT_NUM_PAPERS]):\n","        # API制限のため（3回/60秒）\n","        start = time.time()\n","        \n","        # to OpenAI API\n","        try:\n","            # text = f\"Title: {result.title}\\n Abstract: {result.summary}\\n TLDR: {response_semanticscholar[i]['tldr']['text']}\"\n","            text = f\"Title: {result.title}\\n Abstract: {result.summary}\"\n","            response_chatgpt = openai.ChatCompletion.create(\n","                        model=\"gpt-3.5-turbo\",\n","                        messages=[\n","                            {'role': 'system', 'content': OPENAI_PROMPT},\n","                            {'role': 'user', 'content': text}\n","                        ],\n","                        temperature=0,\n","                    )\n","            summary = response_chatgpt['choices'][0]['message']['content']\n","        except:\n","            print(\"OpenAI API Error\")\n","                    \n","        # to Notion API\n","        try:\n","            print(create_notion_page(result,summary,response_semanticscholar[i]))\n","        except:\n","            print(\"Notion API Error\")\n","            \n","        elapsed_time = time.time()-start\n","        if elapsed_time<=20:\n","            print(f\"Sleep for {20-elapsed_time} seconds due to API limitation\")\n","            time.sleep(20-elapsed_time)\n","            \n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"Vk7hOOcSz5L0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://www.youtube.com/watch?v=Sk1HrR6WGkg\n","\n","0:22:30（３）九州沖縄経済圏スマートフードチェーンプロジェクト成果の事業化と輸出拡大に向けた農研機構の取組み\n","　　農研機構本部 総括執行役 兼 事業開発部長 田中 健一\n","\n","0:41:58（４）各課題研究成果の説明\n","0:42:31 　１）輸送中のかんしょ腐敗問題の対応\n","　　農研機構九州沖縄農業研究センター 所長 森田 敏\n","1:00:57 　２）サツマイモ基腐病対策技術\n","　　農研機構植物防疫研究部門 所長 眞岡 哲夫\n","1:23:01 　３）質疑応答\n","1:55:30 　４）イチゴの輸出促進に向けた課題解決と産地拡大\n","　　農研機構九州沖縄農業研究センター 所長 森田 敏\n","2:10:45 　５）緑茶新品種｢せいめい｣の産地形成と高品質･安定生産技術\n","　　農研機構果樹茶業研究部門 所長 生駒 吉識\n","2:31:57 　６）牛肉輸出拡大に向けた生産基盤強化技術開発\n","　　農研機構畜産研究部門 所長 三森 眞琴\n","2:47:17 　７）質疑応答\n","\n","輸送中のかんしょ腐敗問題の対応\n","\t* \n","高温キュアリング：高温度、高湿度によりコルク層を作る、かんしょの腐敗の減少。根切をするとより下がる（原因にもなっている \n","\n","\n","サツマイモ基腐病対策技術\n","\t* \n","サツマイモの基腐病：平成30年にはじめて発生、沖縄と鹿児島。以降、令和4年には20数件に拡大 ・いちご：輸出、2022年度予想50億円。急増中。恋みのりではがく枯れ、輸送、単収の構造に対応。\n","\n","\n","イチゴの輸出促進に向けた課題解決と産地拡大\n","\t* \n","がく枯れは土壌含水率が多いとなりやすいと比較実験により発見\n","\t* \n","輸送は収穫前後の気温、保管温度、梱包で改善。\n","\t* \n","増収：CO2の局所施用と換気の自動制御\n","\n","\n","\n","緑茶新品種｢せいめい｣の産地形成と高品質･安定生産技術\n","\t* \n","2020（令和2）年のお茶の栽培（さいばい）面積は、1位が静岡県（しずおかけん）15,200ha（39%）、2位が鹿児島県（かごしまけん）8,360ha（21%）、3位が三重県2,710ha（7%）、4位が京都府1,560ha（4%）となっています。\n","\n","\n","牛肉輸出拡大に向けた生産基盤強化技術開発\n","\t* \n","\n","\n"],"metadata":{"id":"K3X1z6Yd0Q8j"}},{"cell_type":"markdown","source":["https://www.youtube.com/watch?v=IXtxQO9dzXE\n","\n","(1) 開会\n","0:00:00 開会\n","\n","(2) 主催者挨拶\n","0:00:49 農研機構 理事長　久間 和生\n","\n","(3) 来賓挨拶\n","0:07:08 北海道十勝総合振興局長　水戸部 裕 様\n","0:12:24 フードバレーとかち推進協議会会長　米沢 則寿 様\n","\n","(4) 講演\n","0:17:14 北海道十勝発スマートフードチェーンプロジェクトの目指すもの\n","    農研機構 北海道農業研究センター 所長　安東 郁男\n","\n","北海道、てんさい、小麦、ばれいしょ、生乳、野菜、米、シェア1位\n","農家人口、農家戸数の減少に伴う大規模・効率化が必要不可欠\n","・畑作物の終了予測ノウハウ・知見と目指す予測精度（5%）\n","\n","\n","\n","0:45:35 北海道十勝発スマートフードチェーンプロジェクトの可能性と期待 (仮題)\n","　株式会社NTTアグリテクノロジー 代表取締役社長　酒井 大雅 様\n","\n","\n","1:09:56 畑作物のデータ連携を前提とした高度気象情報と生育・収量予測モデル\n","　農研機構 北海道農業研究センター 寒地畑作研究領域 領域長　奥野 林太郎\n","・西日本農研、50mメッシュを地形データなどを使いながら推計する技術を持つ\n","・てんさい、2ヶ月前時点の精緻な予測があると良い→トラックの集荷、配送の効率化\n","\n","\n","\n","1:40:13 乳業と酪農の振興に向けたスマート酪農フードチェーンの構築\n","    農研機構 北海道農業研究センター 寒地酪農研究領域 領域長　須藤 賢司\n","2:14:42 土壌の特性と可給態養分に基づくスマート施肥技術\n","    国立大学法人帯広畜産大学 グローバルアグロメディシン研究センター教授 (兼任 : 環境農学研究部門植物生産科学分野長)　谷 昌幸 様\n","\n","(5) 閉会挨拶\n","2:43:27 農研機構 理事　折戸 文夫\n","\n","\n","・"],"metadata":{"id":"_Y_jJsQC0kdR"}}]}