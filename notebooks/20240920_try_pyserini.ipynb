{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (0.2.19)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.2.23-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.2.39 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from langgraph) (0.3.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<2.0.0,>=1.0.2 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from langgraph) (1.0.9)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.117 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (0.1.120)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (23.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (2.9.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from langchain-core<0.4,>=0.2.39->langgraph) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.39->langgraph) (2.4)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.2.39->langgraph) (0.26.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.2.39->langgraph) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.2.39->langgraph) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.39->langgraph) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.39->langgraph) (2.23.3)\n",
      "Requirement already satisfied: anyio in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.2.39->langgraph) (4.2.0)\n",
      "Requirement already satisfied: certifi in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.2.39->langgraph) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.2.39->langgraph) (1.0.2)\n",
      "Requirement already satisfied: idna in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.2.39->langgraph) (3.4)\n",
      "Requirement already satisfied: sniffio in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.2.39->langgraph) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.2.39->langgraph) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.2.39->langgraph) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.2.39->langgraph) (2.0.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.2.39->langgraph) (1.2.0)\n",
      "Downloading langgraph-0.2.23-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m926.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langgraph\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 0.2.19\n",
      "    Uninstalling langgraph-0.2.19:\n",
      "      Successfully uninstalled langgraph-0.2.19\n",
      "Successfully installed langgraph-0.2.23\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "CondaError: KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyserini\n",
    "!pip install faiss-cpu\n",
    "!pip install torch\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "from pyserini.encode import DocumentEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "class LukeDocumentEncoder(DocumentEncoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        tokenizer_name: Optional[str] = None,\n",
    "        device: str = \"cpu\",\n",
    "        l2_norm=False,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                tokenizer_name if tokenizer_name else model_name, use_fast=True\n",
    "            )\n",
    "        except:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                tokenizer_name if tokenizer_name else model_name, use_fast=False\n",
    "            )\n",
    "\n",
    "        self.has_model = True\n",
    "        self.l2_norm = l2_norm\n",
    "\n",
    "    def encode(self, texts, span, max_length=256, **kwargs) -> np.ndarray:\n",
    "        \"\"\"Lukeを用いて入力Entityのベクトル化を行う\"\"\"\n",
    "        tokenizer_kwargs = {\n",
    "            \"max_length\": max_length,\n",
    "            \"truncation\": True,\n",
    "            \"padding\": \"longest\",\n",
    "            \"return_tensors\": \"pt\",\n",
    "        }\n",
    "\n",
    "        inputs = self.tokenizer(text=texts, entity_spans=span, **tokenizer_kwargs)\n",
    "        inputs.to(self.device)\n",
    "\n",
    "        outputs = self.model(**inputs)\n",
    "        entity_vector = outputs.entity_last_hidden_state.detach().cpu().numpy()\n",
    "        batch_size = entity_vector.shape[0]\n",
    "        entity_vector = entity_vector.reshape([batch_size, -1])\n",
    "        if self.l2_norm:\n",
    "            entity_vector = normalize(entity_vector, norm=\"l2\", axis=1)\n",
    "        return entity_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertModel' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m spans \u001b[38;5;241m=\u001b[39m [[(start, end)] \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\n\u001b[1;32m     19\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m: spans,\n\u001b[1;32m     22\u001b[0m }\n\u001b[0;32m---> 23\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     24\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m embeddings\n\u001b[1;32m     25\u001b[0m embedding_writer\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DistilBertModel' object has no attribute 'encode'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "from pyserini.encode import FaissRepresentationWriter\n",
    "\n",
    "# from transformers import AutoModel\n",
    "encoder = LukeDocumentEncoder(\"studio-ousia/luke-japanese-base-lite\")\n",
    "# encoder = AutoModel.from_pretrained(\"bandainamco-mirai/distilbert-base-japanese\")\n",
    "\n",
    "embedding_writer = FaissRepresentationWriter(\"output/encoded\", dimension=768)\n",
    "\n",
    "# batch size 1 のサンプルデータ\n",
    "data = {\n",
    "    \"start\": [0],\n",
    "    \"end\": [3],\n",
    "    \"contexts\": [\"渋谷区（しぶやく）は、東京都の区部南西部に位置する特別区。\"],\n",
    "    \"entity-name\": [\"渋谷区\"],\n",
    "    \"id\": [\"Q193638\"],\n",
    "}\n",
    "\n",
    "with embedding_writer:\n",
    "    spans = [[(start, end)] for start, end in zip(data[\"start\"], data[\"end\"])]\n",
    "    kwargs = {\n",
    "        \"texts\": data[\"contexts\"],\n",
    "        \"span\": spans,\n",
    "    }\n",
    "    embeddings = encoder.encode(**kwargs)\n",
    "    data[\"vector\"] = embeddings\n",
    "    embedding_writer.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pyserini.index.faiss \\\n",
    "  --input /output/encoded \\\n",
    "  --output /output/index \\\n",
    "  --hnsw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pyserini.search import DenseSearchResult, FaissSearcher, PRFDenseSearchResult\n",
    "\n",
    "\n",
    "class LukeFaissSearcher(FaissSearcher):\n",
    "    \"\"\"Faiss searcher for Luke.\n",
    "\n",
    "    This code is based on the following code:\n",
    "    https://github.com/castorini/pyserini/blob/b56d04a823d8fd063614524dec799ef84db0cac1/pyserini/search/faiss/_searcher.py#L379\n",
    "    \"\"\"\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        span: List[Tuple[int, int]],\n",
    "        k: int = 10,\n",
    "        threads: int = 1,\n",
    "        return_vector: bool = False,\n",
    "    ) -> Union[List[DenseSearchResult], Tuple[np.ndarray, List[PRFDenseSearchResult]]]:\n",
    "        emb_q = self.query_encoder.encode(query, span)\n",
    "        assert len(emb_q) == self.dimension\n",
    "        emb_q = emb_q.reshape((1, len(emb_q)))\n",
    "        faiss.omp_set_num_threads(threads)\n",
    "\n",
    "        if return_vector:\n",
    "            distances, indexes, vectors = self.index.search_and_reconstruct(emb_q, k)\n",
    "            vectors = vectors[0]\n",
    "            distances = distances.flat\n",
    "            indexes = indexes.flat\n",
    "            return emb_q, [\n",
    "                PRFDenseSearchResult(self.docids[idx], score, vector)\n",
    "                for score, idx, vector in zip(distances, indexes, vectors)\n",
    "                if idx != -1\n",
    "            ]\n",
    "        else:\n",
    "            distances, indexes = self.index.search(emb_q, k)\n",
    "            distances = distances.flat\n",
    "            indexes = indexes.flat\n",
    "            return [\n",
    "                DenseSearchResult(self.docids[idx], score)\n",
    "                for score, idx in zip(distances, indexes)\n",
    "                if idx != -1\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "from pyserini.search import QueryEncoder\n",
    "\n",
    "\n",
    "class LukeQueryEncoder(LukeDocumentEncoder, QueryEncoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        tokenizer_name: Optional[str] = None,\n",
    "        device: str = \"cuda:0\",\n",
    "        l2_norm=False,\n",
    "    ):\n",
    "        super().__init__(model_name, tokenizer_name, device, l2_norm)\n",
    "\n",
    "    def encode(self, texts, span, max_length=256, **kwargs) -> np.ndarray:\n",
    "        entity_vector = super().encode(texts, span, max_length, **kwargs)\n",
    "        return entity_vector.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LukeQueryEncoder(\"studio-ousia/luke-japanese-base-lite\")\n",
    "searcher = LukeFaissSearcher(\"/path/to/output/index\", encoder)\n",
    "\n",
    "query_mention = {\n",
    "    \"mention\": \"屋久島\",\n",
    "    \"context\": \"同9時30分までの1時間に90mm、降り始めからの合計が319mmとなる豪雨を記録するなど、種子島や屋久島は局地的な豪雨となった。\\n\\n一方気象庁は22日、東日本と西日本を中心に5月の連休明けからの日照時間が\",  # 今回はmentionの周囲50文字を取り出しています\n",
    "    \"start\": 50,\n",
    "    \"end\": 53,\n",
    "}\n",
    "kwargs = {\n",
    "    \"query\": query_mention[\"context\"],\n",
    "    \"span\": [[(query_mention[\"start\"], query_mention[\"end\"])]],\n",
    "}\n",
    "results = seacher.search(**kwargs)\n",
    "\n",
    "for i in range(0, 10):  # top-10\n",
    "    entity_id = results[i].docid\n",
    "    print(f\"{i+1:2} {entity_id} {results[i].score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "9月 21, 2024 9:15:18 午前 org.apache.lucene.store.MMapDirectory lookupProvider\n",
      "警告: You are running with Java 22 or later. To make full use of MMapDirectory, please update Apache Lucene.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-21 09:15:18,429 INFO  [Thread-0] index.SimpleIndexer (SimpleIndexer.java:138) - Using DefaultEnglishAnalyzer\n",
      "2024-09-21 09:15:18,439 INFO  [Thread-0] index.SimpleIndexer (SimpleIndexer.java:139) - Stemmer: porter\n",
      "2024-09-21 09:15:18,439 INFO  [Thread-0] index.SimpleIndexer (SimpleIndexer.java:140) - Keep stopwords? false\n",
      "2024-09-21 09:15:18,440 INFO  [Thread-0] index.SimpleIndexer (SimpleIndexer.java:141) - Stopwords file: null\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LuceneIndexer' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m index_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_index\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m indexer \u001b[38;5;241m=\u001b[39m LuceneIndexer(index_dir)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m(jsonl_file)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# BM25検索器の初期化\u001b[39;00m\n\u001b[1;32m     33\u001b[0m searcher \u001b[38;5;241m=\u001b[39m LuceneSearcher(index_dir)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LuceneIndexer' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pyserini.index.lucene import LuceneIndexer\n",
    "from pyserini.search.faiss import FaissSearcher\n",
    "from pyserini.search.hybrid import HybridSearcher\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# サンプルデータ\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A fast orange fox leaps above a sleepy canine.\",\n",
    "    \"The lazy dog sleeps all day long.\",\n",
    "    \"Foxes are known for their agility and speed.\",\n",
    "]\n",
    "\n",
    "\n",
    "# JSONLファイルの作成\n",
    "jsonl_file = \"sample_docs.jsonl\"\n",
    "with open(jsonl_file, \"w\") as f:\n",
    "    for i, doc in enumerate(documents):\n",
    "        json.dump({\"id\": str(i), \"contents\": doc}, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# インデックスの作成\n",
    "index_dir = \"sample_index\"\n",
    "indexer = LuceneIndexer(index_dir)\n",
    "indexer.index(jsonl_file)\n",
    "\n",
    "# BM25検索器の初期化\n",
    "searcher = LuceneSearcher(index_dir)\n",
    "\n",
    "# 密ベクトル検索のためのインデックス作成\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # 軽量なモデルを使用\n",
    "embeddings = model.encode(documents)\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(np.array(embeddings).astype(\"float32\"))\n",
    "\n",
    "# FAISSSearcherの初期化\n",
    "dsearcher = FaissSearcher(index, model)\n",
    "\n",
    "# ハイブリッド検索器の作成\n",
    "hsearcher = HybridSearcher(dsearcher, searcher)\n",
    "\n",
    "# 検索の実行\n",
    "query = \"fast fox\"\n",
    "hits = hsearcher.search(query)\n",
    "\n",
    "# 結果の表示\n",
    "for i, hit in enumerate(hits):\n",
    "    print(f\"{i+1:2} {hit.docid:4} {hit.score:.5f} {documents[int(hit.docid)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "/Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "9月 21, 2024 8:46:55 午前 org.apache.lucene.store.MMapDirectory lookupProvider\n",
      "警告: You are running with Java 22 or later. To make full use of MMapDirectory, please update Apache Lucene.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 7157707 11.00830\n",
      " 2 6034357 10.94310\n",
      " 3 5837606 10.81740\n",
      " 4 7157715 10.59820\n",
      " 5 6034350 10.48360\n",
      " 6 2900045 10.31190\n",
      " 7 7157713 10.12300\n",
      " 8 1584344 10.05290\n",
      " 9 533614  9.96350\n",
      "10 6234461 9.92200\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "searcher = LuceneSearcher.from_prebuilt_index(\"msmarco-v1-passage\")\n",
    "hits = searcher.search(\"what is a lobster roll?\")\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(f\"{i+1:2} {hits[i].docid:7} {hits[i].score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to initialize pre-built index msmarco-v1-passage-unicoil.\n",
      "Unrecognized index name msmarco-v1-passage-unicoil\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyserini\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlucene\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LuceneImpactSearcher\n\u001b[1;32m      3\u001b[0m searcher \u001b[38;5;241m=\u001b[39m LuceneImpactSearcher\u001b[38;5;241m.\u001b[39mfrom_prebuilt_index(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsmarco-v1-passage-unicoil\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcastorini/unicoil-msmarco-passage\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m hits \u001b[38;5;241m=\u001b[39m \u001b[43msearcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhat is a lobster roll?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhits[i]\u001b[38;5;241m.\u001b[39mdocid\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhits[i]\u001b[38;5;241m.\u001b[39mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'search'"
     ]
    }
   ],
   "source": [
    "from pyserini.search.lucene import LuceneImpactSearcher\n",
    "\n",
    "searcher = LuceneImpactSearcher.from_prebuilt_index(\n",
    "    \"msmarco-v1-passage-unicoil\", \"castorini/unicoil-msmarco-passage\"\n",
    ")\n",
    "hits = searcher.search(\"what is a lobster roll?\")\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(f\"{i+1:2} {hits[i].docid:7} {hits[i].score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LuceneImpactSearcher.from_prebuilt_index() missing 1 required positional argument: 'query_encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyserini\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlucene\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LuceneImpactSearcher\n\u001b[0;32m----> 3\u001b[0m searcher \u001b[38;5;241m=\u001b[39m \u001b[43mLuceneImpactSearcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_prebuilt_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmsmarco-v1-passage-unicoil\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m hits \u001b[38;5;241m=\u001b[39m searcher\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhat is a lobster roll?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: LuceneImpactSearcher.from_prebuilt_index() missing 1 required positional argument: 'query_encoder'"
     ]
    }
   ],
   "source": [
    "from pyserini.search.lucene import LuceneImpactSearcher\n",
    "\n",
    "searcher = LuceneImpactSearcher.from_prebuilt_index(\"msmarco-v1-passage-unicoil\")\n",
    "hits = searcher.search(\"what is a lobster roll?\")\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(f\"{i+1:2} {hits[i].docid:7} {hits[i].score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmina/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to initialize pre-built index msmarco-passage-tct_colbert-hnsw.\n",
      "Downloading index at https://rgw.cs.uwaterloo.ca/pyserini/indexes/faiss/faiss-hnsw.msmarco-v1-passage.tct_colbert.20210112.be7119.tar.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "faiss-hnsw.msmarco-v1-passage.tct_colbert.20210112.be7119.tar.gz: 31.1GB [5:24:47, 1.71MB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/tmina/.cache/pyserini/indexes/faiss-hnsw.msmarco-v1-passage.tct_colbert.20210112.be7119.tar.gz into /Users/tmina/.cache/pyserini/indexes/faiss-hnsw.msmarco-v1-passage.tct_colbert.20210112.be7119.6b7285a7f0163d1a547214396be20488...\n",
      "Initializing msmarco-v1-passage.tct_colbert.hnsw...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "from pyserini.search.faiss import FaissSearcher, TctColBertQueryEncoder\n",
    "\n",
    "encoder = TctColBertQueryEncoder(\"castorini/tct_colbert-msmarco\")\n",
    "searcher = FaissSearcher.from_prebuilt_index(\n",
    "    \"msmarco-passage-tct_colbert-hnsw\", encoder\n",
    ")\n",
    "hits = searcher.search(\"what is a lobster roll\")\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(f\"{i+1:2} {hits[i].docid:7} {hits[i].score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.faiss import FaissSearcher, TctColBertQueryEncoder\n",
    "from pyserini.search.hybrid import HybridSearcher\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "ssearcher = LuceneSearcher.from_prebuilt_index(\"msmarco-v1-passage\")\n",
    "encoder = TctColBertQueryEncoder(\"castorini/tct_colbert-msmarco\")\n",
    "dsearcher = FaissSearcher.from_prebuilt_index(\n",
    "    \"msmarco-passage-tct_colbert-hnsw\", encoder\n",
    ")\n",
    "hsearcher = HybridSearcher(dsearcher, ssearcher)\n",
    "hits = hsearcher.search(\"what is a lobster roll\")\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(f\"{i+1:2} {hits[i].docid:7} {hits[i].score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "- aa\n",
    "- aa\n",
    "- aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Documentオブジェクトのリストを作成\n",
    "documents = [\n",
    "    Document(page_content=\"LangChainは自然言語処理のためのフレームワークです。\"),\n",
    "    Document(page_content=\"FAISSは効率的な類似度検索を行うライブラリです。\"),\n",
    "    Document(page_content=\"BM25はキーワード検索アルゴリズムの一つです。\"),\n",
    "    Document(page_content=\"ハイブリッド検索は複数の検索手法を組み合わせます。\"),\n",
    "]\n",
    "\n",
    "# BM25検索の設定\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "\n",
    "# ベクトル検索の設定\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "faiss_retriever = vectorstore.as_retriever()\n",
    "\n",
    "# ハイブリッド検索の設定\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# 検索の実行\n",
    "query = \"効率的な検索手法\"\n",
    "results = ensemble_retriever.get_relevant_documents(query)\n",
    "\n",
    "for doc in results:\n",
    "    print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
