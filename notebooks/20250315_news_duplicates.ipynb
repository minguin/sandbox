{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from janome.tokenizer import Tokenizer\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import simhash\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "#######################################\n",
    "# 基本的なカバー率計算の各手法\n",
    "#######################################\n",
    "\n",
    "def simple_coverage(list_a, list_b, threshold=0.5):\n",
    "    \"\"\"文字列類似度ベースのカバー率計算\"\"\"\n",
    "    a_covered = sum(1 for a in list_a if any(SequenceMatcher(None, a, b).ratio() >= threshold for b in list_b))\n",
    "    b_covered = sum(1 for b in list_b if any(SequenceMatcher(None, a, b).ratio() >= threshold for a in list_a))\n",
    "    \n",
    "    return a_covered / len(list_a), b_covered / len(list_b)\n",
    "\n",
    "def ngram_coverage(list_a, list_b, n=2, threshold=0.3):\n",
    "    \"\"\"文字n-gramのJaccard係数によるカバー率計算\"\"\"\n",
    "    def get_ngrams(text, n):\n",
    "        return set(text[i:i+n] for i in range(len(text) - n + 1))\n",
    "    \n",
    "    ngrams_a = [get_ngrams(text, n) for text in list_a]\n",
    "    ngrams_b = [get_ngrams(text, n) for text in list_b]\n",
    "    \n",
    "    a_covered = sum(1 for a_set in ngrams_a if \n",
    "                   any(len(a_set & b_set) / len(a_set | b_set) >= threshold for b_set in ngrams_b))\n",
    "    b_covered = sum(1 for b_set in ngrams_b if \n",
    "                   any(len(a_set & b_set) / len(a_set | b_set) >= threshold for a_set in ngrams_a))\n",
    "    \n",
    "    return a_covered / len(list_a), b_covered / len(list_b)\n",
    "\n",
    "def janome_coverage(list_a, list_b, threshold=0.4):\n",
    "    \"\"\"Janomeによる形態素解析を使用したカバー率計算\"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    \n",
    "    # 内容語のみを抽出\n",
    "    def get_content_words(text):\n",
    "        content_pos = ('名詞', '動詞', '形容詞', '副詞')\n",
    "        return {t.base_form for t in tokenizer.tokenize(text) \n",
    "                if t.part_of_speech.split(',')[0] in content_pos}\n",
    "    \n",
    "    tokens_a = [get_content_words(text) for text in list_a]\n",
    "    tokens_b = [get_content_words(text) for text in list_b]\n",
    "    \n",
    "    a_covered = sum(1 for a_tokens in tokens_a if \n",
    "                   any(len(a_tokens & b_tokens) / len(a_tokens | b_tokens) >= threshold \n",
    "                      for b_tokens in tokens_b if a_tokens and b_tokens))\n",
    "    b_covered = sum(1 for b_tokens in tokens_b if \n",
    "                   any(len(a_tokens & b_tokens) / len(a_tokens | b_tokens) >= threshold \n",
    "                      for a_tokens in tokens_a if a_tokens and b_tokens))\n",
    "    \n",
    "    return a_covered / len(list_a), b_covered / len(list_b)\n",
    "\n",
    "def tfidf_coverage(list_a, list_b, threshold=0.5):\n",
    "    \"\"\"TF-IDFベクトル化と余弦類似度を用いたカバー率計算\"\"\"\n",
    "    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "    all_headlines = list_a + list_b\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_headlines)\n",
    "    \n",
    "    tfidf_a = tfidf_matrix[:len(list_a)]\n",
    "    tfidf_b = tfidf_matrix[len(list_a):]\n",
    "    similarity_matrix = cosine_similarity(tfidf_a, tfidf_b)\n",
    "    \n",
    "    a_covered = sum(1 for i in range(len(list_a)) if np.max(similarity_matrix[i]) >= threshold)\n",
    "    b_covered = sum(1 for j in range(len(list_b)) if np.max(similarity_matrix[:, j]) >= threshold)\n",
    "    \n",
    "    return a_covered / len(list_a), b_covered / len(list_b)\n",
    "\n",
    "\n",
    "def minhash_coverage(list_a, list_b, num_perm=128, threshold=0.6):\n",
    "    \"\"\"MinHashとLSHを使用したカバー率計算\"\"\"\n",
    "    def get_shingles(text, k=2):\n",
    "        text = re.sub(r'\\s+', '', text.lower())\n",
    "        return [text[i:i+k] for i in range(len(text)-k+1)]\n",
    "    \n",
    "    # MinHashオブジェクトを作成\n",
    "    def create_minhash(text):\n",
    "        m = MinHash(num_perm=num_perm)\n",
    "        for shingle in get_shingles(text):\n",
    "            m.update(shingle.encode('utf8'))\n",
    "        return m\n",
    "    \n",
    "    # 各ヘッドラインのMinHashを計算\n",
    "    minhashes_a = [create_minhash(text) for text in list_a]\n",
    "    minhashes_b = [create_minhash(text) for text in list_b]\n",
    "    \n",
    "    # A→Bのカバー率を計算\n",
    "    lsh_b = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "    for i, mh in enumerate(minhashes_b):\n",
    "        lsh_b.insert(i, mh)\n",
    "    \n",
    "    a_covered = sum(1 for mh_a in minhashes_a if lsh_b.query(mh_a))\n",
    "    \n",
    "    # B→Aのカバー率を計算\n",
    "    lsh_a = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "    for i, mh in enumerate(minhashes_a):\n",
    "        lsh_a.insert(i, mh)\n",
    "    \n",
    "    b_covered = sum(1 for mh_b in minhashes_b if lsh_a.query(mh_b))\n",
    "    \n",
    "    return a_covered / len(list_a), b_covered / len(list_b)\n",
    "\n",
    "def simhash_coverage(list_a, list_b, threshold=3):\n",
    "    \"\"\"SimHashを使用したカバー率計算\"\"\"\n",
    "    def get_features(text):\n",
    "        # 日本語テキストのn-gramを特徴量として使用\n",
    "        text = text.lower()\n",
    "        return [text[i:i+3] for i in range(len(text)-2)]\n",
    "    \n",
    "    # 各ヘッドラインのSimHashを計算\n",
    "    hashes_a = [simhash.Simhash(get_features(text)) for text in list_a]\n",
    "    hashes_b = [simhash.Simhash(get_features(text)) for text in list_b]\n",
    "    \n",
    "    # A→Bのカバー率を計算\n",
    "    a_covered = sum(1 for h_a in hashes_a if \n",
    "                   any(h_a.distance(h_b) <= threshold for h_b in hashes_b))\n",
    "    \n",
    "    # B→Aのカバー率を計算\n",
    "    b_covered = sum(1 for h_b in hashes_b if \n",
    "                   any(h_b.distance(h_a) <= threshold for h_a in hashes_a))\n",
    "    \n",
    "    return a_covered / len(list_a), b_covered / len(list_b)\n",
    "\n",
    "def entity_coverage(list_a, list_b, threshold=0.3):\n",
    "    \"\"\"固有表現を利用したカバー率計算\"\"\"\n",
    "    nlp = spacy.load(\"ja_core_news_sm\")\n",
    "    \n",
    "    def get_entities(text):\n",
    "        doc = nlp(text)\n",
    "        return {(ent.text, ent.label_) for ent in doc.ents}\n",
    "    \n",
    "    # 一方向のカバー率を計算する関数\n",
    "    def calc_directional_coverage(source_entities, target_entities):\n",
    "        covered = 0\n",
    "        for ent_src in source_entities:\n",
    "            if not ent_src:\n",
    "                continue\n",
    "                \n",
    "            if any(len(ent_src & ent_tgt) / len(ent_src | ent_tgt) >= threshold \n",
    "                  for ent_tgt in target_entities if ent_tgt):\n",
    "                covered += 1\n",
    "                \n",
    "        return covered / len(source_entities) if source_entities else 0\n",
    "    \n",
    "    entities_a = [get_entities(text) for text in list_a]\n",
    "    entities_b = [get_entities(text) for text in list_b]\n",
    "    \n",
    "    return (calc_directional_coverage(entities_a, entities_b), \n",
    "            calc_directional_coverage(entities_b, entities_a))\n",
    "\n",
    "#######################################\n",
    "# 統合関数\n",
    "#######################################\n",
    "def calculate_coverage(list_a, list_b, method='tfidf', **kwargs):\n",
    "    \"\"\"ヘッドラインカバー率を計算する統合関数\"\"\"\n",
    "    methods = {\n",
    "        'simple': simple_coverage,\n",
    "        'ngram': ngram_coverage,\n",
    "        'janome': janome_coverage,\n",
    "        'tfidf': tfidf_coverage,\n",
    "        'minhash': minhash_coverage,\n",
    "        'simhash': simhash_coverage,\n",
    "        'entity': entity_coverage\n",
    "    }\n",
    "    \n",
    "    return methods[method](list_a, list_b, **kwargs)\n",
    "\n",
    "#######################################\n",
    "# メイン関数: 各手法を比較\n",
    "#######################################\n",
    "def compare_methods(list_a, list_b, show_details=True):\n",
    "    \"\"\"各カバー率計算手法の結果を比較して表示する\"\"\"\n",
    "    if show_details:\n",
    "        print(\"リストA:\")\n",
    "        for i, headline in enumerate(list_a):\n",
    "            print(f\"  A{i+1}: {headline}\")\n",
    "        print(\"\\nリストB:\")\n",
    "        for i, headline in enumerate(list_b):\n",
    "            print(f\"  B{i+1}: {headline}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(f\"リストサイズ: A={len(list_a)}件, B={len(list_b)}件\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    methods = [\n",
    "        ('simple', {'threshold': 0.5}, 'シンプル文字列類似度'),\n",
    "        ('ngram', {'n': 2, 'threshold': 0.3}, 'N-gram Jaccard係数'),\n",
    "        ('janome', {'threshold': 0.4}, '形態素解析(Janome)'),\n",
    "        ('tfidf', {'threshold': 0.5}, 'TF-IDF+余弦類似度'),\n",
    "        ('minhash', {'threshold': 0.6}, 'MinHash/LSH'),\n",
    "        ('simhash', {'threshold': 3}, 'SimHash'),\n",
    "        ('entity', {'threshold': 0.3}, '固有表現抽出')\n",
    "    ]\n",
    "    \n",
    "    print(f\"{'手法':<20} {'A→Bのカバー率':<15} {'B→Aのカバー率':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for method, kwargs, desc in methods:\n",
    "        try:\n",
    "            a_cov, b_cov = calculate_coverage(list_a, list_b, method=method, **kwargs)\n",
    "            print(f\"{desc:<20} {a_cov:.2%} {' '*8} {b_cov:.2%}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{desc:<20} エラー: {str(e)}\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "閾値の最適化を開始...\n",
      "simpleの最適な閾値を探索中...\n",
      "  simple: 最適な閾値 = 0.3500000000000001\n",
      "ngramの最適な閾値を探索中...\n",
      "  ngram: 最適な閾値 = 0.1\n",
      "janomeの最適な閾値を探索中...\n",
      "  janome: 最適な閾値 = 0.20000000000000004\n",
      "tfidfの最適な閾値を探索中...\n",
      "  tfidf: 最適な閾値 = 0.1\n",
      "minhashの最適な閾値を探索中...\n",
      "  minhash: 最適な閾値 = 0.20000000000000004\n",
      "simhashの最適な閾値を探索中...\n",
      "  simhash: 最適な閾値 = 1\n",
      "entityの最適な閾値を探索中...\n",
      "  entity: 最適な閾値 = 0.1\n",
      "\n",
      "===== 最適化された閾値 =====\n",
      "シンプル文字列類似度: 0.3500000000000001\n",
      "N-gram Jaccard係数: 0.1\n",
      "形態素解析(Janome): 0.20000000000000004\n",
      "TF-IDF+余弦類似度: 0.1\n",
      "MinHash/LSH: 0.20000000000000004\n",
      "SimHash: 1\n",
      "固有表現抽出: 0.1\n",
      "\n",
      "===== データセット1: 非常に類似した短いヘッドライン (最適化閾値) =====\n",
      "リストサイズ: A=20件, B=21件\n",
      "----------------------------------------------------------------------\n",
      "手法                   A→Bのカバー率        B→Aのカバー率        閾値        \n",
      "----------------------------------------------------------------------\n",
      "シンプル文字列類似度           80.00%          71.43%      0.3500000000000001\n",
      "N-gram Jaccard係数     100.00%          95.24%      0.1\n",
      "形態素解析(Janome)        90.00%          85.71%      0.20000000000000004\n",
      "TF-IDF+余弦類似度         85.00%          80.95%      0.1\n",
      "MinHash/LSH          70.00%          66.67%      0.20000000000000004\n",
      "SimHash              0.00%          0.00%      1\n",
      "固有表現抽出               45.00%          42.86%      0.1\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "===== データセット2: より長い非常に類似したヘッドライン (最適化閾値) =====\n",
      "リストサイズ: A=19件, B=20件\n",
      "----------------------------------------------------------------------\n",
      "手法                   A→Bのカバー率        B→Aのカバー率        閾値        \n",
      "----------------------------------------------------------------------\n",
      "シンプル文字列類似度           89.47%          85.00%      0.3500000000000001\n",
      "N-gram Jaccard係数     100.00%          95.00%      0.1\n",
      "形態素解析(Janome)        94.74%          90.00%      0.20000000000000004\n",
      "TF-IDF+余弦類似度         100.00%          95.00%      0.1\n",
      "MinHash/LSH          94.74%          85.00%      0.20000000000000004\n",
      "SimHash              0.00%          0.00%      1\n",
      "固有表現抽出               68.42%          65.00%      0.1\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "===== データセット4: 全く似ていないヘッドライン (最適化閾値) =====\n",
      "リストサイズ: A=21件, B=21件\n",
      "----------------------------------------------------------------------\n",
      "手法                   A→Bのカバー率        B→Aのカバー率        閾値        \n",
      "----------------------------------------------------------------------\n",
      "シンプル文字列類似度           0.00%          0.00%      0.3500000000000001\n",
      "N-gram Jaccard係数     9.52%          9.52%      0.1\n",
      "形態素解析(Janome)        0.00%          0.00%      0.20000000000000004\n",
      "TF-IDF+余弦類似度         4.76%          4.76%      0.1\n",
      "MinHash/LSH          4.76%          4.76%      0.20000000000000004\n",
      "SimHash              0.00%          0.00%      1\n",
      "固有表現抽出               4.76%          4.76%      0.1\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def optimize_threshold_parameters(headlines_a1, headlines_b1, headlines_a2, headlines_b2, headlines_a4, headlines_b4):\n",
    "    \"\"\"\n",
    "    各手法の閾値を最適化して、データセット1・2では高いカバー率、データセット4では低いカバー率になるようにする\n",
    "    \n",
    "    Args:\n",
    "        headlines_a1, headlines_b1: データセット1（非常に類似した短いヘッドライン）\n",
    "        headlines_a2, headlines_b2: データセット2（より長い非常に類似したヘッドライン）\n",
    "        headlines_a4, headlines_b4: データセット4（全く似ていないヘッドライン）\n",
    "        \n",
    "    Returns:\n",
    "        dict: 各手法の最適な閾値を含む辞書\n",
    "    \"\"\"\n",
    "    # 手法と探索する閾値範囲の定義\n",
    "    methods_and_ranges = {\n",
    "        'simple': {'param': 'threshold', 'range': np.arange(0.1, 0.9, 0.05)},\n",
    "        'ngram': {'param': 'threshold', 'range': np.arange(0.1, 0.9, 0.05), 'fixed': {'n': 2}},\n",
    "        'janome': {'param': 'threshold', 'range': np.arange(0.1, 0.9, 0.05)},\n",
    "        'tfidf': {'param': 'threshold', 'range': np.arange(0.1, 0.9, 0.05)},\n",
    "        'minhash': {'param': 'threshold', 'range': np.arange(0.1, 0.9, 0.05), 'fixed': {'num_perm': 128}},\n",
    "        'simhash': {'param': 'threshold', 'range': np.arange(1, 10, 1)},  # SimHashは整数値の範囲\n",
    "        'entity': {'param': 'threshold', 'range': np.arange(0.1, 0.9, 0.05)}\n",
    "    }\n",
    "    \n",
    "    # 最適な閾値を格納する辞書\n",
    "    optimal_thresholds = {}\n",
    "    \n",
    "    print(\"閾値の最適化を開始...\")\n",
    "    \n",
    "    # 各手法について最適な閾値を探索\n",
    "    for method, settings in methods_and_ranges.items():\n",
    "        best_score = -float('inf')\n",
    "        best_threshold = None\n",
    "        param_name = settings['param']\n",
    "        \n",
    "        print(f\"{method}の最適な閾値を探索中...\")\n",
    "        \n",
    "        for threshold in settings['range']:\n",
    "            # この手法のパラメータを設定\n",
    "            params = {param_name: threshold}\n",
    "            if 'fixed' in settings:\n",
    "                params.update(settings['fixed'])\n",
    "            \n",
    "            try:\n",
    "                # 各データセットのカバー率を計算\n",
    "                cov_a1_to_b1, cov_b1_to_a1 = calculate_coverage(headlines_a1, headlines_b1, method=method, **params)\n",
    "                cov_a2_to_b2, cov_b2_to_a2 = calculate_coverage(headlines_a2, headlines_b2, method=method, **params)\n",
    "                cov_a4_to_b4, cov_b4_to_a4 = calculate_coverage(headlines_a4, headlines_b4, method=method, **params)\n",
    "                \n",
    "                # 類似データセット（1と2）の平均カバー率\n",
    "                similar_cov = (cov_a1_to_b1 + cov_b1_to_a1 + cov_a2_to_b2 + cov_b2_to_a2) / 4\n",
    "                \n",
    "                # 非類似データセット（4）の平均カバー率\n",
    "                dissimilar_cov = (cov_a4_to_b4 + cov_b4_to_a4) / 2\n",
    "                \n",
    "                # スコア関数：類似データセットのカバー率を最大化し、非類似データセットとの差を最大化\n",
    "                # 非類似データセットのカバー率には高いペナルティを与える\n",
    "                score = similar_cov - 3 * dissimilar_cov\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_threshold = threshold\n",
    "            \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # 最適な閾値を格納\n",
    "        if best_threshold is not None:\n",
    "            optimal_thresholds[method] = best_threshold\n",
    "            print(f\"  {method}: 最適な閾値 = {best_threshold}\")\n",
    "        else:\n",
    "            print(f\"  {method}: 最適な閾値が見つかりませんでした\")\n",
    "    \n",
    "    return optimal_thresholds\n",
    "\n",
    "# 最適な閾値でカバー率を計算して表示する関数\n",
    "def show_coverage_with_optimal_thresholds(optimal_thresholds, headlines_a, headlines_b, dataset_name):\n",
    "    \"\"\"\n",
    "    最適化された閾値を使用してカバー率を計算し表示する\n",
    "    \n",
    "    Args:\n",
    "        optimal_thresholds: 最適化された閾値を含む辞書\n",
    "        headlines_a, headlines_b: 比較するヘッドラインの2つのリスト\n",
    "        dataset_name: データセットの説明\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== {dataset_name} (最適化閾値) =====\")\n",
    "    print(f\"リストサイズ: A={len(headlines_a)}件, B={len(headlines_b)}件\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    methods_desc = {\n",
    "        'simple': 'シンプル文字列類似度',\n",
    "        'ngram': 'N-gram Jaccard係数',\n",
    "        'janome': '形態素解析(Janome)',\n",
    "        'tfidf': 'TF-IDF+余弦類似度',\n",
    "        'minhash': 'MinHash/LSH',\n",
    "        'simhash': 'SimHash',\n",
    "        'entity': '固有表現抽出'\n",
    "    }\n",
    "    \n",
    "    print(f\"{'手法':<20} {'A→Bのカバー率':<15} {'B→Aのカバー率':<15} {'閾値':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for method, threshold in optimal_thresholds.items():\n",
    "        try:\n",
    "            # 各手法に特有のパラメータを設定\n",
    "            params = {'threshold': threshold}\n",
    "            if method == 'ngram':\n",
    "                params['n'] = 2\n",
    "            elif method == 'minhash':\n",
    "                params['num_perm'] = 128\n",
    "            \n",
    "            # カバー率を計算\n",
    "            a_cov, b_cov = calculate_coverage(headlines_a, headlines_b, method=method, **params)\n",
    "            print(f\"{methods_desc[method]:<20} {a_cov:.2%} {' '*8} {b_cov:.2%} {' '*4} {threshold}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{methods_desc[method]:<20} エラー: {str(e)}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# メイン処理：閾値の最適化と各データセットでのカバー率比較\n",
    "def optimize_and_compare_all_datasets():\n",
    "    \"\"\"\n",
    "    すべてのデータセットに対して閾値を最適化し、結果を比較する\n",
    "    \"\"\"\n",
    "    # データセット1: 非常に類似した短いヘッドライン\n",
    "    headlines_a1 = [\n",
    "        \"日経平均が3.2%上昇、政府の経済対策を好感\",\n",
    "        \"コロナ新変異株「XZ型」の感染例が国内で初確認、専門家が警戒呼びかけ\",\n",
    "        \"東京オリンピック開催に向け組織委が最終準備、IOCが支持表明\",\n",
    "        \"環境規制強化法案が可決、企業に対応求める\",\n",
    "        \"米国FRBが利上げを発表、0.25%の利上げで市場予想通り\",\n",
    "        \"GAFA4社の決算、クラウド事業が好調で増収増益に貢献\",\n",
    "        \"都心の新築マンション価格が過去最高を更新、平均6500万円に\",\n",
    "        \"女子サッカー代表が世界ランキング3位に浮上、監督が手腕を評価される\",\n",
    "        \"大手銀行3行の統合決定、金融再編の動きが加速\",\n",
    "        \"AI技術の特許出願が前年比50%増、競争激化を反映\",\n",
    "        \"新薬の臨床試験結果が発表、成功率は当初予想を上回る\",\n",
    "        \"プラスチック削減法が成立、使い捨て容器に新税導入へ\",\n",
    "        \"テレワーク実施率が調査開始以来最高を記録、企業の67%が導入\",\n",
    "        \"全国の賃金指数が前年同月比3.8%増、7年ぶりの高水準\",\n",
    "        \"暗号資産市場が急回復、ビットコインは半年ぶり高値圏に\",\n",
    "        \"新エネルギー関連株に資金流入、政府の脱炭素方針を好感\",\n",
    "        \"観光客数が回復基調、インバウンド消費が地方経済を下支え\",\n",
    "        \"工場の生産稼働率が過去最高を更新、部品供給問題が解消\",\n",
    "        \"金融庁が投資家保護の新ガイドラインを発表、来年度から適用\",\n",
    "        \"土地価格指数が全国平均で2.5%上昇、6年連続のプラス成長に\"\n",
    "    ]\n",
    "\n",
    "    headlines_b1 = [\n",
    "        \"政府経済対策を受けて日経平均3.2%高、18カ月ぶり高値\",\n",
    "        \"国内初、新型コロナ「XZ型」変異株の感染確認、重症化リスク調査中\",\n",
    "        \"IOCが東京五輪開催を全面支持、準備は最終段階に\",\n",
    "        \"新環境規制法案が可決、企業の排出削減義務化\",\n",
    "        \"米連邦準備制度理事会が0.25%の利上げを決定、年内あと1回の見通し\",\n",
    "        \"IT大手4社、クラウドサービスが牽引し第2四半期は予想上回る決算\",\n",
    "        \"首都圏マンション平均価格が6500万円に到達、バブル期超え\",\n",
    "        \"新電気自動車の予約開始、航続距離は競合の1.5倍に\",\n",
    "        \"女子サッカーナショナルチーム、世界ランク3位に上昇、指揮官の采配が成功\",\n",
    "        \"金融大手3行が経営統合を正式発表、業界再編が本格化\",\n",
    "        \"人工知能関連特許の出願数が半数増、企業間の開発競争が激化\",\n",
    "        \"新開発薬剤の治験成果を公表、有効性は予測を上回る結果に\",\n",
    "        \"プラスチック規制新法が国会通過、使い捨て製品への課税が決定\",\n",
    "        \"在宅勤務導入率67%で過去最高、調査開始以来の最高値を記録\",\n",
    "        \"全国平均賃金が3.8%上昇、7年ぶりの大幅アップを記録\",\n",
    "        \"仮想通貨相場が急騰、主要通貨ビットコインは6ヶ月ぶりの高値に\",\n",
    "        \"脱炭素関連企業の株価が上昇、政府方針を受けて投資家の関心高まる\",\n",
    "        \"訪日客数の回復が鮮明に、外国人観光消費が地域経済を活性化\",\n",
    "        \"製造業の工場稼働率が記録的水準に、部品調達が正常化\",\n",
    "        \"金融当局が投資家向け新保護指針を策定、翌年度より実施\",\n",
    "        \"地価全国平均2.5%上昇、6年連続で前年を上回る\"\n",
    "    ]\n",
    "\n",
    "    # データセット2: より長い非常に類似したヘッドライン\n",
    "    headlines_a2 = [\n",
    "        \"政府は総額20兆円規模の新たな経済対策を閣議決定、企業の設備投資減税や個人消費喚起策を含む\",\n",
    "        \"世界保健機関(WHO)は新型コロナウイルスの新変異株「XZ型」について専門家会議を開催、現時点で深刻な懸念はないと発表\",\n",
    "        \"東京五輪の開催1か月前、IOCと組織委員会が安全対策を強化、無観客開催の可能性も検討\",\n",
    "        \"大手自動車メーカーA社が新型電気自動車を発表、航続距離800kmで業界最長、価格は450万円から\",\n",
    "        \"台風19号が週末に西日本に接近の見込み、気象庁は高波と大雨に警戒するよう呼びかけ\",\n",
    "        \"中央銀行が金融政策決定会合で現行の金融緩和策維持を決定、市場は好感して株高に\",\n",
    "        \"国内の主要携帯電話会社3社が新料金プランを発表、データ通信量無制限で月額料金は平均15%引き下げへ、総務省の指導に対応\",\n",
    "        \"次世代半導体の製造技術で日本企業連合が共同開発プロジェクトを始動、政府が5年間で総額1兆円の支援を表明、国際競争力回復目指す\",\n",
    "        \"医療保険制度改革法案が可決、高齢者の自己負担割合を段階的に引き上げ、2025年度から完全施行、財政健全化が目的\",\n",
    "        \"人工知能を活用した新型の気象予測システムを気象庁が導入、予測精度が従来比30%向上、局地的豪雨の早期警戒に期待\",\n",
    "        \"大手商社5社の4-6月期決算が出揃い、全社が最高益を更新、資源価格高騰と円安が追い風、株主還元も積極化\",\n",
    "        \"国立大学の授業料を世帯年収に応じて無償化する新制度が2024年度からスタート、年収700万円未満が対象に\",\n",
    "        \"再生可能エネルギーの発電比率が初めて30%を突破、太陽光と風力の設備増強が寄与、2030年目標の40%達成に前進\",\n",
    "        \"国内最大の電子商取引企業が物流センターの完全自動化を発表、AI制御のロボットが商品の仕分け・梱包を担当、人手不足解消へ\",\n",
    "        \"世界最速となる次世代通信規格「6G」の国際標準化で日米欧連合が合意、2030年の実用化目指し共同研究開発へ\",\n",
    "        \"大都市圏の鉄道各社が運賃改定を申請、10月から平均12%値上げへ、人件費高騰とエネルギーコスト増が理由\",\n",
    "        \"海洋プラスチック削減に向けた国際条約が発効、日本も批准、2035年までに海洋流出量を80%削減する目標\",\n",
    "        \"新型宇宙望遠鏡の観測データから太陽系外の地球型惑星に水の存在を示す証拠を発見、生命存在の可能性に期待高まる\",\n",
    "        \"政府が子育て支援の新パッケージを発表、第2子以降の保育料完全無償化や児童手当増額など、少子化対策を強化\"\n",
    "    ]\n",
    "\n",
    "    headlines_b2 = [\n",
    "        \"閣議決定された20兆円規模の経済対策、企業投資減税と消費喚起策の二本柱で景気回復目指す\",\n",
    "        \"WHO専門家会議、コロナ新変異株「XZ型」は感染力がやや強いが現時点で深刻な脅威ではないと結論\",\n",
    "        \"東京オリンピック開幕まで1ヶ月、感染対策強化でIOCと組織委が合意、無観客選択肢も\",\n",
    "        \"A社が次世代EV「エコフューチャー」を正式発表、一充電あたり800kmの走行を実現、税込450万円から\",\n",
    "        \"気象庁：台風19号が週末に九州・四国地方に接近、暴風雨に警戒を\",\n",
    "        \"金融政策決定会合、ゼロ金利政策と資産買入れ継続を決定、インフレ目標には届かず\",\n",
    "        \"ノーベル物理学賞に日米の共同研究チーム、量子コンピューティングの研究で受賞\",\n",
    "        \"大手携帯3社、新たな料金体系を一斉発表、データ使い放題で平均15%値下げ、総務省からの要請受け改定\",\n",
    "        \"国内企業連合による次世代半導体開発プロジェクト発足、政府が1兆円規模の資金援助を決定、技術覇権競争に参戦\",\n",
    "        \"医療保険改革法が成立、高齢者負担率を順次引き上げ、25年度より全面適用、財政再建へ一歩前進\",\n",
    "        \"気象庁がAI搭載の新予報システム運用開始、従来より予測精度30%向上、ゲリラ豪雨対策に効果期待\",\n",
    "        \"主要商社5社、第1四半期で過去最高益を達成、資源高と円安効果で収益拡大、配当増額も発表\",\n",
    "        \"国立大の学費無償化制度が確定、年収700万円以下の世帯対象、24年度入学生から適用へ\",\n",
    "        \"再エネ発電シェアが初の30%突破、太陽光・風力発電所の増設が進み、30年目標の40%に向け順調\",\n",
    "        \"EC大手が物流施設の無人化技術を導入、AIロボットによる完全自動化で出荷処理、人員不足を解消\",\n",
    "        \"6G通信規格の共同開発に日米欧が基本合意、次世代通信技術で連携、30年商用化を目標に研究加速\",\n",
    "        \"首都圏の鉄道会社が運賃引き上げを正式申請、10月より約12%値上げ実施へ、コスト増加が主因\",\n",
    "        \"海洋プラスチック対策国際条約が正式発効、日本も参加表明、35年までに流出量8割減を目指す\",\n",
    "        \"新観測機器が系外惑星の大気中に水分子を検出、生命存在の可能性を示す重要な発見と専門家\",\n",
    "        \"子育て支援強化策を政府が正式発表、第2子以降の保育完全無料化など、少子化に歯止めをかける狙い\"\n",
    "    ]\n",
    "\n",
    "    # データセット4: 全く似ていないヘッドライン\n",
    "    headlines_a4 = [\n",
    "        \"国内最大規模の美術展が東京で開幕、欧州の名画80点を展示\",\n",
    "        \"高速道路の渋滞予測システムがAI活用で精度向上、連休前に実用化\",\n",
    "        \"新種の深海生物を発見、南太平洋の調査で10種以上の未知生物を確認\",\n",
    "        \"プロ野球選手の年俸調査、平均年収は4500万円で前年比3%増\",\n",
    "        \"世界的ピアニストが40年ぶりに来日公演、チケットは発売1分で完売\",\n",
    "        \"特定外来生物の駆除作戦が始動、地域住民とNPOが協力\",\n",
    "        \"月面探査機が新たな氷の痕跡を発見、将来の月面基地建設に期待\",\n",
    "        \"古代遺跡から新たな文字が刻まれた石板を発掘、解読作業が進行中\",\n",
    "        \"食物アレルギー治療の新薬が治験で高い効果、来年にも実用化へ\",\n",
    "        \"スポーツ栄養学の国際学会が初の日本開催、最新研究成果を発表\",\n",
    "        \"希少な蝶の新生息地を確認、保全活動が実を結ぶ\",\n",
    "        \"伝統音楽の保存プロジェクトが発足、無形文化財の継承に取り組む\",\n",
    "        \"火山活動の予測精度が向上、新システム導入で前兆現象を早期検知\",\n",
    "        \"絶滅危惧種の保護区域を拡大、生息数の回復傾向が報告される\",\n",
    "        \"古典文学の復刻版が異例のベストセラーに、若年層の関心高まる\",\n",
    "        \"宇宙線観測施設が完成、宇宙の謎解明に期待\",\n",
    "        \"南極の氷床調査で新データ、温暖化の影響を詳細に把握\",\n",
    "        \"新作映画が興行収入記録を更新、シリーズ最高のヒット作に\",\n",
    "        \"独自の農法で収穫量50%増、農業革新として注目される\",\n",
    "        \"珍しい天体ショーが今夜観測可能、200年ぶりの現象と専門家\",\n",
    "        \"考古学者が失われた古代都市の遺構を発見、新たな歴史的知見が得られる\"\n",
    "    ]\n",
    "\n",
    "    headlines_b4 = [\n",
    "        \"農業支援ドローンの販売が急増、人手不足の農家に導入広がる\",\n",
    "        \"児童向け体験型科学館がオープン、最新技術で宇宙や海洋を疑似体験\",\n",
    "        \"電子書籍市場が前年比30%増、紙の出版物を初めて上回る\",\n",
    "        \"伝統工芸の担い手育成プログラム開始、若手職人の減少に歯止め\",\n",
    "        \"国内最大のファッションイベント開催、サステナブルデザインに注目集まる\",\n",
    "        \"オンライン診療の利用者が1年で倍増、地方の医師不足解消に貢献\",\n",
    "        \"史上最大の恐竜化石を発掘、体長30メートル以上と推定される\",\n",
    "        \"人工衛星による環境モニタリング技術が進化、森林減少を精密に追跡\",\n",
    "        \"休眠火山の地下マグマ活動に変化、専門家チームが監視強化\",\n",
    "        \"仮想現実技術を用いた教育プログラムが全国展開、没入型学習に成果\",\n",
    "        \"新たな遺伝子療法が臨床試験で有望な結果、難病治療に光\",\n",
    "        \"海洋深層水を活用した新産業が地方創生の柱に、雇用創出効果も\",\n",
    "        \"長寿研究の国際会議が招致決定、世界の専門家が一堂に会する\",\n",
    "        \"気候変動による生態系への影響調査結果を公表、保全策の見直しへ\",\n",
    "        \"最新のスーパーコンピュータが稼働開始、創薬研究の加速に期待\",\n",
    "        \"次世代バッテリー技術の特許を公開、充電時間を従来の10分の1に短縮\",\n",
    "        \"熱帯雨林の未踏地域で新種の哺乳類を発見、学術的価値が高いと評価\",\n",
    "        \"古代DNA分析から新事実判明、縄文人と弥生人の関係に新説\",\n",
    "        \"ワクチン新技術の開発に成功、複数の感染症に対応可能に\",\n",
    "        \"世界文化遺産の修復プロジェクトが完了、10年の歳月をかけた保存作業\",\n",
    "        \"最新の海底探査で沈没船を発見、江戸時代の交易船と特定\"\n",
    "    ]\n",
    "    \n",
    "    # 閾値の最適化\n",
    "    optimal_thresholds = optimize_threshold_parameters(\n",
    "        headlines_a1, headlines_b1,\n",
    "        headlines_a2, headlines_b2,\n",
    "        headlines_a4, headlines_b4\n",
    "    )\n",
    "    \n",
    "    # 最適な閾値の一覧表示\n",
    "    print(\"\\n===== 最適化された閾値 =====\")\n",
    "    methods_desc = {\n",
    "        'simple': 'シンプル文字列類似度',\n",
    "        'ngram': 'N-gram Jaccard係数',\n",
    "        'janome': '形態素解析(Janome)',\n",
    "        'tfidf': 'TF-IDF+余弦類似度',\n",
    "        'minhash': 'MinHash/LSH',\n",
    "        'simhash': 'SimHash',\n",
    "        'entity': '固有表現抽出'\n",
    "    }\n",
    "    \n",
    "    for method, threshold in optimal_thresholds.items():\n",
    "        print(f\"{methods_desc[method]}: {threshold}\")\n",
    "    \n",
    "    # 各データセットでの最適化閾値によるカバー率の比較\n",
    "    show_coverage_with_optimal_thresholds(\n",
    "        optimal_thresholds, headlines_a1, headlines_b1, \n",
    "        \"データセット1: 非常に類似した短いヘッドライン\"\n",
    "    )\n",
    "    show_coverage_with_optimal_thresholds(\n",
    "        optimal_thresholds, headlines_a2, headlines_b2, \n",
    "        \"データセット2: より長い非常に類似したヘッドライン\"\n",
    "    )\n",
    "    show_coverage_with_optimal_thresholds(\n",
    "        optimal_thresholds, headlines_a4, headlines_b4, \n",
    "        \"データセット4: 全く似ていないヘッドライン\"\n",
    "    )\n",
    "    \n",
    "    return optimal_thresholds\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 閾値の最適化とカバー率比較を実行\n",
    "    optimize_and_compare_all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "検出された類似グループ数: 4\n",
      "\n",
      "グループ 1 (3件):\n",
      "  [0] 日経平均が3.2%上昇、政府の経済対策を好感\n",
      "  [1] 政府経済対策を受けて日経平均3.2%高、18カ月ぶり高値\n",
      "  [2] 経済対策発表後、東証で日経平均株価が急伸、3%超の上昇\n",
      "\n",
      "グループ 2 (3件):\n",
      "  [3] コロナ新変異株「XZ型」の感染例が国内で初確認\n",
      "  [4] 国内初、新型コロナ「XZ型」変異株の感染確認、重症化リスク調査中\n",
      "  [5] XZ型変異株、国内で感染拡大の兆し、専門家会議が対応検討\n",
      "\n",
      "グループ 3 (2件):\n",
      "  [6] 自動車大手A社が電気自動車の新モデルを発表、航続距離は業界最長\n",
      "  [8] 大手自動車メーカーの新型EVが話題、競合他社を上回る性能で市場に挑戦\n",
      "\n",
      "グループ 4 (2件):\n",
      "  [11] 東京都が新たな防災計画を発表、AI活用で避難誘導を効率化\n",
      "  [12] 首都直下型地震に備え、東京都が最新AI技術導入の防災計画を策定\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def find_duplicate_news(headlines, threshold=0.1):\n",
    "    \"\"\"TF-IDFベースでニュースの重複を検出する\"\"\"\n",
    "    if not headlines:\n",
    "        return []\n",
    "    \n",
    "    # TF-IDFと類似度計算\n",
    "    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform(headlines)\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    np.fill_diagonal(sim_matrix, 0)  # 自分自身との比較を除外\n",
    "    \n",
    "    # 類似グループの検出（単純なアルゴリズム）\n",
    "    n = len(headlines)\n",
    "    visited = [False] * n\n",
    "    groups = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        if visited[i]:\n",
    "            continue\n",
    "            \n",
    "        # 類似度が閾値以上のインデックスを見つける\n",
    "        group = [i]\n",
    "        visited[i] = True\n",
    "        \n",
    "        for j in range(n):\n",
    "            if not visited[j] and sim_matrix[i, j] >= threshold:\n",
    "                group.append(j)\n",
    "                visited[j] = True\n",
    "        \n",
    "        # サイズが2以上のグループのみ保存\n",
    "        if len(group) > 1:\n",
    "            groups.append(sorted(group))\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def print_duplicate_groups(headlines, groups):\n",
    "    \"\"\"検出された重複グループを表示\"\"\"\n",
    "    if not groups:\n",
    "        print(\"重複するニュースは検出されませんでした。\")\n",
    "        return\n",
    "    \n",
    "    print(f\"検出された類似グループ数: {len(groups)}\")\n",
    "    \n",
    "    for i, group in enumerate(sorted(groups, key=len, reverse=True)):\n",
    "        print(f\"\\nグループ {i+1} ({len(group)}件):\")\n",
    "        for idx in group:\n",
    "            print(f\"  [{idx}] {headlines[idx]}\")\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    # サンプルヘッドライン\n",
    "    headlines = [\n",
    "        # 日経平均関連（類似グループ1）\n",
    "        \"日経平均が3.2%上昇、政府の経済対策を好感\",\n",
    "        \"政府経済対策を受けて日経平均3.2%高、18カ月ぶり高値\",\n",
    "        \"経済対策発表後、東証で日経平均株価が急伸、3%超の上昇\",\n",
    "        \n",
    "        # コロナ変異株関連（類似グループ2）\n",
    "        \"コロナ新変異株「XZ型」の感染例が国内で初確認\",\n",
    "        \"国内初、新型コロナ「XZ型」変異株の感染確認、重症化リスク調査中\",\n",
    "        \"XZ型変異株、国内で感染拡大の兆し、専門家会議が対応検討\",\n",
    "        \n",
    "        # 自動車業界関連（類似グループ3）\n",
    "        \"自動車大手A社が電気自動車の新モデルを発表、航続距離は業界最長\",\n",
    "        \"A社が次世代EV発表、一回の充電で700km走行可能と発表\",\n",
    "        \"大手自動車メーカーの新型EVが話題、競合他社を上回る性能で市場に挑戦\",\n",
    "        \n",
    "        # 金融政策関連（類似グループ4）\n",
    "        \"中央銀行が政策金利を据え置き、経済動向を注視する姿勢\",\n",
    "        \"日銀、金融政策の現状維持を決定、市場予想通りの判断\",\n",
    "        \n",
    "        # 災害・防災関連（類似グループ5）\n",
    "        \"東京都が新たな防災計画を発表、AI活用で避難誘導を効率化\",\n",
    "        \"首都直下型地震に備え、東京都が最新AI技術導入の防災計画を策定\",\n",
    "        \n",
    "        # 以下は類似性のない単独ニュース\n",
    "        \"国内最大規模の美術展が東京で開幕、欧州の名画80点を展示\",\n",
    "        \"プロ野球選手の年俸調査、平均年収は4500万円で前年比3%増\",\n",
    "        \"世界的ピアニストが40年ぶりに来日公演、チケットは発売1分で完売\",\n",
    "        \"南極の氷床調査で新データ、温暖化の影響を詳細に把握\",\n",
    "        \"新作映画が興行収入記録を更新、シリーズ最高のヒット作に\",\n",
    "        \"ノーベル物理学賞に日米の共同研究チーム、量子コンピューティングの研究で受賞\"\n",
    "    ]\n",
    "    \n",
    "    # 重複検出と表示\n",
    "    groups = find_duplicate_news(headlines)\n",
    "    print_duplicate_groups(headlines, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
