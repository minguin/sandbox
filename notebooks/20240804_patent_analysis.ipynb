{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tmina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tmina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tmina\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 1 sample(s) (shape=(1, 43)) while a minimum of 2 is required by AgglomerativeClustering.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 112\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m    103\u001b[0m patent_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124mA method and system for enhancing the efficiency of solar panels through the application of \u001b[39m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124mnanotechnology. The invention involves coating the surface of solar cells with a layer of \u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124mthe nanotubes onto the solar cell surface, ensuring consistent performance across the entire panel.\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 112\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_patent_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatent_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClustered words:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster_id, words \u001b[38;5;129;01min\u001b[39;00m clusters\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[1], line 96\u001b[0m, in \u001b[0;36manalyze_patent_text\u001b[1;34m(text, n_clusters)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_patent_text\u001b[39m(text, n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     95\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m preprocess_text(text)\n\u001b[1;32m---> 96\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m \u001b[43mcluster_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     G \u001b[38;5;241m=\u001b[39m create_cooccurrence_network(tokens)\n\u001b[0;32m     98\u001b[0m     visualize_network(G, clusters)\n",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m, in \u001b[0;36mcluster_text\u001b[1;34m(text, n_clusters)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Perform hierarchical clustering\u001b[39;00m\n\u001b[0;32m     39\u001b[0m clustering \u001b[38;5;241m=\u001b[39m AgglomerativeClustering(n_clusters\u001b[38;5;241m=\u001b[39mn_clusters)\n\u001b[1;32m---> 40\u001b[0m \u001b[43mclustering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Get words for each cluster\u001b[39;00m\n\u001b[0;32m     43\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[1;32mc:\\Users\\tmina\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tmina\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:989\u001b[0m, in \u001b[0;36mAgglomerativeClustering.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    972\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the hierarchical clustering from features, or distance matrix.\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \n\u001b[0;32m    974\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;124;03m        Returns the fitted instance.\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 989\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n",
      "File \u001b[1;32mc:\\Users\\tmina\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\tmina\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:1087\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1087\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1088\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1091\u001b[0m         )\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1094\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 1 sample(s) (shape=(1, 43)) while a minimum of 2 is required by AgglomerativeClustering."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def cluster_text(text, n_clusters=5):\n",
    "    tokens = preprocess_text(text)\n",
    "    \n",
    "    # Create TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(tokens)])\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    clustering.fit(tfidf_matrix.toarray())\n",
    "    \n",
    "    # Get words for each cluster\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    clusters = {}\n",
    "    for i in range(n_clusters):\n",
    "        cluster_words = [feature_names[j] for j in range(len(feature_names)) if clustering.labels_[j] == i]\n",
    "        clusters[i] = cluster_words\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def create_cooccurrence_network(tokens, window_size=5):\n",
    "    word_pairs = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(i+1, min(i+window_size, len(tokens))):\n",
    "            word_pairs.append((tokens[i], tokens[j]))\n",
    "    \n",
    "    # Count co-occurrences\n",
    "    cooccurrence = Counter(word_pairs)\n",
    "    \n",
    "    # Create graph\n",
    "    G = nx.Graph()\n",
    "    for (word1, word2), count in cooccurrence.items():\n",
    "        G.add_edge(word1, word2, weight=count)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def visualize_network(G, clusters):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Set node colors based on clusters\n",
    "    color_map = plt.cm.get_cmap('viridis')\n",
    "    node_colors = []\n",
    "    for node in G.nodes():\n",
    "        for cluster_id, words in clusters.items():\n",
    "            if node in words:\n",
    "                node_colors.append(color_map(cluster_id / len(clusters)))\n",
    "                break\n",
    "        else:\n",
    "            node_colors.append('gray')\n",
    "    \n",
    "    # Draw the graph\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, node_color=node_colors, with_labels=True, node_size=1000, font_size=8)\n",
    "    \n",
    "    # Draw edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "    \n",
    "    plt.title(\"Patent Text Co-occurrence Network\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_patent_text(text, n_clusters=5):\n",
    "    tokens = preprocess_text(text)\n",
    "    clusters = cluster_text(text, n_clusters)\n",
    "    G = create_cooccurrence_network(tokens)\n",
    "    visualize_network(G, clusters)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "# Example usage\n",
    "patent_text = \"\"\"\n",
    "A method and system for enhancing the efficiency of solar panels through the application of \n",
    "nanotechnology. The invention involves coating the surface of solar cells with a layer of \n",
    "carbon nanotubes, which increases light absorption and improves electron transport. \n",
    "This novel approach results in significantly higher energy conversion rates compared to \n",
    "traditional solar panels. The method also includes a process for uniformly depositing \n",
    "the nanotubes onto the solar cell surface, ensuring consistent performance across the entire panel.\n",
    "\"\"\"\n",
    "\n",
    "clusters = analyze_patent_text(patent_text)\n",
    "print(\"Clustered words:\")\n",
    "for cluster_id, words in clusters.items():\n",
    "    print(f\"Cluster {cluster_id}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Only one sample available. Skipping clustering.\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "#FF6B6B 0\n",
      "patent_network.html\n",
      "Clustered words:\n",
      "Cluster 0: absorption, across, also, application, approach, carbon, cell, coating, compared, consistent, conversion, depositing, efficiency, electron, energy, enhancing, ensuring, entire, higher, improves, includes, increase, invention, involves, layer, light, method, nanotechnology, nanotube, novel, onto, panel, performance, process, rate, result, significantly, solar, surface, system, traditional, transport, uniformly\n",
      "Cluster 1: \n",
      "Cluster 2: \n",
      "Cluster 3: \n",
      "Cluster 4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tmina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tmina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tmina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def cluster_text(text, n_clusters=5):\n",
    "    tokens = preprocess_text(text)\n",
    "    \n",
    "    # Create TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(tokens)])\n",
    "    \n",
    "    # Check if we have enough unique terms for clustering\n",
    "    if tfidf_matrix.shape[1] < n_clusters:\n",
    "        print(f\"Warning: Not enough unique terms ({tfidf_matrix.shape[1]}) for {n_clusters} clusters. Adjusting number of clusters.\")\n",
    "        n_clusters = max(2, tfidf_matrix.shape[1] - 1)\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    if tfidf_matrix.shape[0] > 1:\n",
    "        clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "        clustering.fit(tfidf_matrix.toarray())\n",
    "        labels = clustering.labels_\n",
    "    else:\n",
    "        print(\"Warning: Only one sample available. Skipping clustering.\")\n",
    "        labels = np.zeros(tfidf_matrix.shape[1], dtype=int)\n",
    "    \n",
    "    # Get words for each cluster\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    clusters = {}\n",
    "    for i in range(n_clusters):\n",
    "        cluster_words = [feature_names[j] for j in range(len(feature_names)) if labels[j] == i]\n",
    "        clusters[i] = cluster_words\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def create_cooccurrence_network(tokens, window_size=5):\n",
    "    word_pairs = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(i+1, min(i+window_size, len(tokens))):\n",
    "            word_pairs.append((tokens[i], tokens[j]))\n",
    "    \n",
    "    # Count co-occurrences\n",
    "    cooccurrence = Counter(word_pairs)\n",
    "    \n",
    "    # Create graph\n",
    "    G = nx.Graph()\n",
    "    for (word1, word2), count in cooccurrence.items():\n",
    "        G.add_edge(word1, word2, weight=count)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def visualize_network_pyvis(G, clusters):\n",
    "    # Create a pyvis network\n",
    "    net = Network(notebook=True, height=\"500px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
    "    \n",
    "    # Define a color palette\n",
    "    color_palette = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#F7DC6F', '#85C1E9', '#D7BDE2']\n",
    "    \n",
    "    # Add nodes to the network\n",
    "    for node in G.nodes():\n",
    "        # Find which cluster the node belongs to\n",
    "        cluster_id = next((i for i, words in clusters.items() if node in words), -1)\n",
    "        \n",
    "        # Add node with color based on cluster\n",
    "        color = color_palette[cluster_id % len(color_palette)] if cluster_id != -1 else '#CCCCCC'\n",
    "        net.add_node(node, label=node, title=f\"Cluster {cluster_id}\", color=color)\n",
    "    \n",
    "    # Add edges to the network\n",
    "    for edge in G.edges(data=True):\n",
    "        net.add_edge(edge[0], edge[1], value=edge[2]['weight'], title=f\"Weight: {edge[2]['weight']}\")\n",
    "    \n",
    "    # Set physics layout\n",
    "    net.set_options('''\n",
    "    var options = {\n",
    "      \"physics\": {\n",
    "        \"forceAtlas2Based\": {\n",
    "          \"gravitationalConstant\": -50,\n",
    "          \"centralGravity\": 0.01,\n",
    "          \"springLength\": 100,\n",
    "          \"springConstant\": 0.08\n",
    "        },\n",
    "        \"maxVelocity\": 50,\n",
    "        \"minVelocity\": 0.1,\n",
    "        \"solver\": \"forceAtlas2Based\",\n",
    "        \"timestep\": 0.35\n",
    "      }\n",
    "    }\n",
    "    ''')\n",
    "    \n",
    "    # Save and show the network\n",
    "    net.show(\"patent_network.html\")\n",
    "\n",
    "def analyze_patent_text(text, n_clusters=5):\n",
    "    tokens = preprocess_text(text)\n",
    "    clusters = cluster_text(text, n_clusters)\n",
    "    G = create_cooccurrence_network(tokens)\n",
    "    visualize_network_pyvis(G, clusters)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "# Example usage\n",
    "patent_text = \"\"\"\n",
    "A method and system for enhancing the efficiency of solar panels through the application of \n",
    "nanotechnology. The invention involves coating the surface of solar cells with a layer of \n",
    "carbon nanotubes, which increases light absorption and improves electron transport. \n",
    "This novel approach results in significantly higher energy conversion rates compared to \n",
    "traditional solar panels. The method also includes a process for uniformly depositing \n",
    "the nanotubes onto the solar cell surface, ensuring consistent performance across the entire panel.\n",
    "\"\"\"\n",
    "\n",
    "clusters = analyze_patent_text(patent_text)\n",
    "print(\"Clustered words:\")\n",
    "for cluster_id, words in clusters.items():\n",
    "    print(f\"Cluster {cluster_id}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
