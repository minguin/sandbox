{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR TIMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# いつからの分を取得するか指定する（開始日付 YYYY-MM-DD）\n",
    "START_DT_STR = '2021-04-01'\n",
    "# 検索キーワード\n",
    "SEARCH_WORD = 'ドトール'\n",
    "# 取得数\n",
    "ARTICLE_NUM = 100\n",
    "\n",
    "start_dt = datetime.datetime.strptime(START_DT_STR, '%Y-%m-%d')\n",
    "\n",
    "#webdriveの設定\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "#webdriverを起動\n",
    "# driver = webdriver.Chrome('chromedriver',options=options)\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n",
    "#PR TIMESのトップページを開く\n",
    "target_url = 'https://prtimes.jp/'   \n",
    "driver.get(target_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#検索欄をクリックする\n",
    "driver.find_elements(By.XPATH, \"/html/body/header/div/div[2]/div/input\")[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#検索バーにキーワードを入れ、クリックする\n",
    "kensaku = driver.find_elements(By.XPATH, \"/html/body/header/div/div[2]/div/input\")[0]\n",
    "kensaku.send_keys(SEARCH_WORD)\n",
    "kensaku.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SEE_MORE_FLAG = False\n",
    "while not (len(driver.find_elements(By.XPATH, '//*[@id=\"__next\"]/div/div/main/div/div/section/div/article'))>=ARTICLE_NUM)|(SEE_MORE_FLAG):\n",
    "  # もっと見るを押す\n",
    "  try:\n",
    "    driver.find_elements(By.XPATH, '//*[@id=\"__next\"]/div/div/main/div/div/section/div[2]/button')[0].click()\n",
    "  except:\n",
    "    SEE_MORE_FLAG=True\n",
    "    pass\n",
    "  # 描画のため、2秒間待つ\n",
    "  # TODO:判定は変える必要あり\n",
    "  time.sleep(2)\n",
    "#記事数を出力（進捗確認用）\n",
    "print(len(driver.find_elements(By.XPATH, '//*[@id=\"__next\"]/div/div/main/div/div/section/div/article')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:01<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "articles = soup.find_all('article')[:ARTICLE_NUM]\n",
    "\n",
    "#記事情報を格納する配列\n",
    "records = []\n",
    "\n",
    "#記事ごとの情報を取得\n",
    "for article in tqdm(articles):\n",
    "\n",
    "    relative_href = article.find_all('a')[0]['href']\n",
    "    url = urljoin(target_url, relative_href)\n",
    "\n",
    "    #URLを1記事ずつ開く\n",
    "    r = requests.get(url)\n",
    "    html = r.text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    #記事タイトル\n",
    "    title_elem = soup.find('article').find('h1')\n",
    "    if title_elem:\n",
    "        title = title_elem.text\n",
    "    else:\n",
    "        title = \"\"\n",
    "    #記事サブタイトル\n",
    "    sub_title_elem = soup.find('article').find('h2')\n",
    "    if sub_title_elem:\n",
    "        sub_title = sub_title_elem.text\n",
    "    else:\n",
    "        sub_title = \"\"\n",
    "    #会社名\n",
    "    company = article.find_all('a')[1].text\n",
    "    #記事公開日時をdatetime表記に変換\n",
    "    str_to_dt = datetime.datetime.strptime(article.find('time').text, '%Y年%m月%d日 %H時%M分')\n",
    "    article_time_dt = datetime.datetime(str_to_dt.year, str_to_dt.month, str_to_dt.day, str_to_dt.hour, str_to_dt.minute)\n",
    "    #記事本文\n",
    "    content = soup.find('article').get_text(separator='\\n', strip=True)\n",
    "\n",
    "    #配列に記事の情報を追加\n",
    "    records.append({\n",
    "        'url': url,\n",
    "        'title': title,\n",
    "        'sub_title': sub_title,\n",
    "        'company': company,\n",
    "        'article_time_dt': article_time_dt,\n",
    "        'content': content,\n",
    "    })\n",
    "\n",
    "    #1秒間待つ\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(records).to_csv(f'prtimes_{SEARCH_WORD}_from_{START_DT_STR}.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_by_year(url):\n",
    "    html = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    pressrelease_link_list = [s['href'] for s in soup.find('div', id='content2').find_all('a', class_='pressrelease_link')]\n",
    "    len(pressrelease_link_list)\n",
    "\n",
    "    records = []\n",
    "    for pressrelease_link in tqdm(pressrelease_link_list):\n",
    "        url = 'https://www.doutor.co.jp/'+pressrelease_link\n",
    "        html = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        date = soup.find('div', id='content2').find('p', class_='datetxt date').text\n",
    "        sub_title = soup.find('div', id='content2').find('p', class_='typetxt').text\n",
    "        title = soup.find('div', class_='newspost body').find_all({'p','div'})[0].text\n",
    "        # title = soup.find('div', style='text-align: center;').text\n",
    "        content = soup.find('div', class_='newspost body').get_text().replace(title,'')\n",
    "        # content = soup.find('div', class_='newspost body').find_all({'p','div'})[1].text\n",
    "\n",
    "        #配列に記事の情報を追加\n",
    "        records.append({\n",
    "            'url': url,\n",
    "            'title': title,\n",
    "            'sub_title': sub_title,\n",
    "            'date': date,\n",
    "            'content': content,\n",
    "        })\n",
    "\n",
    "        #1秒間待つ\n",
    "        time.sleep(1)\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:54<00:00,  1.15s/it]\n",
      "100%|██████████| 40/40 [00:45<00:00,  1.13s/it]\n",
      "100%|██████████| 44/44 [00:48<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "for YEAR in [2023,2022,2021]:\n",
    "    url = f'https://www.doutor.co.jp/news/newsrelease/{YEAR}.html'\n",
    "    records.extend(scraping_by_year(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(records).reset_index().to_csv('newsrelease_doutor.csv', index=None, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
